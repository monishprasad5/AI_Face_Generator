{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xVK-O7FNHh8z"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "import random\n",
        "import cv2\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Convolution2D,BatchNormalization,Convolution2DTranspose\n",
        "from tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout\n",
        "\n",
        "# from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow-version:\", tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDRfpm30JLdJ",
        "outputId": "2ec51bf0-c54a-49c6-f1d4-c16355651646"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow-version: 2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYBtlYFiImun",
        "outputId": "f40c14d5-4ae6-4669-e488-8be4e0accd6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Final Project/Final/dataset'\n",
        "\n",
        "with zipfile.ZipFile(data_dir + '.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/image_dataset/2')\n",
        "\n",
        "data_dir = '/tmp/image_dataset/2'"
      ],
      "metadata": {
        "id": "GKfQbgRtIvDM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = data_dir\n",
        "def create_training_data(path):\n",
        "  dataX=[]\n",
        "  dataY=[]\n",
        "  # enumerate filenames in directory, assume all are images\n",
        "  for root, subdirs, files in os.walk(path+\"/dataset/neutral/\"):\n",
        "    #print(root, subdirs, files)\n",
        "    for filename in files:\n",
        "      file_path = os.path.join(root, filename)\n",
        "      #print(file_path)\n",
        "      file_path = file_path.replace('\\\\', '/')\n",
        "      img_array=cv2.imread(file_path)\n",
        "      print(\"neutral--\"+file_path)\n",
        "      #print(len(np.array(img_array)))\n",
        "      new_array=cv2.resize(img_array,(128,128))\n",
        "      dataX.append(new_array)\n",
        "  # enumerate filenames in directory, assume all are images\n",
        "  for root, subdirs, files in os.walk(path+\"/dataset/smile/\"):\n",
        "    for filename in files:\n",
        "      file_path = os.path.join(root, filename)\n",
        "      file_path = file_path.replace('\\\\', '/')\n",
        "      img_array=cv2.imread(file_path)\n",
        "      print(\"smile--\"+file_path)\n",
        "      #print(img_array)\n",
        "      new_array=cv2.resize(img_array,(128,128))\n",
        "      dataY.append(new_array)\n",
        "  return dataX,dataY"
      ],
      "metadata": {
        "id": "KbzYLRwxOplJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 128\n",
        "dataX,dataY = create_training_data(path)\n",
        "#dataX,dataY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anwa14OgPGb0",
        "outputId": "6166ac67-0805-4f1e-dc26-96f9f678878f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral--/tmp/image_dataset/2/dataset/neutral/34.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/376.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/54.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/25.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/92.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/90.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/11.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/14.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/272.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/97.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/96.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/275.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/278.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/106.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/211.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/33.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/339.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/308.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/387.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/300.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/328.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/377.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/262.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/98.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/48.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/350.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/32.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/104.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/63.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/216.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/4.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/22.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/265.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/80.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/336.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/303.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/248.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/91.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/8.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/53.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/105.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/212.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/321.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/293.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/49.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/38.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/203.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/56.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/208.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/23.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/214.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/249.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/373.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/351.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/41.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/31.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/70.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/232.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/255.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/284.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/20.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/274.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/109.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/78.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/251.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/326.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/74.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/57.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/6.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/83.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/100.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/348.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/42.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/302.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/3.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/371.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/334.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/230.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/222.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/240.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/88.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/86.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/89.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/359.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/71.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/395.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/368.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/111.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/36.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/26.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/50.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/28.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/384.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/382.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/243.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/316.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/356.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/207.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/295.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/210.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/46.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/37.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/112.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/79.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/383.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/213.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/84.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/344.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/317.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/27.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/58.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/352.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/398.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/113.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/370.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/10.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/94.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/7.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/202.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/93.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/357.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/313.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/9.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/5.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/320.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/35.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/40.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/301.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/215.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/337.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/264.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/372.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/239.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/68.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/322.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/30.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/375.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/242.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/72.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/67.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/1.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/2.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/95.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/363.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/304.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/332.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/299.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/12.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/296.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/21.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/24.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/87.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/311.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/18.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/347.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/69.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/16.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/367.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/227.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/366.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/45.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/82.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/60.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/51.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/327.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/271.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/62.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/43.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/283.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/355.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/29.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/333.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/39.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/217.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/13.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/219.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/85.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/103.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/254.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/400.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/391.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/55.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/209.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/252.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/261.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/73.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/269.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/244.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/360.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/335.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/298.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/77.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/61.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/110.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/279.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/102.jpg\n",
            "neutral--/tmp/image_dataset/2/dataset/neutral/260.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/34.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/376.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/54.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/25.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/92.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/90.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/11.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/14.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/272.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/97.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/96.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/275.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/278.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/106.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/211.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/33.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/339.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/308.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/387.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/300.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/328.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/377.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/262.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/98.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/48.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/350.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/32.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/104.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/63.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/216.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/4.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/22.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/265.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/80.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/336.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/303.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/248.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/91.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/8.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/53.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/105.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/212.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/321.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/293.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/49.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/38.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/203.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/56.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/208.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/23.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/214.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/249.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/373.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/351.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/41.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/31.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/70.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/232.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/255.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/284.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/20.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/274.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/109.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/78.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/251.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/326.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/74.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/57.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/6.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/83.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/100.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/348.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/42.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/302.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/3.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/371.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/334.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/230.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/222.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/240.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/88.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/86.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/89.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/359.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/71.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/395.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/368.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/111.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/36.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/26.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/50.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/28.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/384.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/382.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/243.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/316.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/356.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/207.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/295.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/210.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/46.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/37.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/112.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/79.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/383.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/213.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/84.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/344.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/317.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/27.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/58.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/352.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/398.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/113.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/370.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/10.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/94.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/7.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/202.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/93.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/357.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/313.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/9.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/5.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/320.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/35.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/40.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/301.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/215.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/337.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/264.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/372.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/239.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/68.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/322.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/30.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/375.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/242.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/72.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/67.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/1.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/2.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/95.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/363.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/304.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/332.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/299.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/12.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/296.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/21.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/24.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/87.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/311.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/18.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/347.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/69.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/16.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/367.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/227.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/366.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/45.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/82.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/60.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/51.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/327.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/271.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/62.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/43.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/283.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/355.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/29.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/333.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/39.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/217.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/13.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/219.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/85.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/103.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/254.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/400.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/391.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/55.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/209.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/252.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/261.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/73.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/269.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/244.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/360.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/335.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/298.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/77.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/61.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/110.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/279.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/102.jpg\n",
            "smile--/tmp/image_dataset/2/dataset/smile/260.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(np.array(dataX[0])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbsw0hya9tuf",
        "outputId": "15516222-2f39-4314-f8b4-d902ff2871e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(dataY[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "wEFkmsGt-G_t",
        "outputId": "7b819ccf-28d9-49f0-d387-dc2f4fb8b628"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAABRgUlEQVR4nG39a3okSY4tCJ4DQFTN+HL38MjIrLw19/ume1u9oN5Zb6Rn5lZVZoQ/SJqZqgCYHxBRY2Q1MyKSTjeSaiJ4HBy8+H/9n/8HSUl4796dCYq4+967h19v1927u7t39+57b6qPz08vn1+en56fHp5UxGjMMDXJvqgKQSAAT2Ryi3x7v/54f/vx4/X19e3tet16z0QAASYEAMneu7urKroDaKZAhifBiHB3AJmZmadmomLazFREkBmZBBJ4u16u12t3uncSKiChqiQzU0SSzIze/eHhwW9ba6ZkM3lc2peXp1//8vXz588vnz49PD9oa0CkB9ORiYj0nt7BrF/qGT/398veX39evv94/fnt7dsfP2/Xzay1xVSVTBWYmelKqqit68lMhRAhAFGx1my+sfH2MsGIyMiIiECiPjIjM0GApLA+AABEZr0i60cBqL+p/0sAPL5Q35g5/gKZSbBeWf8RqR9FgERG4v5qkEzU6yOCIJCIiPmQOV/24Sfi+FJdBOrJyfk1UkREREVFZDzseLP1ixPHQWC8p3pGkiIUCikEMzMzMjJlvmS+zUOAQGSSBBKZaZmZ88Azwj3qKz28d6/3FhHu4e5MqKqpllihfgiS4ySRJY9E/Vs3y3nyIqKqmglExvGETEJVM5OkmMDjOL7MiEySGQFAVevNuEdEampmln6AdHdkloCISD1FfQgppI9TEyGhWuduqtbasqxtadaa1FvL4wmQCWZdySF0qB+kIqrWWjNTEBHRu1NEhJCMgHsIQ0TqOYUQCpiH+FlmRkR6lJC7u7tnpkfsfS/xy8wIjwgrOTETUYqMA8xSg0wONfgoLPWk8idBk0xEOJDjfZRsz9dkAhkkQNbj1RuoF0REIjPuMtXr3IUlLuPHTDG//5kl8SjlreNXVTNbWlvWZVmW1pqoHFI+PslDtQ6xADJJUERVmzVVozAzPVycriIYjzRNS7q7CDUI5DixTIuIuoCsU4+oC4jMej/JjIhMkKKtLcuiZiUjeeh6nWUOUS9TQhL1A0hSdErcfAGFMiyBSMzzHe9x6ui8klLm6L4jMpFCAZGByPQIZCIR85jIu52ROnihCJlGIiJFhSJmujQ7rcv5fDqdTsuymJmKkh9sV533lEQM5agrkYwuU7GXZbXltm+7h3fvWqo2XFRZigyPLhQkKZRkpEWJfAQSGXFoQCIjgmTkuBIRNWttXVSNZIKZAGWYtMhk1ltOZMlZYFyAyBD/Mg9k1LHoEEhJ79M9ZAIybiLrXYQ7iESWGfRwMxORyFJWn7qYw09k/TenWmE8ALV0SCgiNNOlLeu6nk51AauZiUqpyTD/01ochpycFgTMgFCUqmrLurTWtm3r7hSCqsJARoYkkMjIGGZSRBAhIrBx3OU8pr6U/6331sN775mpChEpF0BKGabwGE8FCAUf9PZwViXQzcxUlVJnocdbZDk7isjHQ8TxyTDbGsMus14zFDTnEREFFKwNAzO9DilULR9rFGakiqiwNT2t6/l8LvFX0zJVw0EnE/nBkU9dzxINZESBgUSqSmvLelpvt9ve97I2CSVkHG2GgFl+McdRR6S5ewmLR2TEeG/jOmIv7Nk7SYCqoh89cGaEAwwRJSk8ABGTAMt8lpWx+lZRoYskBoTBMKccZ37YonrQwz2ICIFMqEp53ciYxrmksiAA5slP06AiKqpUUdVWBk0JVWnNTqflfDqdz+dlXVQVwpwmrOSI5N2VD4DN4QkiQBZWFNFlaeuymrXbdgNSVUAAzEREBKPMxiFkdRHm7gNNe+CDBvi4oui9R6RaGVGZfk0wvdod7WVmIsYfk1NPC/mqqqmZmUWA9EzPjEREJgYKqoeZ+AogkgAwHGNiuLUhIocXlwFlB1jJw5/buAAWclMzUyVFmKq6mJ7WdV2XtixmbSjLdFEfwCZATi8sORwzkEDktKgoh17exSM8wiNNhyuNIVYSEQUDJJRCK0OPyEgflzLE3zNREVgeIEUEmHeA8qgiPGwCpA4+IXfkjjIvqmrWWvNAogs8kJEeZCKpKnXfVBFKZgz1ZwIQYUkEycI8gGQeyHDENmSSGZlKVggmIipUUatfb2qtmRoJU2vKdV0L/KgZZWCn47mHCoLTGPJuhEBAMl1EC98KOUysSHf3DM+ITEFEIBhkikTEiFM0PELGBdSpH8HYMEZDFVJMzMzMVE1EZvjCA3FnRIKJ+i/r1DDjtumDU9XMbEZ0jqgXE0iZGkBARCIy4hC6Ao8HvsIwAnl/jjJBdYAkRUVVrDCN0lRbqzdgrVlrC8mmasK1La01ayblAA6Q9i/B19B1mbB8aEDG9GNTQK01NcO+R3jv7hqFB0JScgaPIADPFITV284YTrO89gFDIzORqtpaGzB5PuKHaygrycwB+ojiIwhCRSJLtkJVmllZ6pghkUR0VmgIVc0IkfLopHNg23kNJE0NQHhOK8jjjqCiIgDMtJm11kypQjNrS2tmItqatdZUaCKmsiytNVOTchVl67JiSPzpY/4iZJZvG6jXPTIhogkn2Vqz1ni7unvvvZtTRYc3zIjIeXAl83bIfN1AFOeQGHpQIFK11ZMWkClkN31s3p/vUNK6D9RB5gxEVVTNLNMj/TAfmWT9eqpKTDwk9GCMgKB+FpiTqxg2v6RhnlUZusw0UzNri9lAO7a0Zs1IaWbL0pQwETNtFcWqspCpFhEykScn6JkylwjcYT0isriDoZhCa2atiUgvCi1cRWVghDIUFABEIKI0oOKMQSRMqU+kZ/kWKd9p01BSVCiJdB92QlAiqyMurpA4IzmfvA5LQ0NCVTUsU0SC0ISL9+4k02zPXUiolD+upy0/FJkRTmr54bJeeYjl/NBy+GZtGE22Zsu6mKlQW9PWzIQCtqZLs9asYkQIIYIyEwMHEccvGaHhhxA5stiaOrPh94urMXPvw4qEJwlUQJbMTBkCGpk2eZny+eH1LcVWjvBX1UytqZWizqge8IiSeVFlwZ3IqQEZGYBQCtkNCqZEVDUsEcgkInEwDZka4aUYBSSH5iIBBKK7m4nHDLjul31HPnUBalqBrooszZb6TK1ZXYAQWMocmaoIZcJ/4aCvDmtNDDQG1MEN8c/08rYTvpMiOvR822Wcf0RIcQkhOR0bB/9pQ7tzEJ51mxUEJKBmQpoZhQcWqhh4Mg5Sp+sZcDKDQsyIbHhUoKASSRVJ1WYJsheEzgTEzABERmstMyvyEJFDuMp/mRlAVa0AcLyXojUGtyNLG4jBVJfWTLU1XVorTGSmi7VmkpmtVeQ7fS9nbEJCCvFXNMCDPpv6lokMT4Blvz3TIyNHsKjNdLOYrJpGRAYCQUhQCIEwwt0Nk9irl3qMeMwzAmimTc3MBhAu2ecRPR3IpFgnCDEumYN1LRWgCCOETKEm0jJJuPcIZJBMtSI/pImH997LppdkyYBvMDP3lPHrprYxWUhXRFXKqpe6loUx02VpdaPNbGnWFkuP1tRMZXDJd84cIAQQwpGTlsJBy+WMAaJoSNSRHYwIAGtNTSuM8IxBMjAmh0cQRceMC8AMawIZSE+PDICRqcMEqWg53orRizoC0qlyuOIs8EnmtJWRCeRBPySCVAoi90hpwkiJQEYPj2YWGXCY2XHE+cENVlx9fCUzyRTRUi/RQXAuS1taW1qh/6EHIiwUVHw65HC9MswdkRnlYdMLqCUy8q4IM+eRIFnpo+PBIrL87kG+D/OQiERkEDK8eplnEqQVhxyZQtbfOMIzHCHUyBQbXoUiIAfLMMAwOkJkMGgcQKX+GY89tLKMT1F2FAa6uyNJqVglPXuminoKgDB1l6KsM4MoFRq+MCIK+xz6gdIPHR91AW3kzrQ105Fw0VbRQfHpUsaLIgKRGeCW1Dgzi14qpccAiojMAEBRtcRGmWgwo/fe3ae7EwqH005EJCVJxgcIRdJiOookY2CXHO7XVE0PbFnPO1FhkJLA9BlJYuQtkDEcDHKEA5CCU5lFhoJqVnQcXDIiGxSZRMWEaTYM6JGPZAXscZj+CUrGH0jhPG5dWzsta2vaVApmHsbezMrumMggq5VUweA4hyYPV3u3+gcZOj58QLHDIc/QdTJU5ZCL5/dMZmhOj343ZWl9OmqvnzJkNklWridmkFBsZd1GZMpMKdQdcJrsyPn0x1sgSaiwUisglWxmIhEJuBPDmCbB4CSkRobH3T8mHac5iiM+LsNQTEBr1sya2XpaTKXpiOGXZZERo5mpqlIplSfgEP/DB5QClxlNUgYtN8i+LJ649+6ZmSy+IergK5LKweSras9ypxEkKTrouKkYCSs2dXzjjARAiGpFhmUcPwKAuvlADA9bv3E8dTmAxAfehDNaV5EQKXLRTFnoLJMIkEitY1YRmGZGprn6IF8H7viQcxn5oOP0RUVaa63Z0mxdmgpNi49TUxWVYv1UVXSwbsUUDojxkXoYB5QH5Pgg/ojM7p4UiMCLHIzpnIce3DWnLEaESCWLi8GQRCTSIn1Yq4jyQpkhKta0WB81KyGITKmUGwigIxdtas3DPQOQyJQJKYAs3n4+8/glpho4aDQ6Mgp3k9BkRiZMSWpRdWUrYng6d5bcpYDJMllJQZH9kzIxW8xMTURVTMSGIAlIVRlpdDkirHKwyLsWDNEpNm4gzQFpcupnUFVMGHJc1wELhzGPLDsVBJEezhBmTCtCybREDtI3h+GIcDFRLe+EMp1ljKM4SAxqPpuJSvceERREEaVH4cI0kRMZkYBqFZKESBUOhFYUVtRzUIpmZTI1LEzVzSr1kZ5CifQKvhnTwQA2mOZBAS3NzLRNYFrh44BBxfmMWDIHv8A/cz+cVn88WP53DQhARahKiek/Dn8xXlPGc5iw8oyY6JlDXWwm7KN7T478TnllNalEC8h93+uMJAtNMeHdVVw8fBifgRBCBg4gxtcrwqx3KsgYIlG5XREdeJOR1Dp8IBPNEGuOWg0gNFV1uAsgGYJCkdpaW9d1WZq1irPMpMh5UZEC+yLUYkdH8Dg9Ij/gn2ncDmtLRL2Lu98EQAi1nFuBWcyMUBUMcFRI9IiQUb80SQJi5C9ICozkqDmJwAcqNJHCkSYksPVe0XoZ3vou773ruABLPSDQCNemmQYOs1S/VIJZtRl5z7lnxuASpvMIwDLh7tx3j9C6gInFJxdHUWnN1rWQp7XBGoqZCsVUrLRZRVWpOvz2wbR+QLQj0zGfIDOGefyAf4b9FKkAtqwbKR9NfoXulcotDDw9RBmH4SFJmIqkMAghetHwQtVi1kFCRfa+k3TvTAg1o5j33HuPqhRDerhHjHuddDXmDZTBwUyx1iEPlJNEMhIYub8BO5ySTBUx0aRXrDqIqJEFlJly0WZtXUoF9HC8KqaCYXOEIpNsGID5w1kf1r/Ef3LDdS08eN8j9kZS6EBSwBRVa2punkHvpSGlIrOgrfjjqkaJACplG+mmKkV/iTC7Z0aZSxGWb1CR7XZbtPXuIFQyJUTETHfve/j54QQUjeEEC2xMXoVHDBFgICq01yKLcro6ZIZzsmEcd8YMUYGKumghxrvpBoQsGtNUltZO67osy/LhAkxVBCooDyyVpOVAZndzz/kUJTdDHSaCmMn5MgxTgkmREf7M4iJrvbsWUV9aD7Aw+hTBEQQLQUhmRNIGYyMyw4EoxDaFNHrvrBKEQleRqQJYBClDZQOJ8N5dm87goAiOAZliuDNOZ1yyVWQRlczi8Irxk6K3Wb+hjrJZE8phBeqnlOMt4k0EVkSQqghLh0UosxDjcJBDrnnge5LTieUBI/PA/gXy+whHmBwBk3sEuPe99+6jdGpylGQFHzO5XrZWjr89tHmkJIsPCQ93b2hKIUeStmcnmRHMzIweqamkRIomIcAI3bOzN2s5MuYJL7aIQGYIZATVLMwwEaCwuLqc+eUKi1A1RCliqmnW3FV0JPEzJ6bSNky+kVSVpfJboxQOIriTnRPa/OkCpgGaL5gpwarLKSMCjIKpTIoCI9Z3D0dWIVBV9wx9J5CoC9i3rag4UMtWc9rSgsOmMjgPHzihMIOUc/K9Z2EaBwp9JzJHpSMVCnXvmclkzHKKA/JklLuotxbDKUzJwtBgZFKy4NDw2JlQkRRkDixQ4mOmEVbvITOXpa3L0lortrmpmhXdJiZKSZnx1sQ2CXw89+Fsxx8P8f8ASaOCKK/sOAgGqmQhItKRey8GqLiJQQAHUkQrhdX7Fh/4xGHrpiyaqiCRSalyVWVrTYTFzHTvqkSmRIU8dvgr712MkrLtOwiThlF/SkCpVBKZOt9JhlN16HwW8QqSOXB/iurIe5BIhmhKRSUyrX+xI1m+N8KbtXU9ra2tbTGVEQq01kxMFYdh+3DmU94nDD3+ajzS4EEG+17Gc9QOjjr4AMrhecJz1BKOisIoCkezu8h4nu6DZMNdyeqPScBUSEpmeLiaQmjNykUkEEVZAggyEelljsI9EOIqFt5TRASRkh4hPUAISJVB08yoZsjaBzaBg8yHsMoReQjfFPyhAaqKpGkANjxEWGnAuizr0ibtLJWKEVOg3N0weMM6/Ilhy/mAo7Qcd6gzPUYOmq20NjwciPDeuycdo562bNR0bAOCFDl+xBAYR/nBGQqtSqaA9ExrjfXHzO4Z3pHwvqtqBjEUQtzZIyhQl3AGQNI9QjIQjiARlX/hvICDmhvyVcQAQMKjHkpQeLTSHDyCiQk0rf4WA9QJkAU9l8WWdVmanZZWTvfIXiAcI4FMiMy6NjCBUdgYyWLAJx8WSVSYcmeHDmzmURcQu3tPOg7+ODx6+BGqDYGb/n/cokwC8PDDVoEiQMtoSwOppuGOHh6OYFFEU5iSNNATLpBwd9ckIoTI0AxmoVFhlAWuOL3AcE7rP+3ytEYlHYnpY4ZsHk+pmpkK0NOOL4pIVVUV9bYubVnaQYtSOJQ9fKJWAjNrTZZ1z4xx1MXdHGT9h9OfEDWrErsnukf33lPKH1RCzN3DR+kuJ7dRlXmJySl/ePjh1aoiPkmFNW8VXyI8MtI9dldtfdsFOpQmSUiCo3yuO9W6pwoyQJUo55sRSKUGkhCPVJVCCQMZD/Q6z8ETyco1AuHhUYB0YqGqtgLoEsWkFflsqotZhWCqHFmngvPu6T28JyBqpXjpfSAZr6RXjiL5GDWEA32HD945iymKHgOHekaP7hG9UFDfwruHe+yZEChJFXQPUsxMVLtHzCqFSccKicy01iwJUxis5eKRBMKJjPDe991E+3YTsWEMQpg6QKoH6SJWNHFqyTGiiEBARTwhpEeotSzrVLUslJGMq3ydB1OEggQC7p6kWmG1YI4TF5GQLJ5pJBeFS2trG9HLgScrkxDd3XtZNDigku7l99M9s/hDomxEeJELE+YwZm6k+GfPiJQesbvXBXTk3vcId+8eVes0GhIqyjEzVdl9KvYo85Eqgo8Ia60l0hOtOmSKCnJRkU5E+PV6CQ9AiymiJMTJNFFkhnt6TzLZwqP3ripiSkFmRDqz3jj2vasSgO9OBEUrQxkRDPjeNWVzj/Be+SFhZHG1VeAORAhFTbMoaFI5GH8bEjX548jovV9v3rdMB7L3EGtqiO6mFMXkz4p7CESwAgCGggQ9ImbzQffBl/XM7lVz1cOL3/SsBouq8YBHxvRhKYQ1k5BEUDk8E+6GyFpr1ZvRmEl4RLhnV1Epc7Hvm4jhIPskkq6aVhSQu/SelNT0CPYu2kQIQVULICFBQiL2AjC9OwEVpLD0WhLed6TerleSPdwBKBGRmDULICLEtJmWfhRVa0IrwpnAaN2T9Iit3y7X8I4quxM3TwTdXVIYoBEJARHIcNZjuAfRzACJ8Miowiv33r13z57Ry+AU8M8qR/EMr/KlIpIFpI54xZpKZ2aM5PORnlIlaaYWiKOFxD1CnB6xrOi+X7ftsodG9y1SoG2hVQkosvpDmOEizPQMBNMd3VMSkppIKegSmukIeOa+b0oJyQB63bdH9O4p2+0GwDO9mDpWjVOLyHQvuxGDOCinhMqGhDvA6jXcuLt3bPt23d33wc0w9j33LQCgUeii7LHP+NSLqfAIALtnEB5R8VVWVjEzEBHpvpc25NYT4ddbZsZoYhXQQYxPFBRRUHd2z+HlRDnTFKqF1ygATNNc3d17r5pHeNza7We8Z8a+bw6lMdgoaFWiEmnG3buIZlQ6obr7XAWSCiRCytd6BopU2fcQEZllSOH7bUdEDxSBnkTPdCRElrYq1HvPTDFklbXYCNRlppLCRzF9ZIbjtt2k+37bPPZy3wFE7ipdRBGsMottv8mgg7MJWTRoBQ4m7rn3nhkgZhxWaKd79AjPfc8Iv9xAum8RTlNIihkkk506eEexQSxRODJEaqNdQiZplYQJu9AFyEQEA7fTzaxtPa7X7eZbiK87es88JyJWFbIlMtxdHAlVyQ0ICaWGhruEJhamOGLfmBne3VSF3idT5duOqtjtXgrsVaDjQahg63sv+wJSwxe0Io+Gwu3McJm9q8K4Xm+a2XcPDyFEM5AeIZIiHoHB8u43WtFPsRTDLlUx7wwrticywHQftijcw7v78FQxXG+4Rw9npphmpWuqlUJStBiUKKGvmoCRIxUxBSlsTUm4Yu/ZqZXnk+Tt/HA6ve2vl9u2/7zsW2522W9bdz95X2JtJLXJvu8iGhGZij3SsKyWUZosIYlkZA56PMLUSJYGAAkf4Y/3XinrEFTHeM8dnn3fRRhuYEqXarkhgdRC2OUowiOShN9uu4LZIyKIZEQC3V0FENn3EMLU9r6hM9IlYxMxipgmmVRxiNFHuUhOrqccsRdER9B7IsV9d8/ukR6aKYim4CgMDwrNqma7amJsZChEhDSpcj5VEUSEcPTSV55tu+0P5/PlsrvH5ba9b6C6O5Dw7tGd5JJGgVoQZDL7nj2JEJHeI11ckcGR9s5EpOrov6wEnFZ8lAOEJZA6yuvCe989uqtJ706FeOVhZmogR6irZIyiEdn3cCD2zgww6UihR7o7JBigUMW77xBEOsLb6GKTVEmaOKwx4B49Ed29uw8GtEc43CM8t923Htetu/ceDkUjhKmLKlktEyrSlkZgrR4LayqiFOpMl6pqU1FJn4mjquEguD/05+fH1/ebqrnn5eZeiVJI371vOynL3s/nVTtMJZ2+R+y9Aui9R4RsSqRABZFKIlLYq9ql6q5URJLu3VR77xSJKLAv6em7k5kQD6+S89t2q3wXVRSjYi5Fyg1Ux0Tvvd+20Q/GrAjRvasZPEju8K3fqEwkInakSVZ7NQzSU7dQzRJ4D68GLu/de3RH97ztft369bq/v188HMy2KE3F0yJMRuHz0qqsgad1PZ9OrVLtlZQBTaWyplSBplQ/hzpMoJS++/PT4x/f31trSG6b39w9CWjvfd9E1U6LQgzqawoRsVVxqYPYevSgmWSKmDFTKYwq6BRR8Yh9301USe8dlO7BKiWTVGH03LZdlUn0vhezH9tuFmpNpK500Hfh2Xv0niLqe99uezNVlUAKJMndIxnRvWjOvfdKrWcEM1SEZc4ahV3ZrTHTPaPnaAaPve+b746tx/vW36772+vl7e09wpvJCYs0NaFXHUs4RM3UxJiyLutpWSpiVEoyEGmilfaDSlbZiKaIwh1C6Sd/enx8fnp/fHxc1p8iW+x52Xri2l37IgCfHlandNpmWEDxUGIPBzKA7vDcATMwuttseBQJCY3M3h1KB5TSPSDSqwjCIzVyNMHzIDAyEd0z06yrVIIapHRE9Ny27kFh9s23vQOMZBVBVW3t3nvf9rJfgUzPBAZPkx3pSaInAcFuChEGcs905iAHut86rj1+XG5//Hz/9vsf18s7Ik5re0GI8kGXiNy3LWGryThxyNpaU9H6ikiC6WEiVKEwlVktbk4y0IVS3v8pn54eHx8flnURuUT2betbj951X2Xfvbt3wO10aliBhTBJZgBOkR6AdxEk2Lc9NEf/GKFZNxRAIFOXxT3MrPteCYwMJyQSUvElUF2i5RB33VUskyJQZVEP294TSmDf+7Y7qQkyQa32t/SIbd9JqgmA2Z1eZcleYBq7I5O5K7O1FkRH9hok0Pvucet56fj+fvvHj/d//POP7XoTxPPjSZssi65ni8h928E8V4WktGCuVaWtI+UF0AmTomAka74NVUipWQyAr+sSjuenh8+fnj49PT2cb6/bJfdwcEtyj8zuP95fd//9libytCzPp/VhVTM0xUJJkJDBjEUQXnU5ZKGf4jdG5Qur2WE0PoQDNW6iUp6U+XM8VSUThbOKuCfCvbJW4X3UMHoEPJGMjh6+b7tn+L6RVNfErCHMqrLZAU+ho0uGIRxZUbeLOrl7bNt+ve0/3m+vm//nz/d//PHz5+uePR4WQXVjLmtrzahAKrmYmWgzU5N1aUtTVbWmSkkwlCZMGfmQlErcijAVzEAKNZPPzw+fPz29PD8+PrzZ2y33DMgtEJm99/fbrpfdf1yA+OXx6eun50/P5/Nq50UhYpDB0Hl0dyQgGDMvYnSmhUCIvXcZg2mO7EcnRE0jEJlNNJKZ4ZGiGIHdqOIfh+me0Nx7l4iRPe9JpTO3fbvdtojI6AAKGkYmhQL63ve+gUHhFlkdB/AQkYCkLaF62+P9ur+9X//x7ecf1+0/fl7++e3ndnHLbGYQE2vLsjZbVBhwJZtqU61iyaVZMxu9XiKZEiO/KtAaYzESIEzQqmbYxCMfzsvL8+MvX15++XH9dtlvcb16JuDJ4t+3HntNVspLj3zbtqfH5dPjSZ7swVTB3rtZqwkRMXNG9RFIL+otwqCj5ioHPUGwMSWluBTuXQXVUN+7kz2rTl4qHSgeHuF73w0YQ0hQbbFahGXvHekYTfceGXDKqB8JpGdijjKiQAjeemy363v3n+9v75fL5bZ9+3l53fuP9+22j+IgMz2fT89Pzy/Pz4+P1haIrutJH06n09LqAiptV/Vho1acNMxWjkpNVmpKKKCwiYTs+346tefnx6+/fPr68/r7z+vbFv267z0CDMJEu3uNFupvt8t1+3G5PN9Wx8vD6XReViZ736p4ZOQ1OAaiFEvr4aB4JCWP4rLuvldcxjmniR6J02K9hzB2dKSEglKjI1LVwmOPvvedIt19JmED2hMRiB69UhkaGuHdHfjTZIvISFYzEVSFtEvfvr/dfv/5+s9v317fL7v7+61vmW97dM9FaZJtsYeHh88vL59eXs5nsZbrSVqTh/NyWpamtpitzdQUyjGYi5FBKzL8KJeoVs1qmSUqR6/rujw9xqfnx18+P//y7f3nZdt7VJlSAgHpEVGjcYorEWST83W7RsCUHpTq1RcAKWN4wpGog7Cm9ETMQk0f9SE4JpIxs6cBe4e7O1URPXbXKIcWmSG+e+/wgWj7JkPJgklIUZWOCBLhu+cIdEcdZVbVSxSBu3n27Xbt12+v1z9+Xr69vn3/+Xa5bZHRq1nY00ROizwv9uXl6evnly+fnj49PZ7OohbrKta4rGV5tKlamyXPFAKSEgiroVSz5o6jdlQVokxWfctpxeNDvDydv7w8/vrl6fvP98tlC/ctERAHPQlqAqKS3jvkGvm691sGTYWxLOp0VCk30iN6ZJX7kyI0MRHIvnWpNEPdblWTMHt0IT1dRPqe3iMYwQzv7KOBwCNc5LZdQ4ucg++7zgS5FD+O0X5oJn0o7ZCAJGr6UCXTQLlG//3n+399f/3n98v39+269d1970BNKhOa+GLytNjXl8e//fr5r18//fLy9PJ0Pp9FNMwgja1pM607UIVq0aMQIFNI2MgeAUdJLUZFmSCJSDNFw/mUz0+nz8+PXz89ffv0dnm/eUTuRbcfHSZCIKgOboGr+80jTQhq0x7OwryJ6FVcFsW5qbaKxXsGyfB0hMNBiAgqQwqGh4XXCK+R6MpIwFQp4hkAbts1DZlRfr9yuUkqVSiQHBPb0nv3GqwQrHK3kbTKmpIHu3r+4/Xt//PHj3/+uLxduqMKl1QQVeHeDE/n9cvz+W+/fv77b1//+vXLl0+PTw/L6SSiIZLSqs5KmqmZHDXyo2STkEwDa3wWZ01XZYp4FItRaU3XBU+P50/P2y+fnl9/ud2ufXfvcclAUtBa9wzUdEjtmT1y97x1DwJGQmLrzWQ5LQLutw2EKHvfI0Kb6qmpaUNmZL/5nr37rqrWDER1bQbDR095du+jTxcZKRQpWmmPPSMj3GgmrPqRJEyWptbMwJC0y9vrvu9U0VYduGNeYHbveweFgpvH7+/X77f9tccGVm2GUpShmk35QPnLp5d/++3Lv//t67//29ff/vL5+fG0LmyWYkoJserZl+qZZfUFztIY5kgLjTTmLFFiHnpAQYSoIHNZ8PiwvjydP396vFz26+X29n55v948s8osInpNsahhM92ze27dnRlCJbMnG9upjSoBRDiRPTN0UVtNm1mG7z07HOHozbSdLJFODw8k59ij8OLgqhA8QZFkUZeVP3EVoZrvufkOglATVROImujrq+99F6qwetArXaXRo3cXARuu3b9fbj/2/T0ygVVERE20MZr42uTzafnrL5/+599++x9///pvv33++unxtIowRFyVkBSjiNSIQ9XRC0Xcp8CANOEsha7wVIy0HAHUoRUigqXZ+dSeH0/XT4/Xy8vr2/vr+yUv/eIBiFVkn6XCmZLec+9+8zi1DMmQhIJGqsgqTGLPIia5iCzCJpIakhqmu3qInuz0dKLI0td937frXmRqPVX6KKsKQpgpcIQI0oQJMzW1PXZHUKTQOJtoNlWyaWxpTbWZmGQxYKm+O2fL2PvWL923qi5QNYplLtFXw8Minx9P/9vffvuff/vt73/79evnp6dzWxdZV1NNoQYCUrUcUmkZjLyqJATHeEFRG0NHj0JnagVjoODuEkjh0ux8Wp4fT32Pvu0/f77+85/f91v0HgmGcPeqiE4A8AyPvcfWfV+gzJCEAQYqZVG6jIKboCyUhWIVkISlyi502snWp1Nrzd1vt5vo9Xa7sY9ewKrarDqcwrXBUBNpmqA1M1Fs6HRTkUVtNZoIqUppEpLaxBZT05QUEcasCAYDuGz92n2L9JIQsEU05hny3PTr4/q//Y+//c+//fXXv3x6fGinE5dFllVbIxC9d0iShNYB10CTqi2Yc8BIIm1WSM2izKqS4b18FKIAaVTgdFqfHjM69uv26+eXP76+efzwt1uVNSwqe4ZX57HHddvfr7etoiyiou1KGYiJmrhTQkSpptKK9yJNdLVlaxnd1qYnU1MGQxKEnRo9o3e/7Z690pIioqayqCDbajAFctEmoN6srctyWtfHUzstM4OQbKpLE2sc7QOjXC0pUAM0qO+3221392TCMhfmSfjY7MvL6be/PP/7X7/+9dcvv/zy/Onl8XSS1rCsVCMUSIpJ9RCO2qajOLhQHeQokbSR1jhQUEViEFKqvHHcVVLI0xkZZMC3/e3Ly4+fl/drv+yee9y2XFURCPeM7BnY/P123br3HMV4IKsjUKp0pVNCWJWEpik1+UmM1k4toreT2apiypCmkKZLpARi299+vKLfwpMKMZGm7bSYcj2vUBFSKempF7PelofT8nC2xXIU/LjUBTSlCUxQOeKq5pWWlKC+X2+3PbynEUYszLPgebVfXx7+/bev/+//17/97dfPX788v7w82ALRUEux6uaoevuCMkLKmK6Ee/vArA1OqzhsFqTOycmYBbyYXQcCJrXZcsJp7w8Pp5fn86dPj1/erpfe9+9vq9KBIHvl1hI3z7dbf71cvz49V+V2JioTh4jqWUmdMx5UUqhRLRhQ02Vd2rrI0tQskdlHiXylENc4QbNvmwirMcwWSxU7LynQcqw9ZGnmi62NJqkiIr5npodAFoMpTKFaFxCBPbGDAf153d4vt+y5ipxUnkVfzD6v7eunp3/7+vnvv/3y17/88vnzy8PjeTktYgEN0aSOJm+kj4qjO7ivI5cx4rmK9xI2W3Kymk+PptViw2Y/C0Y+y3RZ6Gt7OC9Pj+eXl4cvl9vbvn9/fT8bt54u3CFCje6b59vmr++X8Jeouk/PmiQZ2aswpn5z5eFSqE0rxaiqsixtbbqYNMvR3wpGwj1yP+vZFtluSqC1JmZiChVdzeFilj0yU1YzrLK2VAmlNa1MtRO6tFRJLYACigZj87wlE/L97f39ssHz1PTZ7EXty9J+fXr465fnv3/98ve//PKXr18+vTyt58VWhQrEqYVdEiA8EICDAGp+z6yWG2AHg3QpDchRql3yP4upcZS0jwbgoIo0Lms7ndrD4/ry8vj5tr9eb9++rfRQeJLdw6oEMbFt/efP9313GyJWQx5lWjmtSd6iWoopomJApJlBta2rtqbNApECRBKg07JJS1EkA0BbFmoNhxNpClCbJQOQtq5B2rA2ClWauDMIqaM3paqAFMPe94jd4YhvP372vS/CR7OXZfls7ZeH02+fnv72y6e//vrp6y+fXl4ez49nbcJGaA3qKW4sEkSw5prcScdRmF42JXNKuQ2z/KEmPjMRAa2DIqLKjJjwKge0xZbzcr6dPr88vd+2t8v156cn7K7Ys2MHWlJBTXjvP3683m79tEhK5YmV1GOMpohBSNFMjoa3StA3U5G2LrY0MUX20YEsUDNyQe9gqCsStjaqpFCa1dgNNQNSmMs5US30Szv+qu97UY9ScaqogKoGiZ6x99wC3759Z8Rj0+elfVqXz+vy9fnxt18+/fbr57/88vnzp+fzw2qnRgUNUEIC6DkSSffDxMcPcsx+itH/nZnGaXQKH81GEWAShAcmqr4sKqVpW9v5vDw/nT9dzm/v59eXB3+/SiCYO3AJX0kn0eP97Xq53J7sIbqP+pDh4ZUSyAGWqw6u0kEZSUcza8tiZjDJSAUY0Gr+hIKJDK3C+qWhXEizrLksi5II5hJjTpetS0XNYjoConL+RweZMJHufd9963p7vyyUpdmX0/rL+fTL4+nXL8+//vLp65eXz5+fnh4f19Mqi0ESCliJ9mDzprznvIMcrVEcpOJwAAMFVSXpcNPH6Vclz+hmn1PfMHOJFNN1befT/vywfno4vT2d/fkhewY8wK33XYQqHe6bv/58//z4sF9vannb3ZYVYqKj0CbGZAVKxTA1eC6lmVlrogZhMkWrtgnoPR00ZYY1TYg2o2qqiBmEga6mtZyjxjPW1MdE+t6VVYtWTfZjZE4iPbz73vdtv+37ri3xZT2ti/369PDL4+nXl6dff/385evL8+enh6dzWxeqQWtMgoxdHdUCgYD7kPXyBzWKhHlcyZgihCTShtstgHRvKApkNe+Nmrpy6MOTqGjTZW3n1Z4f1veH0/vT2d9u22X32ANy2/euRsSbR2zx8+f79tVf3y6isW1+fiCpFKOMEtdBiqpGdc4kyDRVs6aqUCIhyohQgYeHihKJtGYJ0daomrNQJQJqIpCcfQqiuiyrD56i7NywQaNtlgiPfb/t222/bX23hfKwrg9r+8vj+cvLw69fnr9+/fT5y8vzy+Pp8WxroykEUECrd0IQyhoL9yHkup8n+MEiDd6PSIsIMRu6UtTh0A6OfpXRHDJpDJImSJGNbbHzaXk8t5fH8+3hennctp6OiHwgRW+Ujt799fvP79++f//xjZKX6/apxrxCBLZv18v1vZ1OyyNO2kioKOPoGLTKaRIUk943MsV0kdUiXWTfeyatGURhVlNykTQlqno4ECnarC0r9j003CGqEclmlGrEVxLbfr28XS6vb31L9Hw+rc/r6eXx/PX56dPT+ZdPz58/vzx9ejw9ndt5RTOoVPVgHdqYdTa6/cdMsWFJxkCA6v6ompA8/rHZk0ZkHfOhK6N1uSxXIK3oCgFEkCLGpdnptD6el9vT+fJwuT5s79ctEKZNKGBwi+/79vP7z29/PPzx7RsU1+tWOXimCrLv/vP1/fzy9BRQbawJmCkyfKepGhSJUJOInUg1MZpmdrLtSwTUjGpQo6o2I0QFKdV5I2BoW9rSIjIsIkKk5ofXKJwaLYrNr5e39/e3d7oi8HRaP59PX5+evj4/Pj+dv7w8f/78/PTydH46tYcVZjCBjh1EY4hTpXWO6VMYSbZifmbH2WTYRr0NrZJAEsHRfIuKkhBkoGLmjBDI3buXr68pAk2X03I6L09PD/vul9sO7laUMFbVNIR7v7y+bbdIwbfff/z16+3htGSP6/v1x+8///GPf1qzr7/+5j0qJV0qVyiJSSZSayGCll4KKZ7WAFxAWGsQrQsgxaQJMiW7BMWppCjFRIwMwjPkP//zn79+5bKe666U9M1vb7dGO51Oqz6uZp/W05fnxy/PT8/P5+fHh/N6WtZV1zp9JoNSZ5IT3yfSkTVFh3PIx2zvi9kNMkCPVLu3RXUJRdTUvtFkEjk5Ls6+xnuMPIoaRKCipsu6nk7b8/NDeL693yKhUjOtT025Cl7fXy+vr7ctkvnH7z+ub9cHW2LP29vt+x8//vGf/2in9r9HhjuoaDJkZM5+kik3NatGRRlBpI40Cq0ZaLVggoSJIQMCZ2cV2mi5HBM4YeHyn//x+7o8fvqU2YRQSYktbm+3hfZ0Or+cn89Le27L58fHL8/PT8/n58eH02ld1lXXBaowSfQ5gjgGwMnqr49BfCXvYeyBTUcH4CA+I7OWkRz2qhxGgjlGatVMlo94Niu4GEPy1GRZbF3b6bxsN39+fOh7EDuForSNTbEoX7dbct98f/vx4+f372ez7Xr9/Z9/fPvj+3brrz/e3t/en18+paRv3cwyER5pVSQwzGLF88q5iWiMwYCqUS1qSLWImUXvNbphDODgZAEDscfl7fL64/L7f/7+6+evJztLdXw4z3Y6Py6fnp4+nZ9Oiz219el8ejivjw/n88N5Oa26GMvl1uXjHsOOJ8yBMUfMxelP7zaoTP+4LyBtXN7443xd9c9KIkDhnIEwsRDGYHpRgWlNZj6d+n72p8eH7bZnUjVq18li8nBeT9sGfXu7vl9+/vzxx7fHtlwv1//6j398++NHAG9v15/ff/72298jcs++qEWO5iAONBBAEGPafWUqHVmhtaqJmSd6Zo253mvQ4h2BFz7M7Om7v/14u/zc/uH/9e9//funx09qhIcGnx+eGuTl8fF5PZ9ae1yXh2U5r8v5tJ5Op3Za2BQqUGA0iIxqbN7Fc6ZQDgoo7wdbMe0U9CzHa1m91OWms+LieVsRY5LDHTxxvqNRQsGx2seWta2rP5zX6/kUSdtDuqrKyRdHrNsO8KS43d7evn37B3i9bt9/vF5vHY23W//27fXyfrXWnF3Ct9u7mZhKLK2mOU/SPImRvthjj0hVSg0lTTC8tLWmjFKEjKE/7uiRvffr9fs/fo+b36Jvb++8dW1Bj1Xsy9PLIvKwrI/Lel6Xp9PJlMvaltPSTk2aQXWS6qAIug8ibXjd0WU85sbP25hijUr4zwisCJ8xPT0C5bcw2bjB0HF+8yDw5jeP/xLVc2PNWmvrGufT+vBwyhRV575ba9WJt9xuhnxZ5NsPv/z48fbzbXe837agZmR3/PH7z+9//Pz0+SWjX358f/35+9Pzw+m8LOcTgipjlkFGClJVJKV7zwjaGGotyDI0vXcREYWmKoM1oH3bc9+x9f3t8vv/+g/t0STj7crbpqsz8qEt68unRkrmydrT6fz0cE64NbO1ydLQBCOxJQeWYRnsap3KSaJl0VaZs+hirqFJ5FCCUY7qUdPTk5VaSqCWUQ0fnJUCTGYVtOVI549bKapPVHQYolzX/eF8zhTIDpGVELPd90XlJLzeTLP/8ePHz/f3S88+7ppku7zdfv54bWb7drldXn/++Ef4y+dfPj88P2fK0OLqC8wxHaxv+957a0NBh2hmFplaihLd0xO03Pa87Xnb+ts7btsvj4/nhWexJWDdCVpr1lYFYt8W0dPSzqfVs7e16dLYhunHaADNe4yV07JPgjMj70qAGckerx9DwcvkxFzgMGaNygciaXr2sZAwamXY2BKG+aYTlLGRyJdc13Y+nyJZBQuizZb2dnlnxEOz66JLk8fz+dvb9R/f3r6/XTxC1Z7OD03t8vb+X/t+ef2esYVf3pvc3i8VjmdEdIcE0wFkSPTcbrfr+2WxVSH0ECWFGY6ULDa4x/uP19j60/kRPfy69ct1f7/++1///vcvsTb9y5dfnpdzq0k5pkpqwDMbUWuWzNZ2WmRRKPPYt0JkeJHLeRBoM7gdtiNigJsx/Wx60RwsA8aoHhnDuyOD86TnrX10z9VTDRZVOSf3AaheZBFas2XJZW1nRwRGWeey2LJ43xVYVc6LPj2eHh8ez+eLO2+3jr2r2tPpYbF2fb++//z549vvTXNdcX2T2+WK0QDtGXtKjvmQVO+x3bbL++Xx/CQAPFREiR4+Iv9A7nH58dqv2ylVU7DteevY+r//9tdzW5fWzm1hIDNq6pySmvAMRSqhwnZadF1kMShTsjLqWYstQ+4njplSyRF7zfC2LMqRdByWf9KcYNWGzuzYhEwVLzABmbTdiOHGPUeOFPO8JwrUaE2XxbzHvuu6mIiYNpCn1pQixMPSPHbVZW2nVddP5+c/Xl9v4U+n9XxaEaHk08OjMU1Cs+2X7fLzVfBgDZRgZiWU3Dsh+7bdLpd927K7CqVnZl8oeyQS0cPfr7fvP7fLbePysJ4XaHt4bH/57dTWBmlqRtYSzeqZM5UG8QhBmAoF1kxXkyapc6B9DSS8g/oxYgiTUb4X9RyS/yGAPTIt07PSRrnENGATM03q7iOUGy+qWoQ51HtkkUmlWS5L63u01tfFTKFiHrE2UzXPOJ/X8H21Pc745enz1+dP/99//POPt5/Luq7n9f22UbmcH5RgdAX2y359fTs1NSo0WLIjCHdS9m2/Xq79tsNdIJIZvi/LKXoS2ffwt+vt++v17XLT88NLW9XW8+nT+mCU9DBK9h67ExVqsIk2VQ8ffl5gi+lqaJJaU1iIj4smj71TMd3mOPFpHkaq5RhjPGp/MYIvkGr480eZlTtzV275+Jtho/TQk3v8VojIRjHesph3ZNJ7nNf1tvWeYmIJ2CLRwwV8YmQ+PJ64WpgwAB37OVge9Lp//8cf17ef1rgscn5YlsXEDJQMub5e+3X/8c9v//F//6+HdWmaJnzLn85FZYEnN//6/Ann509Pz+e2KLWpiZKRIpoeEIVx0GS1+wRIUgW2mDWlVrIF4x8ePNpHpnOeDg6h5Z+4z7zL8fjqHGoI5L9cAP/0p+P071O/ambvzNdj5g9m3F3jFk1ladaJvgcizudzRDJRJdAizbmnYrG2tPbcnzrzrd/gniIRiOgMIcOv/dt//ZHYQH94XD9/fj4/nKwtSfE9b6/Xfu3ftj/+f9I+PT4shsfTertttjy1djaQnr99+mVRW7RV4Kpipurb3tR2v1GVZHkN0fLiFEJM2riAaXlG8HUEUId9v8P6D/dxMG78b2c6AriRvQEsjyti1S1gsA33wHjebOZs5c8//bxx9qAMTqw1rRLX9BBhDfKJY1ZdpeMFmkxCmvQMM12oQUZg957eM3vAe996jW2TfpWrX3fRhSKEYcOKJinYEgtU1EIpq+nSaAYRxbq0RiPG7CZSTDXF1TRcM8YIeI512BGZoqxBONIUTatddCzFBapbknG30sVGz5O/y/205B8+LyU4gFIGALsPaD+oihyTpTPi3kcxxkqN/MCdpx6Zh5KU6gqXXCyTGd0FtZ9XgPsmUVSpIyPTkqQ06tqW5/W8Rybk5vu+b17lVxv2zICoS3/vt9cLoKJ2Pj2Z64M9mNhZTyvXh7aebFnPTdpJaZLQwCKNNQ2K8IwEjeoy1jD7tveMmq4b4d07QGtiaxNTacqlKh5IEQgya4Z4wP+UbznC4CGsh5JMzFPnlvdY4e6OywRx/IzhVMa+kxxoSXLAurpAzp81DBR5zJ7k6LI3MU+fhQq1iWQQMmPT87jAmhVSdwKxnplg67qrenR362rua8JtESD3/RaODCxop0XPcm5mT+eHh/P6fDrVdmaxRSHwpOdCzQjPXjJSLSI1q0pEg51jpPE8GEJMbDGaYBninzIIseOMBgkqI9bFpDf/ZMtHjHbowAeMMxmiiLRJVx8vq3VvYMTIUjKOxOadAozJ6ZUBEozGSgnIMa+2AhzZ9w1RjXgU0kzde7iTVBUmI8IDRlEiEiHSdEkYcgEewBDBspoIe+/btl+vt2U5LW01bWt1xTU7n5Zqc2ttVWq/bXCXrG2yiaK3wYwgWft+QZipsjZDBIUqUnsl0ch2X98yMWadtQyQOafr3g96ZgY+Wv5DrGfEdkgvMtPKDk+tGPoxCnjk4FcTiBouOJQsDs9bsiV1QbW7OjVlbPGVVNn3rTIKwNx3V9rDGlYvFd8ZJaXIJ6cKCamNqAozXU6LmXr45XJ7fX01a+tyOq2n07LUYLPTuhDo3ltbTSR2H4sTgJqHX8gvI2v+JsQ44cUYAl1rBqpe2gT2AWpPszIB4rQwnH7zXz8+UvgH23CwQMf4K1gNkMZxznWb9e+kTMsHTVXLjBgJ/5zEbPWdVF2vspJFairmEiKmuXeKRnRSIzqyVuyJUDMhNDaoWSCbme4VaVeMLRAubczIjUxCM7LVpi4RE2k1t7gtJLBT1wUUvN8ia2CM1oKCmsIHDiVIj1pNUEJhrVVPkC6aKjCBcGDQGQPNk40h8hwVh7yb+wMIlb0ezbg1aej4MSXiHnUBGQotg1RlUYfBmvEe5iDjO44ddrA+qW7O8KEBKlUzqya1NFItEikyxsCGVxFWmV+pgcy16wIVtVXrUi0JJiBcra22LMspBKRkYlEzkfQw1WaWoLWlzDTbUppY4TvLhY6JlGUMXFTHmrvuEFiz1kyUPV3M0oRW7ldmTJsjxzIs8JQ8HMzkkGkOtD9uaBiM2Rp6MEY+l+6NfWlEdasOc35YmpyU6rHYrwjRjCPc+5PaUaQaDNKDkmJCpyhRkyehA0VUE1DWQOlZlp0jKj9mGSzLQpG991qNpNbEFNTbtpvasq7MUFFtDQmIMZ1q0Xvx42g7YgSidzid0btrmdsYpQxjJGMTJGGEkgN9Tvc6zcIB2Yc4TpbtTjIjq1VluAQeL0AFQ4f+lAAPNnQ0L02EMgwVcFzAkbjk1A8ctvHDNYgKEjCGJ4Vqwr1S68PIMrB7FxA62+LKDmfJOoEcgSd5WpZSi0Wt2WLasK5my/vbu6mu54caey1mSEAVPUWx77vUAi6z3B2HCy1xjPDeUenamgdcE+hM1IwlJLVZbO4buKdkq4JsFEAfhz/kcOKcqDvIOvxykgVnYmjICBtI1o6QmOSPlPcdkdi4iVKEUWo98glABsvyDDCAw0VRxn6dKj/UphYxhiJj7IacuKCKQzA3qNbUsJHvrA0JAR+9Wa1BDWY8n5f3N82kSQ22qwgLo8c3PYPuKoQKYjC4UWN/eq/BrmNaKGuJDceWDROQLMszKosGyOa0JCUmGB4zBzgljzTvn2KCnJ8NUijneR5QCBYYi22B9Kh6CAIzITNOdujYnwBA/fr46KM4CgIkhgdzWrPwbGZl9EfXdZYrTwNF1LuDrFosYq56EI1wjAIe5bJAFdrw9LS8vmK/UcW9ZzhDaVYI28M9kt5Fjap1AfSxmWjb9+rWr8A2J2yrDtIxQXFO7ynhzhkDT7eHO9l5J4XqwMarPrrinEurhnaUbx2aTxQVEUhJBCEzPOM08Pmhvvr+MaOJIxz/YIOq/Loqn2v6FilUUySVlr7XyzjAdPUzpaqMG8oO5L1DDQCx9517J6jhxZvh2GBc6/vCI4MjQSoiqIL17NHDa9Rq1Hw6ZBXjFgxgjXce8XrZnzGmOZmzNu1wnjklcfqUcq0zaX3g+/m/qC22dwtRkVbeXYhhxsDHwcrBexyXMQzdh5eO357/rWJF6uhEGc7qeENZ/xQV9b2Pu9GRLe/VYSyDC+teO19Rqq0iPeN2u+K6aKY0Y3cgK5pVkTBFpPfuiXU5gTWRIkCKase+923bt+j7OHqBFIIZpQuiptoUSjDL9KcK5v7k+UYLOwbmNpjiIPgvnvkuoVUwERGTQZixck5vgKQgbUDUKrHD/adhcG4fznvcwLRId/6PH343hp8ajUeoNhtRMmgqOwevUuRieNTgwckp1so/DFN7WGGiJsSj73m7MJ1S15RSE44rWDERgM3Qd++9RkzO/SIxNAYpKpgbdklqUzEZxGfxbhUEDPxzBK45gfifZe4Izu5fygOV5mDPjgM6lCArFLXuPgRhFC/V1Y4TL+UbpdIjkTm3EIzrmeV2QzmjoIUIU0UNtXuJQkaqzHYwZCAF8Iytd5MWQ8qqdrNmdksgMrO2tNnakpmx4/0nvbMJuk8qHyomaliM+wZh9v329ha3Pfa9QoohKcJEiDIBMTGrbgLSCKtSZ6DcgBylmPcF1fNIipWZf86jveWY0D9DpINdqL+cxxYF6hNRcYBH1FhtTJGOGiQxrz8y5XCy8xdMGxUYZaNTWyMAFjuXKgMO1VTUWvrY1Gtk+hwWJLSoAMkxagNqikMmmU1lXRaauDui52UHAG2ZzsouUaiCxSDITEbk3ve3i/des7APWaIwE6JS1b7WNAnRAT0x0i9zn8eRdx8yPNW99r3cq96mRxxWY8C8u1U+PMLUkhxl0IlqUx1fG8iwzNzYP3DYPqdb9dTxSGDGcAFjL0D94hiqOvWk5nO0ppEBoZqsXPbe975FVltujWXJMhQRVZYwlfsIRD2qaji8d++aXgkS3pe9En3PvqN337a+beGBrKmHtaihXk4zTbDmnRA58i11BxWyYFjICXxmHFEh43TFFabXwLupLoOXmK+6E50x9aHisPEHETsADz6EXQDnUY/vI5mihyWbaHbeOg5zmTlGwE1QKjUvwXp01GpQ1RT02NO9ABL7KD3JTPcxxG0iMAI1tDjgQYkM3283eLAZRYrkGCZ323zftYff9r7t4516WbIka+Q61TQTA/aQqTPbrpL3OkzwQy/dvGGOKOw417F1OA8Z4FQBzCg4hxyj4P7xASAibISEHEnlPOwL7gFaZo3gQFb7dh6njnLpo3EDw9rnDAoxMFV1Zkv32LNXM7lI+Yuo3bWjJt0jKzMiY7Fe1D6soNWmD3eBMJAeMLrH3JHS4dgvV9/38PDtlt5ZiR8RMGs9xLrUzhzxqCXALmNsJ1KSAshRNFIrjmPoQCYjhxoNCeXUjmF7P9xBYfSogLJcW00JjgmAhtlKjF3B+FeQeTiA1NpbLmMXVg7jdzzYeNZ7yD2MXsXkEEotW5DUW+97uJK1lKRm+gz/HyDSY2xO4e6jU5/JGMOl13UFXbQhhgmsRewqmdGRvr9ffd81s/ctey+OjyKV2lPF0lrxfgEEwyOlKbV412p3GatcCyXkiF9LOqapyURUpKJ3CJTD8gz/O6EMJ0lUewUy5+rnWp0MsVFJKRQ52oRLru+eBIdOHXEfOJ0BZ06TI9abxRoYiwpTVDPBgJiIS71AVDS0yODIJAKs+ar1rKGo3UbeIwh00lT71iGb7ztb871v217Z6CJ8fdt9uyXovhfPgRhFmqpsVh3xqOE0KhJMVNNkFRwWniNBZmSFYtOqH/Wyw19CDut0ZwIOPuZeiTJR55gBNo1PuZBM2L7vtfE+JVOHNtUsyBjjJobRJybvVDR4crQ04QMOi8gy4jMJhYSq9qihDqoevnsgKy+bGki6R01qHIpZZDmSNQsUUYM0ounleikRMVHP/fZ+EZEmmpnee+xb3zaluPeq4Jt1OCliNfuJVeVaYJ8J01ROA5CHFiMCRynIOIAc0CCZGYyEzthp0vMDMUy1yEMUB+4fe4FwHCtp01ojspaSz4se1Q619rTkNCSPfgF+cBbHI84bzuEAyoiIiDAlIZEiTJGYTVWTrx/xe4yF0xkZkp6A9z3CVQV77Jv2favfswtF1LdbCLtZIsdikd5BRniNBB2NWzLLAjAmbg18NfMN0A+qzVkMeCda6vvy2Bgxc4HTvX40DvMwjnajQ/orTqpzGWYu0kaaGqwZheN5AcwyHpARQWVEBFxZI1RmVH3P28woccR5ExFQRCkRR0e0miDFex/ToFUEjD1iNq4BqCnDmfC+9763xRK+bYB7Jrr3GmYXfSO53y5j5Xtm1NawzIiQWpBonCRfRARTC03WSuISXYrUTrBhTcLnHZQdGxcH1JqwQZ9Pe50TLvN+X+UvD3uaWeUMNZF0yjwi3A5bVQAyPyxXP9K34zXlwyn4QFJ90IPp2yeQHcHjaGfgzE3V9ioJZwHDBGsCQrVeQMvNFU5BbQmsY932YU9735kpadF3ALvI3M5GD9epnYnh5Fl4fWxT42z0w2yZv6e3BviIAYGGvx3ud5rfQ+CHQb/jx2H972qTE/dk/kk4jws4ytOLS9Txo3NgnXHQInpn+IaMTAUo0Z85I1a0T4l5r2WDx3UKtSkikRoW9EClIEAqx6YJEvVdJctMKiMcQOzB2uMTDpGapZ2ZIlJzVqni0dUWIGvy2DxOyNj2g0q5iZI1rEPH9tx5bNOAoIYOHFI1UAaYrMhzyN3czAmMaxjYk3EIP5Bg9eyPPcXzRiLDMjKlFFbyzyn+GZEFj8A4MmUSRcP25UcYMEMYDMuU0+nU1ysI8tTy1j26RyKkZroKhJpEVSwPYyIQ1h4mJlypGV5HMjaaDi56zBQZVfzFuxGodX2luPUUQtSYAhHes753sb0f93QZE5t/aDyaEeiBiir4+ei1a59RzPirxrSXazogfmRaZGrWAkGGREA8B0xA1ojOMb9chR5Bd6ocxof4mBoe425rryvGZJrZukOwBtQgEtBINZNaRZ9JpkcoR4NrUUc55lhoyoyqVcu1jEBKKBQxrUunVjXSRI+ciLrAbWXc5secoscPsl+GZUjbDLkwWYUpXCPomXcxXjJqTubZDqh/l+UR6kcc1okZCRuGLpKMiNFejghQk8kM91DVinkyIxghIdTjmac/GNbfKzkSoKRIDQm8awDBBBVMg1jILp1IhJIeSZGsBFGVNcSonpimA6y4GfBtH1CtZnQUyDCTevi80zFVjhBjaLuwxsOPSZ0yjfnhST8iz+qenvdwD/br/+vgCzQN+zxWhU5rMSIizBupapS78mQeVREFmmVgt7nhNBAogv6DyE/PPKPeQ0YKT1CE4cPPx7R0pQLjDcgYFju2aXkLdkDCvYdXEoMl6DPurmhUk7uHAOGRGUKp1rXNe8SocS0HglEBVk2sVXguYzSBKkezoxwtj7M2825cy4reL6NkfTbb1UFg3HKOCBRBpFQsmUckxkx+7OKIKHa/ltPluIBB+Ut9Mg14jl2PUwKOLNKQlIOdPqznFKuYeCgG/C5XMO8OFJGanahVB5CZsWeEDzQ2snzj98YQRQnvSuneCwcWSx7eu0cSUpmySKk19YCMckMV0zEYQm3Q/YcJmvp5pL8++rJ5/nkH3nLczWQmBrqJ472OYp8RTE8nwimM5eIiImBjbj6EGZEyw7fyrQQGNspEeJA6sc+ffdG4gfGddTlDEAhApw4jRqyfI0AztUQHwr1cfcmBp49PMQKZDKRLZDbR3nckVCfeFqmlk5qMkfsb2LdslJqK6pieOiYjVfNnMGUkXA4M8QGHTHuSc+znYU0n53nEQHm8EmVqRhP3JOgDKKRQL4zZX2PFwYMQkZwfAGpby/0CkOFJyYGUCo3hSIrew+BBgWYUlhyiVYiwTKJI0VRUqX7Swso0qXK9FPSMzGBCkB5eyMGTEZFqe98z0lIH+K6+LWpphIBBK/mvdIQ00xrTVJUQhTEDeuRVh4LPQ/5T/DNCWPzpIyfCuyPz6ReGBsRhQrL6VmtvGkZXRwUpVZgVeW89G9RyovB1OZzIZMIzGHJYpDkOivnh4Q6Ee5jG4jg4QNyh3kfeeNiJqmcAJoNSlrJWE1dtAaWANYUVMPdR4TLK+kwQLERYo9I5bT3H2FL5UOj5L8f7p08PozQtfk6jew//c4ZBo2p9YqkjrMqo5YzD5swdyzHVYmqazZTT3f8cenA4W48QIjK9ViBEiEpO+f/wDj78/gM7HBiEh8TMC+Nw2hSiYrQRKyF6KUAULVAJ5FrgO8rLyAnRs6AQVIPISFVyet0xTKy88zjGweR/gBKjZIrCQ11BjobeIWrzgO4muC77Xz+phbZ1CB7pPtifzBEEDF6pEAppGPt2MeYSjG3Ew4pVdVI50DJ6cejdh5j8fqRDeydwnv0EkS5i41uP0iwMcFlZs+NNwJNZBhFj4mswZQxDHSnT6TAruaqjh+LANoV8lLOxdHrRcRaH752Afx5p8ZR1AVMC6439SQPuZmv8Zc644/DAFXkV+xETzkRkjLWNoybexlCC45/qmCe98n9y1KqJEBxlnDhwc47Ae4aZOsZOMmNA0kBEeCRVs3KL8Di0gAAgprMmrAJtUagII4Je4uAAqIqYafjageQ90lkyRWE1Q42aTkrlemaBSU4FjQzhwWvmcZ4HmB9NuLgbmKkX+HjuIzU1wXklbyLG7nn3aX9GQqxKhSKqwyzSI0kcGjChfxYSSo+oJpkPKdKRebkTtfMG8ijHEKnk5oDhIFnC4NMrIGIOoJ9Dd0RFQgqeSQhQA30lIrxE2OkRKlLLYUBKs6JvRz6u1o+OLkwZFmYsLJU/lY+NqGI61rLHPLQvOQP8YfrrxMkPivKvDnl+C1HbmIoamYizPvdavT2cwVi8TcImCSHBFA77OqgWd6FkNadHVKePUPTjE/zZp5WX+fiFaWkOAYTnMSeco6CFNeya0JTQFGpt/awd6a70jt5ba9u2SSrJZVkiU0wsLDObNTGrhseqzUsSNXF81DdklYiVFRzm/yPQr0qfQM5ZhX+W8Q+nf+TdMzOCeeCWcb/VcTUtPiLRI2o3/bRIQy1EaFlVskPy7+IdEenQaigFPBwRAYp7qA6oOsP9P4nX0IWJyD6EaoeZm6TM8d4pxwZVVUmQIirhQZGIYJcE1tMpAUkB0JYlAAuL9Ii0SUhUkS3H4LbREZjHox0nXsZ0OrY7uC+cPA3LAbPv8vQn2RoiPjzF4ZZqyfyoaajNczE30A/nOrGKmEeQ7O6ltlrwn8Wa4F/k96PIl1yN02ZN88kcq+6Ldh0OeQCbmvxcIT3mPLTaaMPhJEWF4VP3axqUwqnAQpC0ZjKmYlkSSM05uayAphQdThktNxN/Hibz+O+4k7HUZ7wnkWEVZwNeZuYHWg6VxEiMJAE/3mzmPPgRCpQS+JTKcWMlhZHFEViEU6Q7IaBPO0coZZw7oZCAz2j2w2XkcfxDG3Lww3E88BSOUWgksypvOPIa/cPR7yAidB7GlyzyjCS0toKbKe20rvPIh9ULj0DWIGJWsaXUTOjxU1AkGAnmQW0hs5i+QVsK6g4BHCmwzOGIM8tXyEiGlMeaYKaw4t3tZv0pPOcPxwHxB083LqB+UGREMAcWmRoys2mFRhN5LB8dHyVWdZqTHWLNCWN8JLNIRgQOy8PD4827FLL4YhlbHQ58UbMJieotFYGqWV0nBQlhBOmIENUKN4QDtNXtTmriX2zIocrEYYpG0P4RZR/aMjir40sFMCYPlOl773vvtWaeh7DHsOvMCf8PbhJzVEFdS3mCI4C6n/Vh3catHJp0J4EG0SBkUkKSdzZpWrAIjA1NPN7x+PkzMBKRahIA7gV3FNEclcwZQVQfRbHKSMQw86TIGDoqH5nl+WPmDMV57H/CD9M0cQYBH2TnXyPlvNOK81Qc4b7vfd977+6e0MjImKxDDmsx9GI6ThHOYR3Dvhy27vj8A36bv+6/kyd/EqgP7/3ju8hBgnyEHnn8Oo4ijCGrRwxXJzOoBJEoKzwN1IxsSeGYiI+D1j8M//EWPmrkf3/u46sTev75dfxv6jP92wgI46P/+386GYxTxYwFCeD/D9iGvseDZsHyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensorflow-version:\", tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-vWTICVPE6y",
        "outputId": "ea338444-e8fb-4700-f8e3-7217e352978a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow-version: 2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#InceptionV3"
      ],
      "metadata": {
        "id": "MuEDtx2NSbHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inception=InceptionV3(weights='imagenet',include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "x=model_inception.output"
      ],
      "metadata": {
        "id": "5ieN9uavSd9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce137ea-009d-4620-8691-f449222e1b8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = Convolution2DTranspose(512,kernel_size=(4, 4), strides=(2, 2), use_bias=False, padding = \"same\")(x)\n",
        "x = Convolution2DTranspose(256,kernel_size=(4, 4), strides=(2, 2), use_bias=False, padding = \"same\")(x)\n",
        "x = Convolution2DTranspose(128,kernel_size=(4, 4), strides=(2, 2), use_bias=False, padding = \"same\")(x)\n",
        "x = Convolution2DTranspose(128,kernel_size=(4, 4), strides=(2, 2), use_bias=False, padding = \"same\")(x)\n",
        "x = Convolution2DTranspose(64,kernel_size=(4, 4), strides=(2, 2), use_bias=False, padding = \"same\")(x)\n",
        "preds=Convolution2DTranspose(3,kernel_size=(4, 4), strides=(2, 2), use_bias=False, padding = \"same\")(x) #FC-layer"
      ],
      "metadata": {
        "id": "plzqSpEUTrxW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(inputs=model_inception.input,outputs=preds)\n",
        "\n",
        "for layer in model.layers[:-8]:\n",
        "    layer.trainable=False\n",
        "\n",
        "for layer in model.layers[-8:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "# use tensorflow.keras.losses.KLDivergence() for KL-Div loss function\n",
        "model.compile(optimizer='Adam',loss='mae',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xktq7zNXTv0u",
        "outputId": "9844fbdd-f3bf-4eb8-89d9-84e78ce9f3a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 63, 63, 32)           864       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 63, 63, 32)           96        ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 63, 63, 32)           0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 32)           9216      ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 61, 61, 32)           96        ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 61, 61, 32)           0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 61, 61, 64)           18432     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 61, 61, 64)           192       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 61, 61, 64)           0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 30, 30, 64)           0         ['activation_2[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 30, 30, 80)           5120      ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 30, 30, 80)           240       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 30, 30, 80)           0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 192)          138240    ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 28, 28, 192)          576       ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 28, 28, 192)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 192)          0         ['activation_4[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 13, 13, 64)           12288     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 13, 13, 64)           192       ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 13, 13, 64)           0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 13, 13, 48)           9216      ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 13, 13, 96)           55296     ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 13, 13, 48)           144       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 13, 13, 96)           288       ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 13, 13, 48)           0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 13, 13, 96)           0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " average_pooling2d (Average  (None, 13, 13, 192)          0         ['max_pooling2d_1[0][0]']     \n",
            " Pooling2D)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 64)           12288     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 13, 13, 64)           76800     ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 13, 13, 96)           82944     ['activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 13, 13, 32)           6144      ['average_pooling2d[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 13, 13, 64)           192       ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 13, 13, 64)           192       ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 13, 13, 96)           288       ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 13, 13, 32)           96        ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 13, 13, 64)           0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 13, 13, 64)           0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 13, 13, 96)           0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 13, 13, 32)           0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)        (None, 13, 13, 256)          0         ['activation_5[0][0]',        \n",
            "                                                                     'activation_7[0][0]',        \n",
            "                                                                     'activation_10[0][0]',       \n",
            "                                                                     'activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 13, 13, 64)           16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 13, 13, 64)           192       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 13, 13, 48)           12288     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 13, 13, 96)           55296     ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 13, 13, 48)           144       ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 13, 13, 96)           288       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 13, 13, 48)           0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 13, 13, 96)           0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (Avera  (None, 13, 13, 256)          0         ['mixed0[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 13, 13, 64)           16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 13, 13, 64)           76800     ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 13, 13, 96)           82944     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 13, 13, 64)           16384     ['average_pooling2d_1[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 13, 13, 64)           192       ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 13, 13, 64)           192       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 13, 13, 96)           288       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 13, 13, 64)           192       ['conv2d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 13, 13, 96)           0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)        (None, 13, 13, 288)          0         ['activation_12[0][0]',       \n",
            "                                                                     'activation_14[0][0]',       \n",
            "                                                                     'activation_17[0][0]',       \n",
            "                                                                     'activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 13, 13, 64)           18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 13, 13, 64)           192       ['conv2d_22[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 13, 13, 48)           13824     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 13, 13, 96)           55296     ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 13, 13, 48)           144       ['conv2d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 13, 13, 96)           288       ['conv2d_23[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 13, 13, 48)           0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 13, 13, 96)           0         ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (Avera  (None, 13, 13, 288)          0         ['mixed1[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 13, 13, 64)           18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 13, 13, 64)           76800     ['activation_20[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 13, 13, 96)           82944     ['activation_23[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 13, 13, 64)           18432     ['average_pooling2d_2[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 13, 13, 64)           192       ['conv2d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 13, 13, 64)           192       ['conv2d_21[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 13, 13, 96)           288       ['conv2d_24[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 13, 13, 64)           192       ['conv2d_25[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 13, 13, 96)           0         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_25 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)        (None, 13, 13, 288)          0         ['activation_19[0][0]',       \n",
            "                                                                     'activation_21[0][0]',       \n",
            "                                                                     'activation_24[0][0]',       \n",
            "                                                                     'activation_25[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 13, 13, 64)           18432     ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 13, 13, 64)           192       ['conv2d_27[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 13, 13, 64)           0         ['batch_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 13, 13, 96)           55296     ['activation_27[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 13, 13, 96)           288       ['conv2d_28[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 13, 13, 96)           0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 6, 6, 384)            995328    ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 6, 6, 96)             82944     ['activation_28[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 6, 6, 384)            1152      ['conv2d_26[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 6, 6, 96)             288       ['conv2d_29[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_26 (Activation)  (None, 6, 6, 384)            0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 6, 6, 96)             0         ['batch_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 288)            0         ['mixed2[0][0]']              \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)        (None, 6, 6, 768)            0         ['activation_26[0][0]',       \n",
            "                                                                     'activation_29[0][0]',       \n",
            "                                                                     'max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)          (None, 6, 6, 128)            98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (Ba  (None, 6, 6, 128)            384       ['conv2d_34[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_34 (Activation)  (None, 6, 6, 128)            0         ['batch_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)          (None, 6, 6, 128)            114688    ['activation_34[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_35 (Ba  (None, 6, 6, 128)            384       ['conv2d_35[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_35 (Activation)  (None, 6, 6, 128)            0         ['batch_normalization_35[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)          (None, 6, 6, 128)            98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)          (None, 6, 6, 128)            114688    ['activation_35[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_31 (Ba  (None, 6, 6, 128)            384       ['conv2d_31[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_36 (Ba  (None, 6, 6, 128)            384       ['conv2d_36[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_31 (Activation)  (None, 6, 6, 128)            0         ['batch_normalization_31[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_36 (Activation)  (None, 6, 6, 128)            0         ['batch_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)          (None, 6, 6, 128)            114688    ['activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)          (None, 6, 6, 128)            114688    ['activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_32 (Ba  (None, 6, 6, 128)            384       ['conv2d_32[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_37 (Ba  (None, 6, 6, 128)            384       ['conv2d_37[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_32 (Activation)  (None, 6, 6, 128)            0         ['batch_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_37 (Activation)  (None, 6, 6, 128)            0         ['batch_normalization_37[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (Avera  (None, 6, 6, 768)            0         ['mixed3[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)          (None, 6, 6, 192)            172032    ['activation_32[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)          (None, 6, 6, 192)            172032    ['activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)          (None, 6, 6, 192)            147456    ['average_pooling2d_3[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_30 (Ba  (None, 6, 6, 192)            576       ['conv2d_30[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_33 (Ba  (None, 6, 6, 192)            576       ['conv2d_33[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_38 (Ba  (None, 6, 6, 192)            576       ['conv2d_38[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_39 (Ba  (None, 6, 6, 192)            576       ['conv2d_39[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_30 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_33 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_33[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_38 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_38[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_39 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_39[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)        (None, 6, 6, 768)            0         ['activation_30[0][0]',       \n",
            "                                                                     'activation_33[0][0]',       \n",
            "                                                                     'activation_38[0][0]',       \n",
            "                                                                     'activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)          (None, 6, 6, 160)            122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (Ba  (None, 6, 6, 160)            480       ['conv2d_44[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_44 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_44[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_45 (Ba  (None, 6, 6, 160)            480       ['conv2d_45[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_45 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_45[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)          (None, 6, 6, 160)            122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_41 (Ba  (None, 6, 6, 160)            480       ['conv2d_41[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_46 (Ba  (None, 6, 6, 160)            480       ['conv2d_46[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_41 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_41[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_46 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_41[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_46[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_42 (Ba  (None, 6, 6, 160)            480       ['conv2d_42[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_47 (Ba  (None, 6, 6, 160)            480       ['conv2d_47[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_42 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_47 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_47[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (Avera  (None, 6, 6, 768)            0         ['mixed4[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)          (None, 6, 6, 192)            215040    ['activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)          (None, 6, 6, 192)            215040    ['activation_47[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)          (None, 6, 6, 192)            147456    ['average_pooling2d_4[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_40 (Ba  (None, 6, 6, 192)            576       ['conv2d_40[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_43 (Ba  (None, 6, 6, 192)            576       ['conv2d_43[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_48 (Ba  (None, 6, 6, 192)            576       ['conv2d_48[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_49 (Ba  (None, 6, 6, 192)            576       ['conv2d_49[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_40 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_40[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_43 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_43[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_48 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_48[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_49 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_49[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)        (None, 6, 6, 768)            0         ['activation_40[0][0]',       \n",
            "                                                                     'activation_43[0][0]',       \n",
            "                                                                     'activation_48[0][0]',       \n",
            "                                                                     'activation_49[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)          (None, 6, 6, 160)            122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_54 (Ba  (None, 6, 6, 160)            480       ['conv2d_54[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_54 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_54[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_54[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_55 (Ba  (None, 6, 6, 160)            480       ['conv2d_55[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_55 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_55[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)          (None, 6, 6, 160)            122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_55[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_51 (Ba  (None, 6, 6, 160)            480       ['conv2d_51[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_56 (Ba  (None, 6, 6, 160)            480       ['conv2d_56[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_51 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_51[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_56 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_56[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_51[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)          (None, 6, 6, 160)            179200    ['activation_56[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_52 (Ba  (None, 6, 6, 160)            480       ['conv2d_52[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_57 (Ba  (None, 6, 6, 160)            480       ['conv2d_57[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_52 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_52[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_57 (Activation)  (None, 6, 6, 160)            0         ['batch_normalization_57[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (Avera  (None, 6, 6, 768)            0         ['mixed5[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)          (None, 6, 6, 192)            215040    ['activation_52[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)          (None, 6, 6, 192)            215040    ['activation_57[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)          (None, 6, 6, 192)            147456    ['average_pooling2d_5[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_50 (Ba  (None, 6, 6, 192)            576       ['conv2d_50[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_53 (Ba  (None, 6, 6, 192)            576       ['conv2d_53[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_58 (Ba  (None, 6, 6, 192)            576       ['conv2d_58[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_59 (Ba  (None, 6, 6, 192)            576       ['conv2d_59[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_50 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_50[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_53 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_53[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_58 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_58[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_59 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_59[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)        (None, 6, 6, 768)            0         ['activation_50[0][0]',       \n",
            "                                                                     'activation_53[0][0]',       \n",
            "                                                                     'activation_58[0][0]',       \n",
            "                                                                     'activation_59[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_64 (Ba  (None, 6, 6, 192)            576       ['conv2d_64[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_64 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_64[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_64[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_65 (Ba  (None, 6, 6, 192)            576       ['conv2d_65[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_65 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_65[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_65[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_61 (Ba  (None, 6, 6, 192)            576       ['conv2d_61[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_66 (Ba  (None, 6, 6, 192)            576       ['conv2d_66[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_61 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_61[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_66[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_61[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_62 (Ba  (None, 6, 6, 192)            576       ['conv2d_62[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_67 (Ba  (None, 6, 6, 192)            576       ['conv2d_67[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_62 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_62[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_67[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (Avera  (None, 6, 6, 768)            0         ['mixed6[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_62[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)          (None, 6, 6, 192)            147456    ['average_pooling2d_6[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_60 (Ba  (None, 6, 6, 192)            576       ['conv2d_60[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_63 (Ba  (None, 6, 6, 192)            576       ['conv2d_63[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_68 (Ba  (None, 6, 6, 192)            576       ['conv2d_68[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_69 (Ba  (None, 6, 6, 192)            576       ['conv2d_69[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_60 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_60[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_63 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_63[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_68[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_69[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)        (None, 6, 6, 768)            0         ['activation_60[0][0]',       \n",
            "                                                                     'activation_63[0][0]',       \n",
            "                                                                     'activation_68[0][0]',       \n",
            "                                                                     'activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_72 (Ba  (None, 6, 6, 192)            576       ['conv2d_72[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_73 (Ba  (None, 6, 6, 192)            576       ['conv2d_73[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_73[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)          (None, 6, 6, 192)            147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)          (None, 6, 6, 192)            258048    ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_70 (Ba  (None, 6, 6, 192)            576       ['conv2d_70[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_74 (Ba  (None, 6, 6, 192)            576       ['conv2d_74[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_70[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 6, 6, 192)            0         ['batch_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)          (None, 2, 2, 320)            552960    ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)          (None, 2, 2, 192)            331776    ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_71 (Ba  (None, 2, 2, 320)            960       ['conv2d_71[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_75 (Ba  (None, 2, 2, 192)            576       ['conv2d_75[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 2, 2, 320)            0         ['batch_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 2, 2, 192)            0         ['batch_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 768)            0         ['mixed7[0][0]']              \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)        (None, 2, 2, 1280)           0         ['activation_71[0][0]',       \n",
            "                                                                     'activation_75[0][0]',       \n",
            "                                                                     'max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)          (None, 2, 2, 448)            573440    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_80 (Ba  (None, 2, 2, 448)            1344      ['conv2d_80[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_80 (Activation)  (None, 2, 2, 448)            0         ['batch_normalization_80[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)          (None, 2, 2, 384)            491520    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)          (None, 2, 2, 384)            1548288   ['activation_80[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_77 (Ba  (None, 2, 2, 384)            1152      ['conv2d_77[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_81 (Ba  (None, 2, 2, 384)            1152      ['conv2d_81[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_77 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_77[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_81 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_81[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (Avera  (None, 2, 2, 1280)           0         ['mixed8[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)          (None, 2, 2, 320)            409600    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_78 (Ba  (None, 2, 2, 384)            1152      ['conv2d_78[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_79 (Ba  (None, 2, 2, 384)            1152      ['conv2d_79[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_82 (Ba  (None, 2, 2, 384)            1152      ['conv2d_82[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_83 (Ba  (None, 2, 2, 384)            1152      ['conv2d_83[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)          (None, 2, 2, 192)            245760    ['average_pooling2d_7[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_76 (Ba  (None, 2, 2, 320)            960       ['conv2d_76[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_78 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_78[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_79 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_79[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_82 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_82[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_83 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_83[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_84 (Ba  (None, 2, 2, 192)            576       ['conv2d_84[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 2, 2, 320)            0         ['batch_normalization_76[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)      (None, 2, 2, 768)            0         ['activation_78[0][0]',       \n",
            "                                                                     'activation_79[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 2, 2, 768)            0         ['activation_82[0][0]',       \n",
            "                                                                     'activation_83[0][0]']       \n",
            "                                                                                                  \n",
            " activation_84 (Activation)  (None, 2, 2, 192)            0         ['batch_normalization_84[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)        (None, 2, 2, 2048)           0         ['activation_76[0][0]',       \n",
            "                                                                     'mixed9_0[0][0]',            \n",
            "                                                                     'concatenate[0][0]',         \n",
            "                                                                     'activation_84[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)          (None, 2, 2, 448)            917504    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_89 (Ba  (None, 2, 2, 448)            1344      ['conv2d_89[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_89 (Activation)  (None, 2, 2, 448)            0         ['batch_normalization_89[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)          (None, 2, 2, 384)            786432    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)          (None, 2, 2, 384)            1548288   ['activation_89[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_86 (Ba  (None, 2, 2, 384)            1152      ['conv2d_86[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_90 (Ba  (None, 2, 2, 384)            1152      ['conv2d_90[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_86 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_86[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_90 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_90[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)          (None, 2, 2, 384)            442368    ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (Avera  (None, 2, 2, 2048)           0         ['mixed9[0][0]']              \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)          (None, 2, 2, 320)            655360    ['mixed9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_87 (Ba  (None, 2, 2, 384)            1152      ['conv2d_87[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_88 (Ba  (None, 2, 2, 384)            1152      ['conv2d_88[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_91 (Ba  (None, 2, 2, 384)            1152      ['conv2d_91[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_92 (Ba  (None, 2, 2, 384)            1152      ['conv2d_92[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)          (None, 2, 2, 192)            393216    ['average_pooling2d_8[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_85 (Ba  (None, 2, 2, 320)            960       ['conv2d_85[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_87 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_87[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_88 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_88[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_91 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_91[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_92 (Activation)  (None, 2, 2, 384)            0         ['batch_normalization_92[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " batch_normalization_93 (Ba  (None, 2, 2, 192)            576       ['conv2d_93[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_85 (Activation)  (None, 2, 2, 320)            0         ['batch_normalization_85[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)      (None, 2, 2, 768)            0         ['activation_87[0][0]',       \n",
            "                                                                     'activation_88[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 2, 2, 768)            0         ['activation_91[0][0]',       \n",
            " )                                                                   'activation_92[0][0]']       \n",
            "                                                                                                  \n",
            " activation_93 (Activation)  (None, 2, 2, 192)            0         ['batch_normalization_93[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)       (None, 2, 2, 2048)           0         ['activation_85[0][0]',       \n",
            "                                                                     'mixed9_1[0][0]',            \n",
            "                                                                     'concatenate_1[0][0]',       \n",
            "                                                                     'activation_93[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 4, 4, 512)            1677721   ['mixed10[0][0]']             \n",
            " anspose)                                                 6                                       \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)            2097152   ['conv2d_transpose[0][0]']    \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)          524288    ['conv2d_transpose_1[0][0]']  \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 32, 32, 128)          262144    ['conv2d_transpose_2[0][0]']  \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2D  (None, 64, 64, 64)           131072    ['conv2d_transpose_3[0][0]']  \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2D  (None, 128, 128, 3)          3072      ['conv2d_transpose_4[0][0]']  \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 41597728 (158.68 MB)\n",
            "Trainable params: 19794944 (75.51 MB)\n",
            "Non-trainable params: 21802784 (83.17 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataX = np.array(dataX, dtype=\"float32\") / 255.0\n",
        "dataY = np.array(dataY, dtype=\"float32\") / 255.0"
      ],
      "metadata": {
        "id": "ghTAsltBTwvt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(xtrain,xtest,ytrain,ytest)=train_test_split(dataX,dataY,test_size=0.2,random_state=42)\n",
        "print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gVy3yONTw9T",
        "outputId": "026a8277-5018-4787-8638-790fe7f9c2a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(157, 128, 128, 3) (40, 128, 128, 3) (157, 128, 128, 3) (40, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anne = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
        "model_path = \"/content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/checkpoint.ckpt\"\n",
        "checkpoint = ModelCheckpoint(model_path, verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "7uhJULnrVjaK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip\n",
        "datagen = ImageDataGenerator(zoom_range = 0.2, horizontal_flip=True, shear_range=0.2)\n",
        "datagen.fit(xtrain)\n",
        "# Fits-the-model\n",
        "\n",
        "history = model.fit(xtrain,ytrain, batch_size=32,\n",
        "               steps_per_epoch=xtrain.shape[0]//32,\n",
        "               epochs=1000,\n",
        "               verbose=2,\n",
        "               callbacks=[anne, checkpoint],\n",
        "               validation_split = 0.1,\n",
        "               validation_steps = xtest.shape[0]//32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxcEnLiXR5YR",
        "outputId": "383bbaec-75fb-4397-ab57-20213313c112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.49570, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 - 15s - loss: 0.8481 - accuracy: 0.3565 - val_loss: 0.4957 - val_accuracy: 0.3639 - lr: 0.0010 - 15s/epoch - 4s/step\n",
            "Epoch 2/1000\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.49570\n",
            "4/4 - 2s - loss: 0.6168 - accuracy: 0.3556 - val_loss: 1.8768 - val_accuracy: 0.5073 - lr: 0.0010 - 2s/epoch - 453ms/step\n",
            "Epoch 3/1000\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.49570\n",
            "4/4 - 0s - loss: 0.9993 - accuracy: 0.3707 - val_loss: 0.6291 - val_accuracy: 0.3838 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 4/1000\n",
            "\n",
            "Epoch 4: val_loss improved from 0.49570 to 0.38932, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.4935 - accuracy: 0.4075 - val_loss: 0.3893 - val_accuracy: 0.5362 - lr: 0.0010 - 2s/epoch - 612ms/step\n",
            "Epoch 5/1000\n",
            "\n",
            "Epoch 5: val_loss improved from 0.38932 to 0.35629, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 5s - loss: 0.4401 - accuracy: 0.4967 - val_loss: 0.3563 - val_accuracy: 0.4890 - lr: 0.0010 - 5s/epoch - 1s/step\n",
            "Epoch 6/1000\n",
            "\n",
            "Epoch 6: val_loss improved from 0.35629 to 0.27249, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.3289 - accuracy: 0.5064 - val_loss: 0.2725 - val_accuracy: 0.5802 - lr: 0.0010 - 2s/epoch - 578ms/step\n",
            "Epoch 7/1000\n",
            "\n",
            "Epoch 7: val_loss improved from 0.27249 to 0.20965, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 3s - loss: 0.2300 - accuracy: 0.6211 - val_loss: 0.2097 - val_accuracy: 0.7071 - lr: 0.0010 - 3s/epoch - 754ms/step\n",
            "Epoch 8/1000\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.20965\n",
            "4/4 - 0s - loss: 0.2540 - accuracy: 0.6811 - val_loss: 0.2281 - val_accuracy: 0.6999 - lr: 0.0010 - 449ms/epoch - 112ms/step\n",
            "Epoch 9/1000\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.20965\n",
            "4/4 - 0s - loss: 0.2134 - accuracy: 0.7458 - val_loss: 0.2636 - val_accuracy: 0.7023 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 10/1000\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.20965\n",
            "4/4 - 0s - loss: 0.2003 - accuracy: 0.7973 - val_loss: 0.2467 - val_accuracy: 0.9008 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 11/1000\n",
            "\n",
            "Epoch 11: val_loss improved from 0.20965 to 0.17595, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.2002 - accuracy: 0.8153 - val_loss: 0.1760 - val_accuracy: 0.8421 - lr: 0.0010 - 2s/epoch - 620ms/step\n",
            "Epoch 12/1000\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.17595\n",
            "4/4 - 0s - loss: 0.1621 - accuracy: 0.8153 - val_loss: 0.1943 - val_accuracy: 0.7904 - lr: 0.0010 - 471ms/epoch - 118ms/step\n",
            "Epoch 13/1000\n",
            "\n",
            "Epoch 13: val_loss did not improve from 0.17595\n",
            "4/4 - 0s - loss: 0.1628 - accuracy: 0.8463 - val_loss: 0.2023 - val_accuracy: 0.8249 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 14/1000\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.17595\n",
            "4/4 - 0s - loss: 0.1490 - accuracy: 0.8801 - val_loss: 0.1916 - val_accuracy: 0.8727 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 15/1000\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.17595\n",
            "4/4 - 0s - loss: 0.1536 - accuracy: 0.8984 - val_loss: 0.2033 - val_accuracy: 0.8489 - lr: 0.0010 - 382ms/epoch - 96ms/step\n",
            "Epoch 16/1000\n",
            "\n",
            "Epoch 16: val_loss improved from 0.17595 to 0.15352, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.1586 - accuracy: 0.8722 - val_loss: 0.1535 - val_accuracy: 0.8804 - lr: 0.0010 - 2s/epoch - 498ms/step\n",
            "Epoch 17/1000\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.15352\n",
            "4/4 - 0s - loss: 0.1423 - accuracy: 0.9022 - val_loss: 0.1625 - val_accuracy: 0.8934 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 18/1000\n",
            "\n",
            "Epoch 18: val_loss improved from 0.15352 to 0.14352, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 4s - loss: 0.1295 - accuracy: 0.9286 - val_loss: 0.1435 - val_accuracy: 0.9322 - lr: 0.0010 - 4s/epoch - 993ms/step\n",
            "Epoch 19/1000\n",
            "\n",
            "Epoch 19: val_loss improved from 0.14352 to 0.14218, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 6s - loss: 0.1342 - accuracy: 0.9419 - val_loss: 0.1422 - val_accuracy: 0.9465 - lr: 0.0010 - 6s/epoch - 1s/step\n",
            "Epoch 20/1000\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.14218\n",
            "4/4 - 0s - loss: 0.1215 - accuracy: 0.9525 - val_loss: 0.1511 - val_accuracy: 0.9369 - lr: 0.0010 - 469ms/epoch - 117ms/step\n",
            "Epoch 21/1000\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.14218\n",
            "4/4 - 0s - loss: 0.1191 - accuracy: 0.9556 - val_loss: 0.1540 - val_accuracy: 0.9477 - lr: 0.0010 - 422ms/epoch - 106ms/step\n",
            "Epoch 22/1000\n",
            "\n",
            "Epoch 22: val_loss improved from 0.14218 to 0.12989, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 4s - loss: 0.1134 - accuracy: 0.9570 - val_loss: 0.1299 - val_accuracy: 0.9682 - lr: 0.0010 - 4s/epoch - 925ms/step\n",
            "Epoch 23/1000\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.12989\n",
            "4/4 - 0s - loss: 0.0942 - accuracy: 0.9700 - val_loss: 0.1330 - val_accuracy: 0.9681 - lr: 0.0010 - 421ms/epoch - 105ms/step\n",
            "Epoch 24/1000\n",
            "\n",
            "Epoch 24: val_loss improved from 0.12989 to 0.12330, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 4s - loss: 0.1014 - accuracy: 0.9746 - val_loss: 0.1233 - val_accuracy: 0.9725 - lr: 0.0010 - 4s/epoch - 883ms/step\n",
            "Epoch 25/1000\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.12330\n",
            "4/4 - 0s - loss: 0.0948 - accuracy: 0.9781 - val_loss: 0.1353 - val_accuracy: 0.9655 - lr: 0.0010 - 444ms/epoch - 111ms/step\n",
            "Epoch 26/1000\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.12330\n",
            "4/4 - 0s - loss: 0.1103 - accuracy: 0.9771 - val_loss: 0.1265 - val_accuracy: 0.9775 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 27/1000\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.12330\n",
            "4/4 - 0s - loss: 0.1124 - accuracy: 0.9780 - val_loss: 0.1243 - val_accuracy: 0.9779 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 28/1000\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.12330\n",
            "4/4 - 0s - loss: 0.0952 - accuracy: 0.9769 - val_loss: 0.1398 - val_accuracy: 0.9822 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 29/1000\n",
            "\n",
            "Epoch 29: val_loss improved from 0.12330 to 0.11991, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.0992 - accuracy: 0.9802 - val_loss: 0.1199 - val_accuracy: 0.9794 - lr: 0.0010 - 2s/epoch - 509ms/step\n",
            "Epoch 30/1000\n",
            "\n",
            "Epoch 30: val_loss improved from 0.11991 to 0.11723, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.0811 - accuracy: 0.9838 - val_loss: 0.1172 - val_accuracy: 0.9782 - lr: 0.0010 - 2s/epoch - 600ms/step\n",
            "Epoch 31/1000\n",
            "\n",
            "Epoch 31: val_loss improved from 0.11723 to 0.11498, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 5s - loss: 0.0794 - accuracy: 0.9836 - val_loss: 0.1150 - val_accuracy: 0.9824 - lr: 0.0010 - 5s/epoch - 1s/step\n",
            "Epoch 32/1000\n",
            "\n",
            "Epoch 32: val_loss did not improve from 0.11498\n",
            "4/4 - 1s - loss: 0.0768 - accuracy: 0.9844 - val_loss: 0.1159 - val_accuracy: 0.9824 - lr: 0.0010 - 588ms/epoch - 147ms/step\n",
            "Epoch 33/1000\n",
            "\n",
            "Epoch 33: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0794 - accuracy: 0.9840 - val_loss: 0.1349 - val_accuracy: 0.9850 - lr: 0.0010 - 412ms/epoch - 103ms/step\n",
            "Epoch 34/1000\n",
            "\n",
            "Epoch 34: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.1173 - accuracy: 0.9852 - val_loss: 0.1248 - val_accuracy: 0.9815 - lr: 0.0010 - 418ms/epoch - 105ms/step\n",
            "Epoch 35/1000\n",
            "\n",
            "Epoch 35: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.1047 - accuracy: 0.9826 - val_loss: 0.1815 - val_accuracy: 0.9435 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 36/1000\n",
            "\n",
            "Epoch 36: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.1294 - accuracy: 0.9683 - val_loss: 0.1677 - val_accuracy: 0.9119 - lr: 0.0010 - 407ms/epoch - 102ms/step\n",
            "Epoch 37/1000\n",
            "\n",
            "Epoch 37: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.1150 - accuracy: 0.9649 - val_loss: 0.1276 - val_accuracy: 0.9826 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 38/1000\n",
            "\n",
            "Epoch 38: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0955 - accuracy: 0.9832 - val_loss: 0.1227 - val_accuracy: 0.9770 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 39/1000\n",
            "\n",
            "Epoch 39: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0803 - accuracy: 0.9826 - val_loss: 0.1355 - val_accuracy: 0.9700 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 40/1000\n",
            "\n",
            "Epoch 40: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0797 - accuracy: 0.9843 - val_loss: 0.1203 - val_accuracy: 0.9766 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 41/1000\n",
            "\n",
            "Epoch 41: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0792 - accuracy: 0.9878 - val_loss: 0.1204 - val_accuracy: 0.9789 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 42/1000\n",
            "\n",
            "Epoch 42: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0760 - accuracy: 0.9885 - val_loss: 0.1343 - val_accuracy: 0.9801 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 43/1000\n",
            "\n",
            "Epoch 43: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0949 - accuracy: 0.9888 - val_loss: 0.1384 - val_accuracy: 0.9821 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 44/1000\n",
            "\n",
            "Epoch 44: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0921 - accuracy: 0.9901 - val_loss: 0.1196 - val_accuracy: 0.9884 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 45/1000\n",
            "\n",
            "Epoch 45: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0683 - accuracy: 0.9907 - val_loss: 0.1243 - val_accuracy: 0.9880 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 46/1000\n",
            "\n",
            "Epoch 46: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0861 - accuracy: 0.9874 - val_loss: 0.1213 - val_accuracy: 0.9902 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 47/1000\n",
            "\n",
            "Epoch 47: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0764 - accuracy: 0.9888 - val_loss: 0.1355 - val_accuracy: 0.9892 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 48/1000\n",
            "\n",
            "Epoch 48: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0802 - accuracy: 0.9859 - val_loss: 0.1248 - val_accuracy: 0.9824 - lr: 0.0010 - 402ms/epoch - 101ms/step\n",
            "Epoch 49/1000\n",
            "\n",
            "Epoch 49: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0674 - accuracy: 0.9922 - val_loss: 0.1169 - val_accuracy: 0.9901 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 50/1000\n",
            "\n",
            "Epoch 50: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0612 - accuracy: 0.9918 - val_loss: 0.1161 - val_accuracy: 0.9905 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 51/1000\n",
            "\n",
            "Epoch 51: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0599 - accuracy: 0.9927 - val_loss: 0.1183 - val_accuracy: 0.9894 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 52/1000\n",
            "\n",
            "Epoch 52: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0650 - accuracy: 0.9932 - val_loss: 0.1339 - val_accuracy: 0.9797 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 53/1000\n",
            "\n",
            "Epoch 53: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0822 - accuracy: 0.9896 - val_loss: 0.1186 - val_accuracy: 0.9888 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 54/1000\n",
            "\n",
            "Epoch 54: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0815 - accuracy: 0.9912 - val_loss: 0.1324 - val_accuracy: 0.9891 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 55/1000\n",
            "\n",
            "Epoch 55: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0805 - accuracy: 0.9902 - val_loss: 0.1290 - val_accuracy: 0.9915 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 56/1000\n",
            "\n",
            "Epoch 56: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0827 - accuracy: 0.9904 - val_loss: 0.1156 - val_accuracy: 0.9892 - lr: 0.0010 - 409ms/epoch - 102ms/step\n",
            "Epoch 57/1000\n",
            "\n",
            "Epoch 57: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0659 - accuracy: 0.9905 - val_loss: 0.1181 - val_accuracy: 0.9884 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 58/1000\n",
            "\n",
            "Epoch 58: val_loss did not improve from 0.11498\n",
            "4/4 - 0s - loss: 0.0601 - accuracy: 0.9908 - val_loss: 0.1158 - val_accuracy: 0.9907 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 59/1000\n",
            "\n",
            "Epoch 59: val_loss improved from 0.11498 to 0.11285, saving model to /content/drive/MyDrive/smile_dataset/2/dataset/Inception-v3-mae/model.h5\n",
            "4/4 - 2s - loss: 0.0630 - accuracy: 0.9943 - val_loss: 0.1129 - val_accuracy: 0.9916 - lr: 0.0010 - 2s/epoch - 515ms/step\n",
            "Epoch 60/1000\n",
            "\n",
            "Epoch 60: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0540 - accuracy: 0.9914 - val_loss: 0.1136 - val_accuracy: 0.9908 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 61/1000\n",
            "\n",
            "Epoch 61: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0584 - accuracy: 0.9917 - val_loss: 0.1172 - val_accuracy: 0.9870 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 62/1000\n",
            "\n",
            "Epoch 62: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0897 - accuracy: 0.9912 - val_loss: 0.1424 - val_accuracy: 0.9919 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 63/1000\n",
            "\n",
            "Epoch 63: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1242 - accuracy: 0.9571 - val_loss: 0.1257 - val_accuracy: 0.9800 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 64/1000\n",
            "\n",
            "Epoch 64: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1058 - accuracy: 0.9811 - val_loss: 0.1339 - val_accuracy: 0.9805 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 65/1000\n",
            "\n",
            "Epoch 65: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0928 - accuracy: 0.9732 - val_loss: 0.1436 - val_accuracy: 0.9871 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 66/1000\n",
            "\n",
            "Epoch 66: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0854 - accuracy: 0.9893 - val_loss: 0.1218 - val_accuracy: 0.9849 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 67/1000\n",
            "\n",
            "Epoch 67: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0726 - accuracy: 0.9873 - val_loss: 0.1233 - val_accuracy: 0.9905 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 68/1000\n",
            "\n",
            "Epoch 68: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0708 - accuracy: 0.9908 - val_loss: 0.1206 - val_accuracy: 0.9828 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 69/1000\n",
            "\n",
            "Epoch 69: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0631 - accuracy: 0.9876 - val_loss: 0.1178 - val_accuracy: 0.9820 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 70/1000\n",
            "\n",
            "Epoch 70: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0596 - accuracy: 0.9904 - val_loss: 0.1154 - val_accuracy: 0.9897 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 71/1000\n",
            "\n",
            "Epoch 71: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0584 - accuracy: 0.9893 - val_loss: 0.1162 - val_accuracy: 0.9903 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 72/1000\n",
            "\n",
            "Epoch 72: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0573 - accuracy: 0.9914 - val_loss: 0.1151 - val_accuracy: 0.9915 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 73/1000\n",
            "\n",
            "Epoch 73: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0512 - accuracy: 0.9920 - val_loss: 0.1159 - val_accuracy: 0.9916 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 74/1000\n",
            "\n",
            "Epoch 74: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0521 - accuracy: 0.9911 - val_loss: 0.1206 - val_accuracy: 0.9912 - lr: 0.0010 - 346ms/epoch - 86ms/step\n",
            "Epoch 75/1000\n",
            "\n",
            "Epoch 75: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0521 - accuracy: 0.9922 - val_loss: 0.1139 - val_accuracy: 0.9916 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 76/1000\n",
            "\n",
            "Epoch 76: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0479 - accuracy: 0.9928 - val_loss: 0.1146 - val_accuracy: 0.9912 - lr: 0.0010 - 447ms/epoch - 112ms/step\n",
            "Epoch 77/1000\n",
            "\n",
            "Epoch 77: val_loss did not improve from 0.11285\n",
            "4/4 - 1s - loss: 0.0478 - accuracy: 0.9922 - val_loss: 0.1161 - val_accuracy: 0.9913 - lr: 0.0010 - 534ms/epoch - 133ms/step\n",
            "Epoch 78/1000\n",
            "\n",
            "Epoch 78: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0575 - accuracy: 0.9925 - val_loss: 0.1285 - val_accuracy: 0.9904 - lr: 0.0010 - 479ms/epoch - 120ms/step\n",
            "Epoch 79/1000\n",
            "\n",
            "Epoch 79: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0662 - accuracy: 0.9860 - val_loss: 0.1330 - val_accuracy: 0.9800 - lr: 0.0010 - 454ms/epoch - 113ms/step\n",
            "Epoch 80/1000\n",
            "\n",
            "Epoch 80: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0682 - accuracy: 0.9910 - val_loss: 0.1158 - val_accuracy: 0.9913 - lr: 0.0010 - 439ms/epoch - 110ms/step\n",
            "Epoch 81/1000\n",
            "\n",
            "Epoch 81: val_loss did not improve from 0.11285\n",
            "4/4 - 1s - loss: 0.0680 - accuracy: 0.9916 - val_loss: 0.1398 - val_accuracy: 0.9915 - lr: 0.0010 - 504ms/epoch - 126ms/step\n",
            "Epoch 82/1000\n",
            "\n",
            "Epoch 82: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0698 - accuracy: 0.9939 - val_loss: 0.1192 - val_accuracy: 0.9910 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 83/1000\n",
            "\n",
            "Epoch 83: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0750 - accuracy: 0.9927 - val_loss: 0.1184 - val_accuracy: 0.9880 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 84/1000\n",
            "\n",
            "Epoch 84: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0618 - accuracy: 0.9870 - val_loss: 0.1181 - val_accuracy: 0.9909 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 85/1000\n",
            "\n",
            "Epoch 85: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0647 - accuracy: 0.9921 - val_loss: 0.1611 - val_accuracy: 0.9920 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 86/1000\n",
            "\n",
            "Epoch 86: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1085 - accuracy: 0.9786 - val_loss: 0.1329 - val_accuracy: 0.9009 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 87/1000\n",
            "\n",
            "Epoch 87: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1101 - accuracy: 0.9836 - val_loss: 0.1264 - val_accuracy: 0.9845 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 88/1000\n",
            "\n",
            "Epoch 88: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1115 - accuracy: 0.9324 - val_loss: 0.1325 - val_accuracy: 0.9781 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 89/1000\n",
            "\n",
            "Epoch 89: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0949 - accuracy: 0.9662 - val_loss: 0.1308 - val_accuracy: 0.9219 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 90/1000\n",
            "\n",
            "Epoch 90: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0763 - accuracy: 0.9639 - val_loss: 0.1226 - val_accuracy: 0.9850 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 91/1000\n",
            "\n",
            "Epoch 91: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0635 - accuracy: 0.9900 - val_loss: 0.1180 - val_accuracy: 0.9885 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 92/1000\n",
            "\n",
            "Epoch 92: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0620 - accuracy: 0.9894 - val_loss: 0.1254 - val_accuracy: 0.9866 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 93/1000\n",
            "\n",
            "Epoch 93: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0600 - accuracy: 0.9936 - val_loss: 0.1164 - val_accuracy: 0.9899 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 94/1000\n",
            "\n",
            "Epoch 94: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0497 - accuracy: 0.9908 - val_loss: 0.1158 - val_accuracy: 0.9914 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 95/1000\n",
            "\n",
            "Epoch 95: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0483 - accuracy: 0.9945 - val_loss: 0.1179 - val_accuracy: 0.9909 - lr: 0.0010 - 345ms/epoch - 86ms/step\n",
            "Epoch 96/1000\n",
            "\n",
            "Epoch 96: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0454 - accuracy: 0.9933 - val_loss: 0.1144 - val_accuracy: 0.9898 - lr: 0.0010 - 402ms/epoch - 100ms/step\n",
            "Epoch 97/1000\n",
            "\n",
            "Epoch 97: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0417 - accuracy: 0.9937 - val_loss: 0.1138 - val_accuracy: 0.9902 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 98/1000\n",
            "\n",
            "Epoch 98: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0483 - accuracy: 0.9932 - val_loss: 0.1212 - val_accuracy: 0.9913 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 99/1000\n",
            "\n",
            "Epoch 99: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0506 - accuracy: 0.9924 - val_loss: 0.1147 - val_accuracy: 0.9907 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 100/1000\n",
            "\n",
            "Epoch 100: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0565 - accuracy: 0.9924 - val_loss: 0.1323 - val_accuracy: 0.9916 - lr: 0.0010 - 346ms/epoch - 87ms/step\n",
            "Epoch 101/1000\n",
            "\n",
            "Epoch 101: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0795 - accuracy: 0.9830 - val_loss: 0.1283 - val_accuracy: 0.9904 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 102/1000\n",
            "\n",
            "Epoch 102: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0647 - accuracy: 0.9913 - val_loss: 0.1185 - val_accuracy: 0.9891 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 103/1000\n",
            "\n",
            "Epoch 103: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0502 - accuracy: 0.9921 - val_loss: 0.1208 - val_accuracy: 0.9890 - lr: 0.0010 - 346ms/epoch - 87ms/step\n",
            "Epoch 104/1000\n",
            "\n",
            "Epoch 104: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0557 - accuracy: 0.9921 - val_loss: 0.1149 - val_accuracy: 0.9882 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 105/1000\n",
            "\n",
            "Epoch 105: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0516 - accuracy: 0.9926 - val_loss: 0.1169 - val_accuracy: 0.9912 - lr: 0.0010 - 348ms/epoch - 87ms/step\n",
            "Epoch 106/1000\n",
            "\n",
            "Epoch 106: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0502 - accuracy: 0.9929 - val_loss: 0.1303 - val_accuracy: 0.9902 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 107/1000\n",
            "\n",
            "Epoch 107: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0580 - accuracy: 0.9916 - val_loss: 0.1192 - val_accuracy: 0.9894 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 108/1000\n",
            "\n",
            "Epoch 108: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0517 - accuracy: 0.9940 - val_loss: 0.1153 - val_accuracy: 0.9912 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 109/1000\n",
            "\n",
            "Epoch 109: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0597 - accuracy: 0.9901 - val_loss: 0.1271 - val_accuracy: 0.9914 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 110/1000\n",
            "\n",
            "Epoch 110: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0646 - accuracy: 0.9914 - val_loss: 0.1333 - val_accuracy: 0.9902 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 111/1000\n",
            "\n",
            "Epoch 111: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0627 - accuracy: 0.9926 - val_loss: 0.1297 - val_accuracy: 0.9862 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 112/1000\n",
            "\n",
            "Epoch 112: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0573 - accuracy: 0.9932 - val_loss: 0.1227 - val_accuracy: 0.9898 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 113/1000\n",
            "\n",
            "Epoch 113: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0563 - accuracy: 0.9929 - val_loss: 0.1168 - val_accuracy: 0.9918 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 114/1000\n",
            "\n",
            "Epoch 114: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0607 - accuracy: 0.9881 - val_loss: 0.1257 - val_accuracy: 0.9910 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 115/1000\n",
            "\n",
            "Epoch 115: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0575 - accuracy: 0.9931 - val_loss: 0.1217 - val_accuracy: 0.9902 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 116/1000\n",
            "\n",
            "Epoch 116: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0558 - accuracy: 0.9921 - val_loss: 0.1217 - val_accuracy: 0.9899 - lr: 0.0010 - 386ms/epoch - 97ms/step\n",
            "Epoch 117/1000\n",
            "\n",
            "Epoch 117: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0458 - accuracy: 0.9924 - val_loss: 0.1166 - val_accuracy: 0.9907 - lr: 0.0010 - 394ms/epoch - 98ms/step\n",
            "Epoch 118/1000\n",
            "\n",
            "Epoch 118: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0429 - accuracy: 0.9924 - val_loss: 0.1144 - val_accuracy: 0.9907 - lr: 0.0010 - 402ms/epoch - 100ms/step\n",
            "Epoch 119/1000\n",
            "\n",
            "Epoch 119: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0443 - accuracy: 0.9932 - val_loss: 0.1192 - val_accuracy: 0.9910 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 120/1000\n",
            "\n",
            "Epoch 120: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0457 - accuracy: 0.9908 - val_loss: 0.1166 - val_accuracy: 0.9910 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 121/1000\n",
            "\n",
            "Epoch 121: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0471 - accuracy: 0.9921 - val_loss: 0.1208 - val_accuracy: 0.9899 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 122/1000\n",
            "\n",
            "Epoch 122: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0688 - accuracy: 0.9923 - val_loss: 0.1193 - val_accuracy: 0.9865 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 123/1000\n",
            "\n",
            "Epoch 123: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0529 - accuracy: 0.9908 - val_loss: 0.1317 - val_accuracy: 0.9391 - lr: 0.0010 - 349ms/epoch - 87ms/step\n",
            "Epoch 124/1000\n",
            "\n",
            "Epoch 124: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0535 - accuracy: 0.9685 - val_loss: 0.1281 - val_accuracy: 0.9897 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 125/1000\n",
            "\n",
            "Epoch 125: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0515 - accuracy: 0.9931 - val_loss: 0.1235 - val_accuracy: 0.9900 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 126/1000\n",
            "\n",
            "Epoch 126: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0554 - accuracy: 0.9932 - val_loss: 0.1252 - val_accuracy: 0.9921 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 127/1000\n",
            "\n",
            "Epoch 127: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0522 - accuracy: 0.9930 - val_loss: 0.1178 - val_accuracy: 0.9906 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 128/1000\n",
            "\n",
            "Epoch 128: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0490 - accuracy: 0.9936 - val_loss: 0.1282 - val_accuracy: 0.9918 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 129/1000\n",
            "\n",
            "Epoch 129: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0510 - accuracy: 0.9916 - val_loss: 0.1171 - val_accuracy: 0.9903 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 130/1000\n",
            "\n",
            "Epoch 130: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0492 - accuracy: 0.9935 - val_loss: 0.1185 - val_accuracy: 0.9916 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 131/1000\n",
            "\n",
            "Epoch 131: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0440 - accuracy: 0.9926 - val_loss: 0.1148 - val_accuracy: 0.9905 - lr: 0.0010 - 407ms/epoch - 102ms/step\n",
            "Epoch 132/1000\n",
            "\n",
            "Epoch 132: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0369 - accuracy: 0.9941 - val_loss: 0.1176 - val_accuracy: 0.9914 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 133/1000\n",
            "\n",
            "Epoch 133: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0402 - accuracy: 0.9913 - val_loss: 0.1208 - val_accuracy: 0.9896 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 134/1000\n",
            "\n",
            "Epoch 134: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0377 - accuracy: 0.9932 - val_loss: 0.1165 - val_accuracy: 0.9909 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 135/1000\n",
            "\n",
            "Epoch 135: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0340 - accuracy: 0.9936 - val_loss: 0.1161 - val_accuracy: 0.9918 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 136/1000\n",
            "\n",
            "Epoch 136: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0357 - accuracy: 0.9932 - val_loss: 0.1220 - val_accuracy: 0.9853 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 137/1000\n",
            "\n",
            "Epoch 137: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0425 - accuracy: 0.9928 - val_loss: 0.1156 - val_accuracy: 0.9910 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 138/1000\n",
            "\n",
            "Epoch 138: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0415 - accuracy: 0.9928 - val_loss: 0.1207 - val_accuracy: 0.9898 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 139/1000\n",
            "\n",
            "Epoch 139: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0390 - accuracy: 0.9944 - val_loss: 0.1209 - val_accuracy: 0.9911 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 140/1000\n",
            "\n",
            "Epoch 140: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0443 - accuracy: 0.9934 - val_loss: 0.1187 - val_accuracy: 0.9904 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 141/1000\n",
            "\n",
            "Epoch 141: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0362 - accuracy: 0.9935 - val_loss: 0.1141 - val_accuracy: 0.9916 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 142/1000\n",
            "\n",
            "Epoch 142: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0405 - accuracy: 0.9913 - val_loss: 0.1208 - val_accuracy: 0.9900 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 143/1000\n",
            "\n",
            "Epoch 143: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0394 - accuracy: 0.9945 - val_loss: 0.1148 - val_accuracy: 0.9897 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 144/1000\n",
            "\n",
            "Epoch 144: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0328 - accuracy: 0.9925 - val_loss: 0.1145 - val_accuracy: 0.9912 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 145/1000\n",
            "\n",
            "Epoch 145: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0362 - accuracy: 0.9932 - val_loss: 0.1161 - val_accuracy: 0.9911 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 146/1000\n",
            "\n",
            "Epoch 146: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0334 - accuracy: 0.9935 - val_loss: 0.1161 - val_accuracy: 0.9910 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 147/1000\n",
            "\n",
            "Epoch 147: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0322 - accuracy: 0.9936 - val_loss: 0.1167 - val_accuracy: 0.9902 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 148/1000\n",
            "\n",
            "Epoch 148: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0553 - accuracy: 0.9928 - val_loss: 0.1190 - val_accuracy: 0.9747 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 149/1000\n",
            "\n",
            "Epoch 149: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.1191 - val_accuracy: 0.9705 - lr: 0.0010 - 402ms/epoch - 101ms/step\n",
            "Epoch 150/1000\n",
            "\n",
            "Epoch 150: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0403 - accuracy: 0.9831 - val_loss: 0.1179 - val_accuracy: 0.9920 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 151/1000\n",
            "\n",
            "Epoch 151: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0410 - accuracy: 0.9915 - val_loss: 0.1207 - val_accuracy: 0.9919 - lr: 0.0010 - 427ms/epoch - 107ms/step\n",
            "Epoch 152/1000\n",
            "\n",
            "Epoch 152: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0383 - accuracy: 0.9940 - val_loss: 0.1192 - val_accuracy: 0.9914 - lr: 0.0010 - 378ms/epoch - 95ms/step\n",
            "Epoch 153/1000\n",
            "\n",
            "Epoch 153: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0454 - accuracy: 0.9921 - val_loss: 0.1215 - val_accuracy: 0.9914 - lr: 0.0010 - 404ms/epoch - 101ms/step\n",
            "Epoch 154/1000\n",
            "\n",
            "Epoch 154: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0449 - accuracy: 0.9937 - val_loss: 0.1234 - val_accuracy: 0.9913 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 155/1000\n",
            "\n",
            "Epoch 155: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0454 - accuracy: 0.9942 - val_loss: 0.1175 - val_accuracy: 0.9909 - lr: 0.0010 - 404ms/epoch - 101ms/step\n",
            "Epoch 156/1000\n",
            "\n",
            "Epoch 156: val_loss did not improve from 0.11285\n",
            "4/4 - 1s - loss: 0.0575 - accuracy: 0.9940 - val_loss: 0.1245 - val_accuracy: 0.9919 - lr: 0.0010 - 552ms/epoch - 138ms/step\n",
            "Epoch 157/1000\n",
            "\n",
            "Epoch 157: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0572 - accuracy: 0.9929 - val_loss: 0.1336 - val_accuracy: 0.9919 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 158/1000\n",
            "\n",
            "Epoch 158: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0768 - accuracy: 0.9860 - val_loss: 0.1300 - val_accuracy: 0.9905 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 159/1000\n",
            "\n",
            "Epoch 159: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0688 - accuracy: 0.9912 - val_loss: 0.1295 - val_accuracy: 0.9920 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 160/1000\n",
            "\n",
            "Epoch 160: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0700 - accuracy: 0.9864 - val_loss: 0.1376 - val_accuracy: 0.9903 - lr: 0.0010 - 429ms/epoch - 107ms/step\n",
            "Epoch 161/1000\n",
            "\n",
            "Epoch 161: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0630 - accuracy: 0.9933 - val_loss: 0.1253 - val_accuracy: 0.9893 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 162/1000\n",
            "\n",
            "Epoch 162: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0546 - accuracy: 0.9913 - val_loss: 0.1218 - val_accuracy: 0.9854 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 163/1000\n",
            "\n",
            "Epoch 163: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0467 - accuracy: 0.9882 - val_loss: 0.1243 - val_accuracy: 0.9914 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 164/1000\n",
            "\n",
            "Epoch 164: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0496 - accuracy: 0.9929 - val_loss: 0.1191 - val_accuracy: 0.9865 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 165/1000\n",
            "\n",
            "Epoch 165: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0458 - accuracy: 0.9897 - val_loss: 0.1198 - val_accuracy: 0.9913 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 166/1000\n",
            "\n",
            "Epoch 166: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0443 - accuracy: 0.9927 - val_loss: 0.1212 - val_accuracy: 0.9904 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 167/1000\n",
            "\n",
            "Epoch 167: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0413 - accuracy: 0.9885 - val_loss: 0.1178 - val_accuracy: 0.9916 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 168/1000\n",
            "\n",
            "Epoch 168: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0362 - accuracy: 0.9907 - val_loss: 0.1164 - val_accuracy: 0.9909 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 169/1000\n",
            "\n",
            "Epoch 169: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0411 - accuracy: 0.9948 - val_loss: 0.1187 - val_accuracy: 0.9916 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 170/1000\n",
            "\n",
            "Epoch 170: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0413 - accuracy: 0.9936 - val_loss: 0.1148 - val_accuracy: 0.9905 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 171/1000\n",
            "\n",
            "Epoch 171: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0458 - accuracy: 0.9910 - val_loss: 0.1267 - val_accuracy: 0.9920 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 172/1000\n",
            "\n",
            "Epoch 172: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0565 - accuracy: 0.9884 - val_loss: 0.1278 - val_accuracy: 0.9884 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 173/1000\n",
            "\n",
            "Epoch 173: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0561 - accuracy: 0.9917 - val_loss: 0.1200 - val_accuracy: 0.9919 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 174/1000\n",
            "\n",
            "Epoch 174: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0511 - accuracy: 0.9900 - val_loss: 0.1216 - val_accuracy: 0.9908 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 175/1000\n",
            "\n",
            "Epoch 175: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0505 - accuracy: 0.9940 - val_loss: 0.1258 - val_accuracy: 0.9911 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 176/1000\n",
            "\n",
            "Epoch 176: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0513 - accuracy: 0.9948 - val_loss: 0.1213 - val_accuracy: 0.9909 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 177/1000\n",
            "\n",
            "Epoch 177: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0459 - accuracy: 0.9916 - val_loss: 0.1147 - val_accuracy: 0.9919 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 178/1000\n",
            "\n",
            "Epoch 178: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0395 - accuracy: 0.9939 - val_loss: 0.1136 - val_accuracy: 0.9906 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 179/1000\n",
            "\n",
            "Epoch 179: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0409 - accuracy: 0.9941 - val_loss: 0.1150 - val_accuracy: 0.9903 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 180/1000\n",
            "\n",
            "Epoch 180: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0342 - accuracy: 0.9898 - val_loss: 0.1288 - val_accuracy: 0.9901 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 181/1000\n",
            "\n",
            "Epoch 181: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0461 - accuracy: 0.9924 - val_loss: 0.1256 - val_accuracy: 0.9885 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 182/1000\n",
            "\n",
            "Epoch 182: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0556 - accuracy: 0.9869 - val_loss: 0.1281 - val_accuracy: 0.9816 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 183/1000\n",
            "\n",
            "Epoch 183: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0561 - accuracy: 0.9905 - val_loss: 0.1290 - val_accuracy: 0.9901 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 184/1000\n",
            "\n",
            "Epoch 184: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0531 - accuracy: 0.9909 - val_loss: 0.1162 - val_accuracy: 0.9912 - lr: 0.0010 - 394ms/epoch - 98ms/step\n",
            "Epoch 185/1000\n",
            "\n",
            "Epoch 185: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0485 - accuracy: 0.9928 - val_loss: 0.1185 - val_accuracy: 0.9849 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 186/1000\n",
            "\n",
            "Epoch 186: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0426 - accuracy: 0.9904 - val_loss: 0.1180 - val_accuracy: 0.9896 - lr: 0.0010 - 419ms/epoch - 105ms/step\n",
            "Epoch 187/1000\n",
            "\n",
            "Epoch 187: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0389 - accuracy: 0.9929 - val_loss: 0.1159 - val_accuracy: 0.9900 - lr: 0.0010 - 382ms/epoch - 95ms/step\n",
            "Epoch 188/1000\n",
            "\n",
            "Epoch 188: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0383 - accuracy: 0.9926 - val_loss: 0.1182 - val_accuracy: 0.9895 - lr: 0.0010 - 358ms/epoch - 89ms/step\n",
            "Epoch 189/1000\n",
            "\n",
            "Epoch 189: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0325 - accuracy: 0.9923 - val_loss: 0.1171 - val_accuracy: 0.9917 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 190/1000\n",
            "\n",
            "Epoch 190: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0409 - accuracy: 0.9907 - val_loss: 0.1195 - val_accuracy: 0.9893 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 191/1000\n",
            "\n",
            "Epoch 191: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0384 - accuracy: 0.9912 - val_loss: 0.1224 - val_accuracy: 0.9918 - lr: 0.0010 - 404ms/epoch - 101ms/step\n",
            "Epoch 192/1000\n",
            "\n",
            "Epoch 192: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0371 - accuracy: 0.9921 - val_loss: 0.1164 - val_accuracy: 0.9920 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 193/1000\n",
            "\n",
            "Epoch 193: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0380 - accuracy: 0.9940 - val_loss: 0.1209 - val_accuracy: 0.9911 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 194/1000\n",
            "\n",
            "Epoch 194: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0374 - accuracy: 0.9930 - val_loss: 0.1230 - val_accuracy: 0.9910 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 195/1000\n",
            "\n",
            "Epoch 195: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0383 - accuracy: 0.9934 - val_loss: 0.1174 - val_accuracy: 0.9864 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 196/1000\n",
            "\n",
            "Epoch 196: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0343 - accuracy: 0.9916 - val_loss: 0.1213 - val_accuracy: 0.9881 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 197/1000\n",
            "\n",
            "Epoch 197: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0373 - accuracy: 0.9950 - val_loss: 0.1187 - val_accuracy: 0.9907 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 198/1000\n",
            "\n",
            "Epoch 198: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0401 - accuracy: 0.9906 - val_loss: 0.1152 - val_accuracy: 0.9914 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 199/1000\n",
            "\n",
            "Epoch 199: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0420 - accuracy: 0.9924 - val_loss: 0.1166 - val_accuracy: 0.9910 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 200/1000\n",
            "\n",
            "Epoch 200: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0390 - accuracy: 0.9927 - val_loss: 0.1205 - val_accuracy: 0.9915 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 201/1000\n",
            "\n",
            "Epoch 201: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0335 - accuracy: 0.9930 - val_loss: 0.1167 - val_accuracy: 0.9918 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 202/1000\n",
            "\n",
            "Epoch 202: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0323 - accuracy: 0.9939 - val_loss: 0.1168 - val_accuracy: 0.9909 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 203/1000\n",
            "\n",
            "Epoch 203: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0301 - accuracy: 0.9924 - val_loss: 0.1164 - val_accuracy: 0.9914 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 204/1000\n",
            "\n",
            "Epoch 204: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0280 - accuracy: 0.9949 - val_loss: 0.1153 - val_accuracy: 0.9918 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 205/1000\n",
            "\n",
            "Epoch 205: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9922 - val_loss: 0.1180 - val_accuracy: 0.9917 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 206/1000\n",
            "\n",
            "Epoch 206: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0287 - accuracy: 0.9946 - val_loss: 0.1152 - val_accuracy: 0.9919 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 207/1000\n",
            "\n",
            "Epoch 207: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0264 - accuracy: 0.9927 - val_loss: 0.1156 - val_accuracy: 0.9907 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 208/1000\n",
            "\n",
            "Epoch 208: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9935 - val_loss: 0.1182 - val_accuracy: 0.9893 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 209/1000\n",
            "\n",
            "Epoch 209: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0308 - accuracy: 0.9931 - val_loss: 0.1170 - val_accuracy: 0.9893 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 210/1000\n",
            "\n",
            "Epoch 210: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0280 - accuracy: 0.9937 - val_loss: 0.1147 - val_accuracy: 0.9913 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 211/1000\n",
            "\n",
            "Epoch 211: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0425 - accuracy: 0.9936 - val_loss: 0.1190 - val_accuracy: 0.9880 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 212/1000\n",
            "\n",
            "Epoch 212: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0402 - accuracy: 0.9924 - val_loss: 0.1234 - val_accuracy: 0.9898 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 213/1000\n",
            "\n",
            "Epoch 213: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0457 - accuracy: 0.9938 - val_loss: 0.1203 - val_accuracy: 0.9918 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 214/1000\n",
            "\n",
            "Epoch 214: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0479 - accuracy: 0.9911 - val_loss: 0.1160 - val_accuracy: 0.9919 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 215/1000\n",
            "\n",
            "Epoch 215: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0411 - accuracy: 0.9893 - val_loss: 0.1175 - val_accuracy: 0.9919 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 216/1000\n",
            "\n",
            "Epoch 216: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0357 - accuracy: 0.9946 - val_loss: 0.1188 - val_accuracy: 0.9910 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 217/1000\n",
            "\n",
            "Epoch 217: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0352 - accuracy: 0.9924 - val_loss: 0.1160 - val_accuracy: 0.9916 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 218/1000\n",
            "\n",
            "Epoch 218: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0321 - accuracy: 0.9935 - val_loss: 0.1156 - val_accuracy: 0.9910 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 219/1000\n",
            "\n",
            "Epoch 219: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0292 - accuracy: 0.9953 - val_loss: 0.1150 - val_accuracy: 0.9901 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 220/1000\n",
            "\n",
            "Epoch 220: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0314 - accuracy: 0.9924 - val_loss: 0.1196 - val_accuracy: 0.9900 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 221/1000\n",
            "\n",
            "Epoch 221: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0292 - accuracy: 0.9934 - val_loss: 0.1162 - val_accuracy: 0.9911 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 222/1000\n",
            "\n",
            "Epoch 222: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0322 - accuracy: 0.9935 - val_loss: 0.1160 - val_accuracy: 0.9908 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 223/1000\n",
            "\n",
            "Epoch 223: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0314 - accuracy: 0.9920 - val_loss: 0.1181 - val_accuracy: 0.9907 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 224/1000\n",
            "\n",
            "Epoch 224: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0285 - accuracy: 0.9959 - val_loss: 0.1130 - val_accuracy: 0.9916 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 225/1000\n",
            "\n",
            "Epoch 225: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0315 - accuracy: 0.9909 - val_loss: 0.1163 - val_accuracy: 0.9907 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 226/1000\n",
            "\n",
            "Epoch 226: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0334 - accuracy: 0.9926 - val_loss: 0.1164 - val_accuracy: 0.9917 - lr: 0.0010 - 420ms/epoch - 105ms/step\n",
            "Epoch 227/1000\n",
            "\n",
            "Epoch 227: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0323 - accuracy: 0.9946 - val_loss: 0.1219 - val_accuracy: 0.9892 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 228/1000\n",
            "\n",
            "Epoch 228: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0344 - accuracy: 0.9917 - val_loss: 0.1195 - val_accuracy: 0.9897 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 229/1000\n",
            "\n",
            "Epoch 229: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0309 - accuracy: 0.9937 - val_loss: 0.1184 - val_accuracy: 0.9909 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 230/1000\n",
            "\n",
            "Epoch 230: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0401 - accuracy: 0.9934 - val_loss: 0.1300 - val_accuracy: 0.9807 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 231/1000\n",
            "\n",
            "Epoch 231: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0489 - accuracy: 0.9860 - val_loss: 0.1298 - val_accuracy: 0.9906 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 232/1000\n",
            "\n",
            "Epoch 232: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0432 - accuracy: 0.9935 - val_loss: 0.1259 - val_accuracy: 0.9894 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 233/1000\n",
            "\n",
            "Epoch 233: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0417 - accuracy: 0.9946 - val_loss: 0.1186 - val_accuracy: 0.9918 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 234/1000\n",
            "\n",
            "Epoch 234: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0421 - accuracy: 0.9890 - val_loss: 0.1196 - val_accuracy: 0.9916 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 235/1000\n",
            "\n",
            "Epoch 235: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0356 - accuracy: 0.9938 - val_loss: 0.1249 - val_accuracy: 0.9841 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 236/1000\n",
            "\n",
            "Epoch 236: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0386 - accuracy: 0.9901 - val_loss: 0.1175 - val_accuracy: 0.9914 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 237/1000\n",
            "\n",
            "Epoch 237: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0358 - accuracy: 0.9933 - val_loss: 0.1191 - val_accuracy: 0.9911 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 238/1000\n",
            "\n",
            "Epoch 238: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0389 - accuracy: 0.9952 - val_loss: 0.1195 - val_accuracy: 0.9887 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 239/1000\n",
            "\n",
            "Epoch 239: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0332 - accuracy: 0.9920 - val_loss: 0.1206 - val_accuracy: 0.9893 - lr: 0.0010 - 330ms/epoch - 83ms/step\n",
            "Epoch 240/1000\n",
            "\n",
            "Epoch 240: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0348 - accuracy: 0.9916 - val_loss: 0.1182 - val_accuracy: 0.9915 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 241/1000\n",
            "\n",
            "Epoch 241: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0375 - accuracy: 0.9936 - val_loss: 0.1192 - val_accuracy: 0.9910 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 242/1000\n",
            "\n",
            "Epoch 242: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0450 - accuracy: 0.9903 - val_loss: 0.1191 - val_accuracy: 0.9888 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 243/1000\n",
            "\n",
            "Epoch 243: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0391 - accuracy: 0.9890 - val_loss: 0.1183 - val_accuracy: 0.9885 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 244/1000\n",
            "\n",
            "Epoch 244: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0326 - accuracy: 0.9940 - val_loss: 0.1164 - val_accuracy: 0.9901 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 245/1000\n",
            "\n",
            "Epoch 245: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9929 - val_loss: 0.1205 - val_accuracy: 0.9908 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 246/1000\n",
            "\n",
            "Epoch 246: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0329 - accuracy: 0.9926 - val_loss: 0.1187 - val_accuracy: 0.9911 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 247/1000\n",
            "\n",
            "Epoch 247: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0324 - accuracy: 0.9947 - val_loss: 0.1198 - val_accuracy: 0.9900 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 248/1000\n",
            "\n",
            "Epoch 248: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0272 - accuracy: 0.9906 - val_loss: 0.1168 - val_accuracy: 0.9911 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 249/1000\n",
            "\n",
            "Epoch 249: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0327 - accuracy: 0.9952 - val_loss: 0.1245 - val_accuracy: 0.9890 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 250/1000\n",
            "\n",
            "Epoch 250: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0395 - accuracy: 0.9930 - val_loss: 0.1233 - val_accuracy: 0.9912 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 251/1000\n",
            "\n",
            "Epoch 251: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0410 - accuracy: 0.9926 - val_loss: 0.1215 - val_accuracy: 0.9870 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 252/1000\n",
            "\n",
            "Epoch 252: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0378 - accuracy: 0.9927 - val_loss: 0.1176 - val_accuracy: 0.9887 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 253/1000\n",
            "\n",
            "Epoch 253: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0364 - accuracy: 0.9929 - val_loss: 0.1169 - val_accuracy: 0.9853 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 254/1000\n",
            "\n",
            "Epoch 254: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0311 - accuracy: 0.9930 - val_loss: 0.1195 - val_accuracy: 0.9912 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 255/1000\n",
            "\n",
            "Epoch 255: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0325 - accuracy: 0.9930 - val_loss: 0.1173 - val_accuracy: 0.9905 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 256/1000\n",
            "\n",
            "Epoch 256: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9938 - val_loss: 0.1174 - val_accuracy: 0.9901 - lr: 0.0010 - 425ms/epoch - 106ms/step\n",
            "Epoch 257/1000\n",
            "\n",
            "Epoch 257: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0289 - accuracy: 0.9927 - val_loss: 0.1166 - val_accuracy: 0.9906 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 258/1000\n",
            "\n",
            "Epoch 258: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9948 - val_loss: 0.1166 - val_accuracy: 0.9910 - lr: 0.0010 - 408ms/epoch - 102ms/step\n",
            "Epoch 259/1000\n",
            "\n",
            "Epoch 259: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0250 - accuracy: 0.9939 - val_loss: 0.1164 - val_accuracy: 0.9901 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 260/1000\n",
            "\n",
            "Epoch 260: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0289 - accuracy: 0.9918 - val_loss: 0.1174 - val_accuracy: 0.9913 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 261/1000\n",
            "\n",
            "Epoch 261: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0264 - accuracy: 0.9944 - val_loss: 0.1188 - val_accuracy: 0.9912 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 262/1000\n",
            "\n",
            "Epoch 262: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0347 - accuracy: 0.9945 - val_loss: 0.1180 - val_accuracy: 0.9912 - lr: 0.0010 - 402ms/epoch - 100ms/step\n",
            "Epoch 263/1000\n",
            "\n",
            "Epoch 263: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0344 - accuracy: 0.9908 - val_loss: 0.1245 - val_accuracy: 0.9917 - lr: 0.0010 - 384ms/epoch - 96ms/step\n",
            "Epoch 264/1000\n",
            "\n",
            "Epoch 264: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0464 - accuracy: 0.9940 - val_loss: 0.1156 - val_accuracy: 0.9847 - lr: 0.0010 - 383ms/epoch - 96ms/step\n",
            "Epoch 265/1000\n",
            "\n",
            "Epoch 265: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0413 - accuracy: 0.9902 - val_loss: 0.1204 - val_accuracy: 0.9855 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 266/1000\n",
            "\n",
            "Epoch 266: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0371 - accuracy: 0.9893 - val_loss: 0.1205 - val_accuracy: 0.9910 - lr: 0.0010 - 411ms/epoch - 103ms/step\n",
            "Epoch 267/1000\n",
            "\n",
            "Epoch 267: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0328 - accuracy: 0.9936 - val_loss: 0.1178 - val_accuracy: 0.9908 - lr: 0.0010 - 354ms/epoch - 89ms/step\n",
            "Epoch 268/1000\n",
            "\n",
            "Epoch 268: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0270 - accuracy: 0.9940 - val_loss: 0.1171 - val_accuracy: 0.9916 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 269/1000\n",
            "\n",
            "Epoch 269: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9941 - val_loss: 0.1219 - val_accuracy: 0.9918 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 270/1000\n",
            "\n",
            "Epoch 270: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0401 - accuracy: 0.9923 - val_loss: 0.1186 - val_accuracy: 0.9920 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 271/1000\n",
            "\n",
            "Epoch 271: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0348 - accuracy: 0.9930 - val_loss: 0.1248 - val_accuracy: 0.9911 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 272/1000\n",
            "\n",
            "Epoch 272: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0437 - accuracy: 0.9904 - val_loss: 0.1270 - val_accuracy: 0.9853 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 273/1000\n",
            "\n",
            "Epoch 273: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0485 - accuracy: 0.9898 - val_loss: 0.1279 - val_accuracy: 0.9910 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 274/1000\n",
            "\n",
            "Epoch 274: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0652 - accuracy: 0.9942 - val_loss: 0.1237 - val_accuracy: 0.9918 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 275/1000\n",
            "\n",
            "Epoch 275: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0591 - accuracy: 0.9891 - val_loss: 0.1202 - val_accuracy: 0.9911 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 276/1000\n",
            "\n",
            "Epoch 276: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0470 - accuracy: 0.9934 - val_loss: 0.1204 - val_accuracy: 0.9917 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 277/1000\n",
            "\n",
            "Epoch 277: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0392 - accuracy: 0.9932 - val_loss: 0.1205 - val_accuracy: 0.9909 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 278/1000\n",
            "\n",
            "Epoch 278: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0402 - accuracy: 0.9925 - val_loss: 0.1243 - val_accuracy: 0.9888 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 279/1000\n",
            "\n",
            "Epoch 279: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0421 - accuracy: 0.9918 - val_loss: 0.1224 - val_accuracy: 0.9903 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 280/1000\n",
            "\n",
            "Epoch 280: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0433 - accuracy: 0.9934 - val_loss: 0.1196 - val_accuracy: 0.9863 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 281/1000\n",
            "\n",
            "Epoch 281: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0404 - accuracy: 0.9886 - val_loss: 0.1183 - val_accuracy: 0.9917 - lr: 0.0010 - 394ms/epoch - 99ms/step\n",
            "Epoch 282/1000\n",
            "\n",
            "Epoch 282: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0481 - accuracy: 0.9905 - val_loss: 0.1233 - val_accuracy: 0.9919 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 283/1000\n",
            "\n",
            "Epoch 283: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0384 - accuracy: 0.9932 - val_loss: 0.1304 - val_accuracy: 0.9804 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 284/1000\n",
            "\n",
            "Epoch 284: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0531 - accuracy: 0.9890 - val_loss: 0.1305 - val_accuracy: 0.9906 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 285/1000\n",
            "\n",
            "Epoch 285: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0503 - accuracy: 0.9934 - val_loss: 0.1278 - val_accuracy: 0.9811 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 286/1000\n",
            "\n",
            "Epoch 286: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0517 - accuracy: 0.9885 - val_loss: 0.1240 - val_accuracy: 0.9910 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 287/1000\n",
            "\n",
            "Epoch 287: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0420 - accuracy: 0.9906 - val_loss: 0.1203 - val_accuracy: 0.9877 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 288/1000\n",
            "\n",
            "Epoch 288: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0333 - accuracy: 0.9928 - val_loss: 0.1190 - val_accuracy: 0.9918 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 289/1000\n",
            "\n",
            "Epoch 289: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0279 - accuracy: 0.9948 - val_loss: 0.1217 - val_accuracy: 0.9904 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 290/1000\n",
            "\n",
            "Epoch 290: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9933 - val_loss: 0.1179 - val_accuracy: 0.9910 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 291/1000\n",
            "\n",
            "Epoch 291: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0324 - accuracy: 0.9936 - val_loss: 0.1155 - val_accuracy: 0.9899 - lr: 0.0010 - 423ms/epoch - 106ms/step\n",
            "Epoch 292/1000\n",
            "\n",
            "Epoch 292: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0289 - accuracy: 0.9924 - val_loss: 0.1140 - val_accuracy: 0.9916 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 293/1000\n",
            "\n",
            "Epoch 293: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0311 - accuracy: 0.9928 - val_loss: 0.1212 - val_accuracy: 0.9920 - lr: 0.0010 - 382ms/epoch - 96ms/step\n",
            "Epoch 294/1000\n",
            "\n",
            "Epoch 294: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0302 - accuracy: 0.9933 - val_loss: 0.1162 - val_accuracy: 0.9918 - lr: 0.0010 - 394ms/epoch - 99ms/step\n",
            "Epoch 295/1000\n",
            "\n",
            "Epoch 295: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0288 - accuracy: 0.9944 - val_loss: 0.1159 - val_accuracy: 0.9916 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 296/1000\n",
            "\n",
            "Epoch 296: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9945 - val_loss: 0.1160 - val_accuracy: 0.9909 - lr: 0.0010 - 424ms/epoch - 106ms/step\n",
            "Epoch 297/1000\n",
            "\n",
            "Epoch 297: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0260 - accuracy: 0.9949 - val_loss: 0.1189 - val_accuracy: 0.9906 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 298/1000\n",
            "\n",
            "Epoch 298: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9911 - val_loss: 0.1170 - val_accuracy: 0.9904 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 299/1000\n",
            "\n",
            "Epoch 299: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0245 - accuracy: 0.9956 - val_loss: 0.1147 - val_accuracy: 0.9911 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 300/1000\n",
            "\n",
            "Epoch 300: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0313 - accuracy: 0.9922 - val_loss: 0.1172 - val_accuracy: 0.9916 - lr: 0.0010 - 386ms/epoch - 97ms/step\n",
            "Epoch 301/1000\n",
            "\n",
            "Epoch 301: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0310 - accuracy: 0.9931 - val_loss: 0.1151 - val_accuracy: 0.9917 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 302/1000\n",
            "\n",
            "Epoch 302: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0342 - accuracy: 0.9949 - val_loss: 0.1175 - val_accuracy: 0.9909 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 303/1000\n",
            "\n",
            "Epoch 303: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0322 - accuracy: 0.9949 - val_loss: 0.1167 - val_accuracy: 0.9910 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 304/1000\n",
            "\n",
            "Epoch 304: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0318 - accuracy: 0.9915 - val_loss: 0.1174 - val_accuracy: 0.9897 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 305/1000\n",
            "\n",
            "Epoch 305: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0329 - accuracy: 0.9944 - val_loss: 0.1222 - val_accuracy: 0.9904 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 306/1000\n",
            "\n",
            "Epoch 306: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0328 - accuracy: 0.9944 - val_loss: 0.1163 - val_accuracy: 0.9896 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 307/1000\n",
            "\n",
            "Epoch 307: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0387 - accuracy: 0.9922 - val_loss: 0.1203 - val_accuracy: 0.9843 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 308/1000\n",
            "\n",
            "Epoch 308: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0358 - accuracy: 0.9910 - val_loss: 0.1251 - val_accuracy: 0.9911 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 309/1000\n",
            "\n",
            "Epoch 309: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0452 - accuracy: 0.9921 - val_loss: 0.1216 - val_accuracy: 0.9919 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 310/1000\n",
            "\n",
            "Epoch 310: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0423 - accuracy: 0.9930 - val_loss: 0.1190 - val_accuracy: 0.9903 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 311/1000\n",
            "\n",
            "Epoch 311: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0365 - accuracy: 0.9930 - val_loss: 0.1191 - val_accuracy: 0.9920 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 312/1000\n",
            "\n",
            "Epoch 312: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0443 - accuracy: 0.9934 - val_loss: 0.1195 - val_accuracy: 0.9904 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 313/1000\n",
            "\n",
            "Epoch 313: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0351 - accuracy: 0.9924 - val_loss: 0.1252 - val_accuracy: 0.9868 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 314/1000\n",
            "\n",
            "Epoch 314: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0385 - accuracy: 0.9924 - val_loss: 0.1196 - val_accuracy: 0.9916 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 315/1000\n",
            "\n",
            "Epoch 315: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0316 - accuracy: 0.9924 - val_loss: 0.1176 - val_accuracy: 0.9920 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 316/1000\n",
            "\n",
            "Epoch 316: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0290 - accuracy: 0.9933 - val_loss: 0.1168 - val_accuracy: 0.9920 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 317/1000\n",
            "\n",
            "Epoch 317: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0271 - accuracy: 0.9944 - val_loss: 0.1180 - val_accuracy: 0.9912 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 318/1000\n",
            "\n",
            "Epoch 318: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0238 - accuracy: 0.9921 - val_loss: 0.1157 - val_accuracy: 0.9904 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 319/1000\n",
            "\n",
            "Epoch 319: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0264 - accuracy: 0.9951 - val_loss: 0.1235 - val_accuracy: 0.9893 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 320/1000\n",
            "\n",
            "Epoch 320: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0324 - accuracy: 0.9929 - val_loss: 0.1180 - val_accuracy: 0.9917 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 321/1000\n",
            "\n",
            "Epoch 321: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0307 - accuracy: 0.9926 - val_loss: 0.1178 - val_accuracy: 0.9916 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 322/1000\n",
            "\n",
            "Epoch 322: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0300 - accuracy: 0.9929 - val_loss: 0.1218 - val_accuracy: 0.9905 - lr: 0.0010 - 344ms/epoch - 86ms/step\n",
            "Epoch 323/1000\n",
            "\n",
            "Epoch 323: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0347 - accuracy: 0.9937 - val_loss: 0.1195 - val_accuracy: 0.9893 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 324/1000\n",
            "\n",
            "Epoch 324: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0329 - accuracy: 0.9934 - val_loss: 0.1182 - val_accuracy: 0.9912 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 325/1000\n",
            "\n",
            "Epoch 325: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0317 - accuracy: 0.9945 - val_loss: 0.1183 - val_accuracy: 0.9894 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 326/1000\n",
            "\n",
            "Epoch 326: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0269 - accuracy: 0.9938 - val_loss: 0.1188 - val_accuracy: 0.9909 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 327/1000\n",
            "\n",
            "Epoch 327: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0238 - accuracy: 0.9937 - val_loss: 0.1176 - val_accuracy: 0.9910 - lr: 0.0010 - 383ms/epoch - 96ms/step\n",
            "Epoch 328/1000\n",
            "\n",
            "Epoch 328: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9953 - val_loss: 0.1204 - val_accuracy: 0.9898 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 329/1000\n",
            "\n",
            "Epoch 329: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9934 - val_loss: 0.1189 - val_accuracy: 0.9917 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 330/1000\n",
            "\n",
            "Epoch 330: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0306 - accuracy: 0.9936 - val_loss: 0.1175 - val_accuracy: 0.9900 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 331/1000\n",
            "\n",
            "Epoch 331: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0275 - accuracy: 0.9939 - val_loss: 0.1171 - val_accuracy: 0.9909 - lr: 0.0010 - 416ms/epoch - 104ms/step\n",
            "Epoch 332/1000\n",
            "\n",
            "Epoch 332: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0235 - accuracy: 0.9936 - val_loss: 0.1146 - val_accuracy: 0.9902 - lr: 0.0010 - 394ms/epoch - 99ms/step\n",
            "Epoch 333/1000\n",
            "\n",
            "Epoch 333: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0276 - accuracy: 0.9956 - val_loss: 0.1173 - val_accuracy: 0.9899 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 334/1000\n",
            "\n",
            "Epoch 334: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0277 - accuracy: 0.9933 - val_loss: 0.1180 - val_accuracy: 0.9896 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 335/1000\n",
            "\n",
            "Epoch 335: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.1164 - val_accuracy: 0.9902 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 336/1000\n",
            "\n",
            "Epoch 336: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0232 - accuracy: 0.9938 - val_loss: 0.1179 - val_accuracy: 0.9915 - lr: 0.0010 - 406ms/epoch - 102ms/step\n",
            "Epoch 337/1000\n",
            "\n",
            "Epoch 337: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0225 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9914 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 338/1000\n",
            "\n",
            "Epoch 338: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9956 - val_loss: 0.1200 - val_accuracy: 0.9912 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 339/1000\n",
            "\n",
            "Epoch 339: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0260 - accuracy: 0.9935 - val_loss: 0.1172 - val_accuracy: 0.9908 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 340/1000\n",
            "\n",
            "Epoch 340: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9936 - val_loss: 0.1173 - val_accuracy: 0.9903 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 341/1000\n",
            "\n",
            "Epoch 341: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0305 - accuracy: 0.9942 - val_loss: 0.1207 - val_accuracy: 0.9907 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 342/1000\n",
            "\n",
            "Epoch 342: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0331 - accuracy: 0.9943 - val_loss: 0.1231 - val_accuracy: 0.9909 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 343/1000\n",
            "\n",
            "Epoch 343: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0330 - accuracy: 0.9933 - val_loss: 0.1169 - val_accuracy: 0.9913 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 344/1000\n",
            "\n",
            "Epoch 344: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0267 - accuracy: 0.9929 - val_loss: 0.1177 - val_accuracy: 0.9885 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 345/1000\n",
            "\n",
            "Epoch 345: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9948 - val_loss: 0.1161 - val_accuracy: 0.9900 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 346/1000\n",
            "\n",
            "Epoch 346: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9927 - val_loss: 0.1182 - val_accuracy: 0.9904 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 347/1000\n",
            "\n",
            "Epoch 347: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9945 - val_loss: 0.1176 - val_accuracy: 0.9917 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 348/1000\n",
            "\n",
            "Epoch 348: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.1159 - val_accuracy: 0.9916 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 349/1000\n",
            "\n",
            "Epoch 349: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0213 - accuracy: 0.9947 - val_loss: 0.1203 - val_accuracy: 0.9896 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 350/1000\n",
            "\n",
            "Epoch 350: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0241 - accuracy: 0.9936 - val_loss: 0.1183 - val_accuracy: 0.9915 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 351/1000\n",
            "\n",
            "Epoch 351: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9953 - val_loss: 0.1191 - val_accuracy: 0.9908 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 352/1000\n",
            "\n",
            "Epoch 352: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0260 - accuracy: 0.9948 - val_loss: 0.1182 - val_accuracy: 0.9908 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 353/1000\n",
            "\n",
            "Epoch 353: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9932 - val_loss: 0.1174 - val_accuracy: 0.9901 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 354/1000\n",
            "\n",
            "Epoch 354: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0239 - accuracy: 0.9930 - val_loss: 0.1220 - val_accuracy: 0.9886 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 355/1000\n",
            "\n",
            "Epoch 355: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0292 - accuracy: 0.9928 - val_loss: 0.1172 - val_accuracy: 0.9870 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 356/1000\n",
            "\n",
            "Epoch 356: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9937 - val_loss: 0.1174 - val_accuracy: 0.9906 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 357/1000\n",
            "\n",
            "Epoch 357: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0259 - accuracy: 0.9950 - val_loss: 0.1182 - val_accuracy: 0.9895 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 358/1000\n",
            "\n",
            "Epoch 358: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0237 - accuracy: 0.9928 - val_loss: 0.1205 - val_accuracy: 0.9900 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 359/1000\n",
            "\n",
            "Epoch 359: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0328 - accuracy: 0.9930 - val_loss: 0.1169 - val_accuracy: 0.9902 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 360/1000\n",
            "\n",
            "Epoch 360: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0314 - accuracy: 0.9931 - val_loss: 0.1196 - val_accuracy: 0.9917 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 361/1000\n",
            "\n",
            "Epoch 361: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0337 - accuracy: 0.9940 - val_loss: 0.1167 - val_accuracy: 0.9912 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 362/1000\n",
            "\n",
            "Epoch 362: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0302 - accuracy: 0.9932 - val_loss: 0.1204 - val_accuracy: 0.9890 - lr: 0.0010 - 384ms/epoch - 96ms/step\n",
            "Epoch 363/1000\n",
            "\n",
            "Epoch 363: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0336 - accuracy: 0.9946 - val_loss: 0.1231 - val_accuracy: 0.9891 - lr: 0.0010 - 383ms/epoch - 96ms/step\n",
            "Epoch 364/1000\n",
            "\n",
            "Epoch 364: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0342 - accuracy: 0.9943 - val_loss: 0.1169 - val_accuracy: 0.9917 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 365/1000\n",
            "\n",
            "Epoch 365: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0291 - accuracy: 0.9926 - val_loss: 0.1171 - val_accuracy: 0.9914 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 366/1000\n",
            "\n",
            "Epoch 366: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0328 - accuracy: 0.9942 - val_loss: 0.1213 - val_accuracy: 0.9902 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 367/1000\n",
            "\n",
            "Epoch 367: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0306 - accuracy: 0.9949 - val_loss: 0.1181 - val_accuracy: 0.9896 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 368/1000\n",
            "\n",
            "Epoch 368: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0266 - accuracy: 0.9921 - val_loss: 0.1168 - val_accuracy: 0.9921 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 369/1000\n",
            "\n",
            "Epoch 369: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0278 - accuracy: 0.9941 - val_loss: 0.1164 - val_accuracy: 0.9910 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 370/1000\n",
            "\n",
            "Epoch 370: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0234 - accuracy: 0.9938 - val_loss: 0.1196 - val_accuracy: 0.9904 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 371/1000\n",
            "\n",
            "Epoch 371: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0233 - accuracy: 0.9942 - val_loss: 0.1155 - val_accuracy: 0.9904 - lr: 0.0010 - 383ms/epoch - 96ms/step\n",
            "Epoch 372/1000\n",
            "\n",
            "Epoch 372: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9934 - val_loss: 0.1170 - val_accuracy: 0.9909 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 373/1000\n",
            "\n",
            "Epoch 373: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9945 - val_loss: 0.1177 - val_accuracy: 0.9909 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 374/1000\n",
            "\n",
            "Epoch 374: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9948 - val_loss: 0.1160 - val_accuracy: 0.9910 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 375/1000\n",
            "\n",
            "Epoch 375: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.1187 - val_accuracy: 0.9902 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 376/1000\n",
            "\n",
            "Epoch 376: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0201 - accuracy: 0.9946 - val_loss: 0.1149 - val_accuracy: 0.9912 - lr: 0.0010 - 413ms/epoch - 103ms/step\n",
            "Epoch 377/1000\n",
            "\n",
            "Epoch 377: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9934 - val_loss: 0.1162 - val_accuracy: 0.9914 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 378/1000\n",
            "\n",
            "Epoch 378: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9942 - val_loss: 0.1194 - val_accuracy: 0.9913 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 379/1000\n",
            "\n",
            "Epoch 379: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0261 - accuracy: 0.9947 - val_loss: 0.1169 - val_accuracy: 0.9910 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 380/1000\n",
            "\n",
            "Epoch 380: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9932 - val_loss: 0.1180 - val_accuracy: 0.9912 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 381/1000\n",
            "\n",
            "Epoch 381: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0252 - accuracy: 0.9936 - val_loss: 0.1185 - val_accuracy: 0.9917 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 382/1000\n",
            "\n",
            "Epoch 382: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0214 - accuracy: 0.9940 - val_loss: 0.1161 - val_accuracy: 0.9908 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 383/1000\n",
            "\n",
            "Epoch 383: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0234 - accuracy: 0.9957 - val_loss: 0.1197 - val_accuracy: 0.9910 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 384/1000\n",
            "\n",
            "Epoch 384: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9951 - val_loss: 0.1178 - val_accuracy: 0.9896 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 385/1000\n",
            "\n",
            "Epoch 385: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9938 - val_loss: 0.1157 - val_accuracy: 0.9908 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 386/1000\n",
            "\n",
            "Epoch 386: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9934 - val_loss: 0.1203 - val_accuracy: 0.9916 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 387/1000\n",
            "\n",
            "Epoch 387: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0304 - accuracy: 0.9948 - val_loss: 0.1199 - val_accuracy: 0.9907 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 388/1000\n",
            "\n",
            "Epoch 388: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0285 - accuracy: 0.9940 - val_loss: 0.1159 - val_accuracy: 0.9916 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 389/1000\n",
            "\n",
            "Epoch 389: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0254 - accuracy: 0.9928 - val_loss: 0.1165 - val_accuracy: 0.9883 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 390/1000\n",
            "\n",
            "Epoch 390: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0289 - accuracy: 0.9934 - val_loss: 0.1191 - val_accuracy: 0.9916 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 391/1000\n",
            "\n",
            "Epoch 391: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0296 - accuracy: 0.9929 - val_loss: 0.1166 - val_accuracy: 0.9896 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 392/1000\n",
            "\n",
            "Epoch 392: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0235 - accuracy: 0.9938 - val_loss: 0.1177 - val_accuracy: 0.9901 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 393/1000\n",
            "\n",
            "Epoch 393: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9935 - val_loss: 0.1156 - val_accuracy: 0.9898 - lr: 0.0010 - 378ms/epoch - 95ms/step\n",
            "Epoch 394/1000\n",
            "\n",
            "Epoch 394: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0230 - accuracy: 0.9952 - val_loss: 0.1169 - val_accuracy: 0.9905 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 395/1000\n",
            "\n",
            "Epoch 395: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0236 - accuracy: 0.9938 - val_loss: 0.1165 - val_accuracy: 0.9904 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 396/1000\n",
            "\n",
            "Epoch 396: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0221 - accuracy: 0.9940 - val_loss: 0.1211 - val_accuracy: 0.9885 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 397/1000\n",
            "\n",
            "Epoch 397: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0273 - accuracy: 0.9937 - val_loss: 0.1158 - val_accuracy: 0.9900 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 398/1000\n",
            "\n",
            "Epoch 398: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0228 - accuracy: 0.9938 - val_loss: 0.1178 - val_accuracy: 0.9898 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 399/1000\n",
            "\n",
            "Epoch 399: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9924 - val_loss: 0.1156 - val_accuracy: 0.9904 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 400/1000\n",
            "\n",
            "Epoch 400: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0219 - accuracy: 0.9956 - val_loss: 0.1143 - val_accuracy: 0.9906 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 401/1000\n",
            "\n",
            "Epoch 401: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0234 - accuracy: 0.9940 - val_loss: 0.1195 - val_accuracy: 0.9909 - lr: 0.0010 - 418ms/epoch - 105ms/step\n",
            "Epoch 402/1000\n",
            "\n",
            "Epoch 402: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0267 - accuracy: 0.9944 - val_loss: 0.1160 - val_accuracy: 0.9915 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 403/1000\n",
            "\n",
            "Epoch 403: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0267 - accuracy: 0.9934 - val_loss: 0.1176 - val_accuracy: 0.9921 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 404/1000\n",
            "\n",
            "Epoch 404: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0319 - accuracy: 0.9901 - val_loss: 0.1184 - val_accuracy: 0.9909 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 405/1000\n",
            "\n",
            "Epoch 405: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0367 - accuracy: 0.9939 - val_loss: 0.1149 - val_accuracy: 0.9875 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 406/1000\n",
            "\n",
            "Epoch 406: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0304 - accuracy: 0.9926 - val_loss: 0.1167 - val_accuracy: 0.9919 - lr: 0.0010 - 421ms/epoch - 105ms/step\n",
            "Epoch 407/1000\n",
            "\n",
            "Epoch 407: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0353 - accuracy: 0.9905 - val_loss: 0.1179 - val_accuracy: 0.9918 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 408/1000\n",
            "\n",
            "Epoch 408: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0320 - accuracy: 0.9951 - val_loss: 0.1158 - val_accuracy: 0.9904 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 409/1000\n",
            "\n",
            "Epoch 409: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9916 - val_loss: 0.1194 - val_accuracy: 0.9917 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 410/1000\n",
            "\n",
            "Epoch 410: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0266 - accuracy: 0.9937 - val_loss: 0.1172 - val_accuracy: 0.9891 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 411/1000\n",
            "\n",
            "Epoch 411: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0260 - accuracy: 0.9927 - val_loss: 0.1183 - val_accuracy: 0.9921 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 412/1000\n",
            "\n",
            "Epoch 412: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0278 - accuracy: 0.9939 - val_loss: 0.1222 - val_accuracy: 0.9911 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 413/1000\n",
            "\n",
            "Epoch 413: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0291 - accuracy: 0.9932 - val_loss: 0.1159 - val_accuracy: 0.9918 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 414/1000\n",
            "\n",
            "Epoch 414: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9940 - val_loss: 0.1181 - val_accuracy: 0.9911 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 415/1000\n",
            "\n",
            "Epoch 415: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9934 - val_loss: 0.1167 - val_accuracy: 0.9915 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 416/1000\n",
            "\n",
            "Epoch 416: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9943 - val_loss: 0.1180 - val_accuracy: 0.9915 - lr: 0.0010 - 394ms/epoch - 98ms/step\n",
            "Epoch 417/1000\n",
            "\n",
            "Epoch 417: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0284 - accuracy: 0.9935 - val_loss: 0.1180 - val_accuracy: 0.9902 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 418/1000\n",
            "\n",
            "Epoch 418: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0228 - accuracy: 0.9947 - val_loss: 0.1175 - val_accuracy: 0.9912 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 419/1000\n",
            "\n",
            "Epoch 419: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0234 - accuracy: 0.9950 - val_loss: 0.1186 - val_accuracy: 0.9913 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 420/1000\n",
            "\n",
            "Epoch 420: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.1178 - val_accuracy: 0.9908 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 421/1000\n",
            "\n",
            "Epoch 421: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0253 - accuracy: 0.9940 - val_loss: 0.1180 - val_accuracy: 0.9903 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 422/1000\n",
            "\n",
            "Epoch 422: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0341 - accuracy: 0.9937 - val_loss: 0.1166 - val_accuracy: 0.9907 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 423/1000\n",
            "\n",
            "Epoch 423: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0306 - accuracy: 0.9936 - val_loss: 0.1190 - val_accuracy: 0.9918 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 424/1000\n",
            "\n",
            "Epoch 424: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0311 - accuracy: 0.9946 - val_loss: 0.1200 - val_accuracy: 0.9905 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 425/1000\n",
            "\n",
            "Epoch 425: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0313 - accuracy: 0.9945 - val_loss: 0.1289 - val_accuracy: 0.9881 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 426/1000\n",
            "\n",
            "Epoch 426: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0410 - accuracy: 0.9918 - val_loss: 0.1267 - val_accuracy: 0.9914 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 427/1000\n",
            "\n",
            "Epoch 427: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0375 - accuracy: 0.9922 - val_loss: 0.1219 - val_accuracy: 0.9875 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 428/1000\n",
            "\n",
            "Epoch 428: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0336 - accuracy: 0.9912 - val_loss: 0.1177 - val_accuracy: 0.9907 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 429/1000\n",
            "\n",
            "Epoch 429: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0297 - accuracy: 0.9948 - val_loss: 0.1174 - val_accuracy: 0.9919 - lr: 0.0010 - 346ms/epoch - 86ms/step\n",
            "Epoch 430/1000\n",
            "\n",
            "Epoch 430: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0314 - accuracy: 0.9944 - val_loss: 0.1220 - val_accuracy: 0.9878 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 431/1000\n",
            "\n",
            "Epoch 431: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0324 - accuracy: 0.9943 - val_loss: 0.1167 - val_accuracy: 0.9908 - lr: 0.0010 - 413ms/epoch - 103ms/step\n",
            "Epoch 432/1000\n",
            "\n",
            "Epoch 432: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0297 - accuracy: 0.9922 - val_loss: 0.1169 - val_accuracy: 0.9909 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 433/1000\n",
            "\n",
            "Epoch 433: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0327 - accuracy: 0.9927 - val_loss: 0.1142 - val_accuracy: 0.9913 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 434/1000\n",
            "\n",
            "Epoch 434: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0314 - accuracy: 0.9929 - val_loss: 0.1183 - val_accuracy: 0.9913 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 435/1000\n",
            "\n",
            "Epoch 435: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0293 - accuracy: 0.9946 - val_loss: 0.1173 - val_accuracy: 0.9914 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 436/1000\n",
            "\n",
            "Epoch 436: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9935 - val_loss: 0.1150 - val_accuracy: 0.9905 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 437/1000\n",
            "\n",
            "Epoch 437: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0256 - accuracy: 0.9954 - val_loss: 0.1157 - val_accuracy: 0.9910 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 438/1000\n",
            "\n",
            "Epoch 438: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0214 - accuracy: 0.9946 - val_loss: 0.1179 - val_accuracy: 0.9910 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 439/1000\n",
            "\n",
            "Epoch 439: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0226 - accuracy: 0.9940 - val_loss: 0.1176 - val_accuracy: 0.9905 - lr: 0.0010 - 394ms/epoch - 99ms/step\n",
            "Epoch 440/1000\n",
            "\n",
            "Epoch 440: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9941 - val_loss: 0.1179 - val_accuracy: 0.9910 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 441/1000\n",
            "\n",
            "Epoch 441: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0205 - accuracy: 0.9952 - val_loss: 0.1175 - val_accuracy: 0.9907 - lr: 0.0010 - 416ms/epoch - 104ms/step\n",
            "Epoch 442/1000\n",
            "\n",
            "Epoch 442: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.1178 - val_accuracy: 0.9906 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 443/1000\n",
            "\n",
            "Epoch 443: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9953 - val_loss: 0.1186 - val_accuracy: 0.9907 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 444/1000\n",
            "\n",
            "Epoch 444: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9943 - val_loss: 0.1189 - val_accuracy: 0.9898 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 445/1000\n",
            "\n",
            "Epoch 445: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.1178 - val_accuracy: 0.9911 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 446/1000\n",
            "\n",
            "Epoch 446: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0186 - accuracy: 0.9946 - val_loss: 0.1179 - val_accuracy: 0.9905 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 447/1000\n",
            "\n",
            "Epoch 447: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9953 - val_loss: 0.1178 - val_accuracy: 0.9906 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 448/1000\n",
            "\n",
            "Epoch 448: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.1173 - val_accuracy: 0.9911 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 449/1000\n",
            "\n",
            "Epoch 449: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.1162 - val_accuracy: 0.9904 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 450/1000\n",
            "\n",
            "Epoch 450: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.1196 - val_accuracy: 0.9912 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 451/1000\n",
            "\n",
            "Epoch 451: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.1146 - val_accuracy: 0.9914 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 452/1000\n",
            "\n",
            "Epoch 452: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9958 - val_loss: 0.1168 - val_accuracy: 0.9899 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 453/1000\n",
            "\n",
            "Epoch 453: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9922 - val_loss: 0.1156 - val_accuracy: 0.9907 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 454/1000\n",
            "\n",
            "Epoch 454: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9963 - val_loss: 0.1183 - val_accuracy: 0.9896 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 455/1000\n",
            "\n",
            "Epoch 455: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9936 - val_loss: 0.1188 - val_accuracy: 0.9908 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 456/1000\n",
            "\n",
            "Epoch 456: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9948 - val_loss: 0.1179 - val_accuracy: 0.9896 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 457/1000\n",
            "\n",
            "Epoch 457: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0246 - accuracy: 0.9949 - val_loss: 0.1179 - val_accuracy: 0.9912 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 458/1000\n",
            "\n",
            "Epoch 458: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0259 - accuracy: 0.9942 - val_loss: 0.1188 - val_accuracy: 0.9914 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 459/1000\n",
            "\n",
            "Epoch 459: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0261 - accuracy: 0.9937 - val_loss: 0.1205 - val_accuracy: 0.9890 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 460/1000\n",
            "\n",
            "Epoch 460: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0269 - accuracy: 0.9940 - val_loss: 0.1195 - val_accuracy: 0.9893 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 461/1000\n",
            "\n",
            "Epoch 461: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0271 - accuracy: 0.9941 - val_loss: 0.1170 - val_accuracy: 0.9889 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 462/1000\n",
            "\n",
            "Epoch 462: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0225 - accuracy: 0.9950 - val_loss: 0.1173 - val_accuracy: 0.9916 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 463/1000\n",
            "\n",
            "Epoch 463: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9943 - val_loss: 0.1174 - val_accuracy: 0.9914 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 464/1000\n",
            "\n",
            "Epoch 464: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0193 - accuracy: 0.9934 - val_loss: 0.1173 - val_accuracy: 0.9913 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 465/1000\n",
            "\n",
            "Epoch 465: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9942 - val_loss: 0.1189 - val_accuracy: 0.9895 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 466/1000\n",
            "\n",
            "Epoch 466: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9944 - val_loss: 0.1177 - val_accuracy: 0.9917 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 467/1000\n",
            "\n",
            "Epoch 467: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9930 - val_loss: 0.1162 - val_accuracy: 0.9906 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 468/1000\n",
            "\n",
            "Epoch 468: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9939 - val_loss: 0.1217 - val_accuracy: 0.9864 - lr: 0.0010 - 328ms/epoch - 82ms/step\n",
            "Epoch 469/1000\n",
            "\n",
            "Epoch 469: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0312 - accuracy: 0.9919 - val_loss: 0.1259 - val_accuracy: 0.9917 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 470/1000\n",
            "\n",
            "Epoch 470: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0364 - accuracy: 0.9940 - val_loss: 0.1163 - val_accuracy: 0.9910 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 471/1000\n",
            "\n",
            "Epoch 471: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0302 - accuracy: 0.9941 - val_loss: 0.1194 - val_accuracy: 0.9899 - lr: 0.0010 - 416ms/epoch - 104ms/step\n",
            "Epoch 472/1000\n",
            "\n",
            "Epoch 472: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0263 - accuracy: 0.9938 - val_loss: 0.1215 - val_accuracy: 0.9896 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 473/1000\n",
            "\n",
            "Epoch 473: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0292 - accuracy: 0.9936 - val_loss: 0.1143 - val_accuracy: 0.9915 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 474/1000\n",
            "\n",
            "Epoch 474: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.1167 - val_accuracy: 0.9906 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 475/1000\n",
            "\n",
            "Epoch 475: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0258 - accuracy: 0.9950 - val_loss: 0.1201 - val_accuracy: 0.9897 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 476/1000\n",
            "\n",
            "Epoch 476: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0305 - accuracy: 0.9932 - val_loss: 0.1174 - val_accuracy: 0.9903 - lr: 0.0010 - 415ms/epoch - 104ms/step\n",
            "Epoch 477/1000\n",
            "\n",
            "Epoch 477: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9930 - val_loss: 0.1183 - val_accuracy: 0.9910 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 478/1000\n",
            "\n",
            "Epoch 478: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0255 - accuracy: 0.9934 - val_loss: 0.1219 - val_accuracy: 0.9903 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 479/1000\n",
            "\n",
            "Epoch 479: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0306 - accuracy: 0.9913 - val_loss: 0.1197 - val_accuracy: 0.9909 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 480/1000\n",
            "\n",
            "Epoch 480: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0285 - accuracy: 0.9944 - val_loss: 0.1192 - val_accuracy: 0.9909 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 481/1000\n",
            "\n",
            "Epoch 481: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9934 - val_loss: 0.1170 - val_accuracy: 0.9904 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 482/1000\n",
            "\n",
            "Epoch 482: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0286 - accuracy: 0.9941 - val_loss: 0.1211 - val_accuracy: 0.9910 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 483/1000\n",
            "\n",
            "Epoch 483: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9928 - val_loss: 0.1159 - val_accuracy: 0.9905 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 484/1000\n",
            "\n",
            "Epoch 484: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0258 - accuracy: 0.9956 - val_loss: 0.1211 - val_accuracy: 0.9915 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 485/1000\n",
            "\n",
            "Epoch 485: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0249 - accuracy: 0.9940 - val_loss: 0.1170 - val_accuracy: 0.9902 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 486/1000\n",
            "\n",
            "Epoch 486: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0241 - accuracy: 0.9941 - val_loss: 0.1166 - val_accuracy: 0.9917 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 487/1000\n",
            "\n",
            "Epoch 487: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0263 - accuracy: 0.9951 - val_loss: 0.1223 - val_accuracy: 0.9905 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 488/1000\n",
            "\n",
            "Epoch 488: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0366 - accuracy: 0.9917 - val_loss: 0.1195 - val_accuracy: 0.9875 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 489/1000\n",
            "\n",
            "Epoch 489: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0306 - accuracy: 0.9913 - val_loss: 0.1217 - val_accuracy: 0.9906 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 490/1000\n",
            "\n",
            "Epoch 490: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0335 - accuracy: 0.9960 - val_loss: 0.1193 - val_accuracy: 0.9913 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 491/1000\n",
            "\n",
            "Epoch 491: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0347 - accuracy: 0.9932 - val_loss: 0.1159 - val_accuracy: 0.9920 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 492/1000\n",
            "\n",
            "Epoch 492: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0261 - accuracy: 0.9951 - val_loss: 0.1191 - val_accuracy: 0.9917 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 493/1000\n",
            "\n",
            "Epoch 493: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0277 - accuracy: 0.9926 - val_loss: 0.1169 - val_accuracy: 0.9920 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 494/1000\n",
            "\n",
            "Epoch 494: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0259 - accuracy: 0.9954 - val_loss: 0.1157 - val_accuracy: 0.9911 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 495/1000\n",
            "\n",
            "Epoch 495: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0247 - accuracy: 0.9935 - val_loss: 0.1204 - val_accuracy: 0.9896 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 496/1000\n",
            "\n",
            "Epoch 496: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0255 - accuracy: 0.9934 - val_loss: 0.1170 - val_accuracy: 0.9920 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 497/1000\n",
            "\n",
            "Epoch 497: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0252 - accuracy: 0.9936 - val_loss: 0.1185 - val_accuracy: 0.9920 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 498/1000\n",
            "\n",
            "Epoch 498: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9961 - val_loss: 0.1170 - val_accuracy: 0.9910 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 499/1000\n",
            "\n",
            "Epoch 499: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9927 - val_loss: 0.1167 - val_accuracy: 0.9911 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 500/1000\n",
            "\n",
            "Epoch 500: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0238 - accuracy: 0.9952 - val_loss: 0.1186 - val_accuracy: 0.9902 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 501/1000\n",
            "\n",
            "Epoch 501: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0220 - accuracy: 0.9945 - val_loss: 0.1164 - val_accuracy: 0.9914 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 502/1000\n",
            "\n",
            "Epoch 502: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9946 - val_loss: 0.1191 - val_accuracy: 0.9899 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 503/1000\n",
            "\n",
            "Epoch 503: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0211 - accuracy: 0.9953 - val_loss: 0.1174 - val_accuracy: 0.9916 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 504/1000\n",
            "\n",
            "Epoch 504: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9928 - val_loss: 0.1183 - val_accuracy: 0.9914 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 505/1000\n",
            "\n",
            "Epoch 505: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0267 - accuracy: 0.9959 - val_loss: 0.1209 - val_accuracy: 0.9900 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 506/1000\n",
            "\n",
            "Epoch 506: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0257 - accuracy: 0.9943 - val_loss: 0.1202 - val_accuracy: 0.9902 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 507/1000\n",
            "\n",
            "Epoch 507: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9941 - val_loss: 0.1165 - val_accuracy: 0.9907 - lr: 0.0010 - 386ms/epoch - 97ms/step\n",
            "Epoch 508/1000\n",
            "\n",
            "Epoch 508: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0301 - accuracy: 0.9945 - val_loss: 0.1195 - val_accuracy: 0.9919 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 509/1000\n",
            "\n",
            "Epoch 509: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0235 - accuracy: 0.9955 - val_loss: 0.1198 - val_accuracy: 0.9917 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 510/1000\n",
            "\n",
            "Epoch 510: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0282 - accuracy: 0.9929 - val_loss: 0.1168 - val_accuracy: 0.9915 - lr: 0.0010 - 394ms/epoch - 98ms/step\n",
            "Epoch 511/1000\n",
            "\n",
            "Epoch 511: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9943 - val_loss: 0.1166 - val_accuracy: 0.9900 - lr: 0.0010 - 414ms/epoch - 104ms/step\n",
            "Epoch 512/1000\n",
            "\n",
            "Epoch 512: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0244 - accuracy: 0.9946 - val_loss: 0.1221 - val_accuracy: 0.9890 - lr: 0.0010 - 411ms/epoch - 103ms/step\n",
            "Epoch 513/1000\n",
            "\n",
            "Epoch 513: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9943 - val_loss: 0.1153 - val_accuracy: 0.9910 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 514/1000\n",
            "\n",
            "Epoch 514: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0279 - accuracy: 0.9937 - val_loss: 0.1178 - val_accuracy: 0.9909 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 515/1000\n",
            "\n",
            "Epoch 515: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9952 - val_loss: 0.1191 - val_accuracy: 0.9895 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 516/1000\n",
            "\n",
            "Epoch 516: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0269 - accuracy: 0.9941 - val_loss: 0.1153 - val_accuracy: 0.9893 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 517/1000\n",
            "\n",
            "Epoch 517: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9942 - val_loss: 0.1155 - val_accuracy: 0.9909 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 518/1000\n",
            "\n",
            "Epoch 518: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0253 - accuracy: 0.9943 - val_loss: 0.1216 - val_accuracy: 0.9899 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 519/1000\n",
            "\n",
            "Epoch 519: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0269 - accuracy: 0.9936 - val_loss: 0.1158 - val_accuracy: 0.9901 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 520/1000\n",
            "\n",
            "Epoch 520: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9942 - val_loss: 0.1166 - val_accuracy: 0.9911 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 521/1000\n",
            "\n",
            "Epoch 521: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9943 - val_loss: 0.1193 - val_accuracy: 0.9894 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 522/1000\n",
            "\n",
            "Epoch 522: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0220 - accuracy: 0.9958 - val_loss: 0.1172 - val_accuracy: 0.9907 - lr: 0.0010 - 345ms/epoch - 86ms/step\n",
            "Epoch 523/1000\n",
            "\n",
            "Epoch 523: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9918 - val_loss: 0.1135 - val_accuracy: 0.9898 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 524/1000\n",
            "\n",
            "Epoch 524: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0362 - accuracy: 0.9946 - val_loss: 0.1185 - val_accuracy: 0.9919 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 525/1000\n",
            "\n",
            "Epoch 525: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0339 - accuracy: 0.9941 - val_loss: 0.1220 - val_accuracy: 0.9906 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 526/1000\n",
            "\n",
            "Epoch 526: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0283 - accuracy: 0.9930 - val_loss: 0.1178 - val_accuracy: 0.9890 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 527/1000\n",
            "\n",
            "Epoch 527: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0264 - accuracy: 0.9949 - val_loss: 0.1203 - val_accuracy: 0.9909 - lr: 0.0010 - 345ms/epoch - 86ms/step\n",
            "Epoch 528/1000\n",
            "\n",
            "Epoch 528: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0281 - accuracy: 0.9916 - val_loss: 0.1183 - val_accuracy: 0.9899 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 529/1000\n",
            "\n",
            "Epoch 529: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0254 - accuracy: 0.9943 - val_loss: 0.1178 - val_accuracy: 0.9917 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 530/1000\n",
            "\n",
            "Epoch 530: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0300 - accuracy: 0.9949 - val_loss: 0.1192 - val_accuracy: 0.9910 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 531/1000\n",
            "\n",
            "Epoch 531: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9942 - val_loss: 0.1189 - val_accuracy: 0.9910 - lr: 0.0010 - 394ms/epoch - 99ms/step\n",
            "Epoch 532/1000\n",
            "\n",
            "Epoch 532: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0256 - accuracy: 0.9920 - val_loss: 0.1168 - val_accuracy: 0.9920 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 533/1000\n",
            "\n",
            "Epoch 533: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.1210 - val_accuracy: 0.9907 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 534/1000\n",
            "\n",
            "Epoch 534: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9957 - val_loss: 0.1155 - val_accuracy: 0.9915 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 535/1000\n",
            "\n",
            "Epoch 535: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9942 - val_loss: 0.1194 - val_accuracy: 0.9892 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 536/1000\n",
            "\n",
            "Epoch 536: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0213 - accuracy: 0.9940 - val_loss: 0.1159 - val_accuracy: 0.9912 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 537/1000\n",
            "\n",
            "Epoch 537: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9946 - val_loss: 0.1190 - val_accuracy: 0.9904 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 538/1000\n",
            "\n",
            "Epoch 538: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0288 - accuracy: 0.9930 - val_loss: 0.1202 - val_accuracy: 0.9907 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 539/1000\n",
            "\n",
            "Epoch 539: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0289 - accuracy: 0.9951 - val_loss: 0.1169 - val_accuracy: 0.9919 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 540/1000\n",
            "\n",
            "Epoch 540: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0235 - accuracy: 0.9945 - val_loss: 0.1168 - val_accuracy: 0.9918 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 541/1000\n",
            "\n",
            "Epoch 541: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0364 - accuracy: 0.9946 - val_loss: 0.1144 - val_accuracy: 0.9913 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 542/1000\n",
            "\n",
            "Epoch 542: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0306 - accuracy: 0.9932 - val_loss: 0.1188 - val_accuracy: 0.9922 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 543/1000\n",
            "\n",
            "Epoch 543: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0305 - accuracy: 0.9946 - val_loss: 0.1190 - val_accuracy: 0.9907 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 544/1000\n",
            "\n",
            "Epoch 544: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0321 - accuracy: 0.9933 - val_loss: 0.1168 - val_accuracy: 0.9895 - lr: 0.0010 - 406ms/epoch - 102ms/step\n",
            "Epoch 545/1000\n",
            "\n",
            "Epoch 545: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0302 - accuracy: 0.9941 - val_loss: 0.1162 - val_accuracy: 0.9920 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 546/1000\n",
            "\n",
            "Epoch 546: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0261 - accuracy: 0.9944 - val_loss: 0.1178 - val_accuracy: 0.9897 - lr: 0.0010 - 422ms/epoch - 105ms/step\n",
            "Epoch 547/1000\n",
            "\n",
            "Epoch 547: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9945 - val_loss: 0.1186 - val_accuracy: 0.9908 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 548/1000\n",
            "\n",
            "Epoch 548: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.1173 - val_accuracy: 0.9904 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 549/1000\n",
            "\n",
            "Epoch 549: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9928 - val_loss: 0.1169 - val_accuracy: 0.9920 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 550/1000\n",
            "\n",
            "Epoch 550: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0192 - accuracy: 0.9957 - val_loss: 0.1185 - val_accuracy: 0.9914 - lr: 0.0010 - 406ms/epoch - 102ms/step\n",
            "Epoch 551/1000\n",
            "\n",
            "Epoch 551: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.1164 - val_accuracy: 0.9911 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 552/1000\n",
            "\n",
            "Epoch 552: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9946 - val_loss: 0.1177 - val_accuracy: 0.9915 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 553/1000\n",
            "\n",
            "Epoch 553: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.1168 - val_accuracy: 0.9910 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 554/1000\n",
            "\n",
            "Epoch 554: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.1160 - val_accuracy: 0.9908 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 555/1000\n",
            "\n",
            "Epoch 555: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9957 - val_loss: 0.1157 - val_accuracy: 0.9913 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 556/1000\n",
            "\n",
            "Epoch 556: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.1160 - val_accuracy: 0.9904 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 557/1000\n",
            "\n",
            "Epoch 557: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0198 - accuracy: 0.9945 - val_loss: 0.1154 - val_accuracy: 0.9909 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 558/1000\n",
            "\n",
            "Epoch 558: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9955 - val_loss: 0.1176 - val_accuracy: 0.9904 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 559/1000\n",
            "\n",
            "Epoch 559: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9948 - val_loss: 0.1165 - val_accuracy: 0.9910 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 560/1000\n",
            "\n",
            "Epoch 560: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9950 - val_loss: 0.1152 - val_accuracy: 0.9902 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 561/1000\n",
            "\n",
            "Epoch 561: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9938 - val_loss: 0.1185 - val_accuracy: 0.9913 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 562/1000\n",
            "\n",
            "Epoch 562: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9964 - val_loss: 0.1158 - val_accuracy: 0.9915 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 563/1000\n",
            "\n",
            "Epoch 563: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9929 - val_loss: 0.1191 - val_accuracy: 0.9920 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 564/1000\n",
            "\n",
            "Epoch 564: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9952 - val_loss: 0.1160 - val_accuracy: 0.9914 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 565/1000\n",
            "\n",
            "Epoch 565: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9951 - val_loss: 0.1165 - val_accuracy: 0.9919 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 566/1000\n",
            "\n",
            "Epoch 566: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.1153 - val_accuracy: 0.9904 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 567/1000\n",
            "\n",
            "Epoch 567: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0217 - accuracy: 0.9945 - val_loss: 0.1159 - val_accuracy: 0.9920 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 568/1000\n",
            "\n",
            "Epoch 568: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.1212 - val_accuracy: 0.9898 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 569/1000\n",
            "\n",
            "Epoch 569: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0250 - accuracy: 0.9952 - val_loss: 0.1162 - val_accuracy: 0.9915 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 570/1000\n",
            "\n",
            "Epoch 570: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9953 - val_loss: 0.1232 - val_accuracy: 0.9909 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 571/1000\n",
            "\n",
            "Epoch 571: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0288 - accuracy: 0.9929 - val_loss: 0.1179 - val_accuracy: 0.9849 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 572/1000\n",
            "\n",
            "Epoch 572: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0281 - accuracy: 0.9923 - val_loss: 0.1197 - val_accuracy: 0.9896 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 573/1000\n",
            "\n",
            "Epoch 573: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0258 - accuracy: 0.9958 - val_loss: 0.1177 - val_accuracy: 0.9902 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 574/1000\n",
            "\n",
            "Epoch 574: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9942 - val_loss: 0.1192 - val_accuracy: 0.9917 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 575/1000\n",
            "\n",
            "Epoch 575: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9944 - val_loss: 0.1170 - val_accuracy: 0.9908 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 576/1000\n",
            "\n",
            "Epoch 576: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0203 - accuracy: 0.9949 - val_loss: 0.1159 - val_accuracy: 0.9903 - lr: 0.0010 - 404ms/epoch - 101ms/step\n",
            "Epoch 577/1000\n",
            "\n",
            "Epoch 577: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9964 - val_loss: 0.1169 - val_accuracy: 0.9900 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 578/1000\n",
            "\n",
            "Epoch 578: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9948 - val_loss: 0.1195 - val_accuracy: 0.9897 - lr: 0.0010 - 348ms/epoch - 87ms/step\n",
            "Epoch 579/1000\n",
            "\n",
            "Epoch 579: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.1157 - val_accuracy: 0.9915 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 580/1000\n",
            "\n",
            "Epoch 580: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9952 - val_loss: 0.1161 - val_accuracy: 0.9908 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 581/1000\n",
            "\n",
            "Epoch 581: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0184 - accuracy: 0.9946 - val_loss: 0.1195 - val_accuracy: 0.9907 - lr: 0.0010 - 420ms/epoch - 105ms/step\n",
            "Epoch 582/1000\n",
            "\n",
            "Epoch 582: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.1174 - val_accuracy: 0.9909 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 583/1000\n",
            "\n",
            "Epoch 583: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9952 - val_loss: 0.1182 - val_accuracy: 0.9903 - lr: 0.0010 - 394ms/epoch - 98ms/step\n",
            "Epoch 584/1000\n",
            "\n",
            "Epoch 584: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9956 - val_loss: 0.1174 - val_accuracy: 0.9914 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 585/1000\n",
            "\n",
            "Epoch 585: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9944 - val_loss: 0.1189 - val_accuracy: 0.9890 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 586/1000\n",
            "\n",
            "Epoch 586: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9944 - val_loss: 0.1170 - val_accuracy: 0.9918 - lr: 0.0010 - 421ms/epoch - 105ms/step\n",
            "Epoch 587/1000\n",
            "\n",
            "Epoch 587: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0217 - accuracy: 0.9966 - val_loss: 0.1213 - val_accuracy: 0.9899 - lr: 0.0010 - 382ms/epoch - 95ms/step\n",
            "Epoch 588/1000\n",
            "\n",
            "Epoch 588: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0270 - accuracy: 0.9928 - val_loss: 0.1171 - val_accuracy: 0.9913 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 589/1000\n",
            "\n",
            "Epoch 589: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.1155 - val_accuracy: 0.9911 - lr: 0.0010 - 344ms/epoch - 86ms/step\n",
            "Epoch 590/1000\n",
            "\n",
            "Epoch 590: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9957 - val_loss: 0.1194 - val_accuracy: 0.9900 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 591/1000\n",
            "\n",
            "Epoch 591: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0233 - accuracy: 0.9948 - val_loss: 0.1175 - val_accuracy: 0.9909 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 592/1000\n",
            "\n",
            "Epoch 592: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9933 - val_loss: 0.1189 - val_accuracy: 0.9900 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 593/1000\n",
            "\n",
            "Epoch 593: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0287 - accuracy: 0.9950 - val_loss: 0.1146 - val_accuracy: 0.9918 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 594/1000\n",
            "\n",
            "Epoch 594: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0271 - accuracy: 0.9943 - val_loss: 0.1226 - val_accuracy: 0.9920 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 595/1000\n",
            "\n",
            "Epoch 595: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9944 - val_loss: 0.1181 - val_accuracy: 0.9906 - lr: 0.0010 - 345ms/epoch - 86ms/step\n",
            "Epoch 596/1000\n",
            "\n",
            "Epoch 596: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9944 - val_loss: 0.1176 - val_accuracy: 0.9912 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 597/1000\n",
            "\n",
            "Epoch 597: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0205 - accuracy: 0.9960 - val_loss: 0.1207 - val_accuracy: 0.9904 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 598/1000\n",
            "\n",
            "Epoch 598: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.1172 - val_accuracy: 0.9920 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 599/1000\n",
            "\n",
            "Epoch 599: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0216 - accuracy: 0.9948 - val_loss: 0.1198 - val_accuracy: 0.9909 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 600/1000\n",
            "\n",
            "Epoch 600: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9943 - val_loss: 0.1190 - val_accuracy: 0.9886 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 601/1000\n",
            "\n",
            "Epoch 601: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9940 - val_loss: 0.1177 - val_accuracy: 0.9911 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 602/1000\n",
            "\n",
            "Epoch 602: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9925 - val_loss: 0.1179 - val_accuracy: 0.9921 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 603/1000\n",
            "\n",
            "Epoch 603: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9947 - val_loss: 0.1188 - val_accuracy: 0.9895 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 604/1000\n",
            "\n",
            "Epoch 604: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0226 - accuracy: 0.9945 - val_loss: 0.1156 - val_accuracy: 0.9895 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 605/1000\n",
            "\n",
            "Epoch 605: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9944 - val_loss: 0.1197 - val_accuracy: 0.9919 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 606/1000\n",
            "\n",
            "Epoch 606: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9949 - val_loss: 0.1169 - val_accuracy: 0.9913 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 607/1000\n",
            "\n",
            "Epoch 607: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9956 - val_loss: 0.1164 - val_accuracy: 0.9918 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 608/1000\n",
            "\n",
            "Epoch 608: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0193 - accuracy: 0.9945 - val_loss: 0.1174 - val_accuracy: 0.9911 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 609/1000\n",
            "\n",
            "Epoch 609: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.1154 - val_accuracy: 0.9914 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 610/1000\n",
            "\n",
            "Epoch 610: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9948 - val_loss: 0.1203 - val_accuracy: 0.9916 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 611/1000\n",
            "\n",
            "Epoch 611: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9948 - val_loss: 0.1177 - val_accuracy: 0.9893 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 612/1000\n",
            "\n",
            "Epoch 612: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0274 - accuracy: 0.9947 - val_loss: 0.1179 - val_accuracy: 0.9905 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 613/1000\n",
            "\n",
            "Epoch 613: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0250 - accuracy: 0.9929 - val_loss: 0.1174 - val_accuracy: 0.9903 - lr: 0.0010 - 378ms/epoch - 95ms/step\n",
            "Epoch 614/1000\n",
            "\n",
            "Epoch 614: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9960 - val_loss: 0.1183 - val_accuracy: 0.9891 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 615/1000\n",
            "\n",
            "Epoch 615: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0220 - accuracy: 0.9932 - val_loss: 0.1174 - val_accuracy: 0.9911 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 616/1000\n",
            "\n",
            "Epoch 616: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.1191 - val_accuracy: 0.9915 - lr: 0.0010 - 427ms/epoch - 107ms/step\n",
            "Epoch 617/1000\n",
            "\n",
            "Epoch 617: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9937 - val_loss: 0.1185 - val_accuracy: 0.9894 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 618/1000\n",
            "\n",
            "Epoch 618: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0226 - accuracy: 0.9937 - val_loss: 0.1185 - val_accuracy: 0.9910 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 619/1000\n",
            "\n",
            "Epoch 619: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9953 - val_loss: 0.1202 - val_accuracy: 0.9905 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 620/1000\n",
            "\n",
            "Epoch 620: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0236 - accuracy: 0.9953 - val_loss: 0.1192 - val_accuracy: 0.9900 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 621/1000\n",
            "\n",
            "Epoch 621: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0239 - accuracy: 0.9940 - val_loss: 0.1188 - val_accuracy: 0.9908 - lr: 0.0010 - 420ms/epoch - 105ms/step\n",
            "Epoch 622/1000\n",
            "\n",
            "Epoch 622: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0220 - accuracy: 0.9941 - val_loss: 0.1216 - val_accuracy: 0.9899 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 623/1000\n",
            "\n",
            "Epoch 623: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0225 - accuracy: 0.9944 - val_loss: 0.1202 - val_accuracy: 0.9900 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 624/1000\n",
            "\n",
            "Epoch 624: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.1177 - val_accuracy: 0.9906 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 625/1000\n",
            "\n",
            "Epoch 625: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0209 - accuracy: 0.9948 - val_loss: 0.1181 - val_accuracy: 0.9910 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 626/1000\n",
            "\n",
            "Epoch 626: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.1181 - val_accuracy: 0.9911 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 627/1000\n",
            "\n",
            "Epoch 627: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9947 - val_loss: 0.1160 - val_accuracy: 0.9902 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 628/1000\n",
            "\n",
            "Epoch 628: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0186 - accuracy: 0.9960 - val_loss: 0.1212 - val_accuracy: 0.9905 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 629/1000\n",
            "\n",
            "Epoch 629: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0283 - accuracy: 0.9933 - val_loss: 0.1186 - val_accuracy: 0.9903 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 630/1000\n",
            "\n",
            "Epoch 630: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0304 - accuracy: 0.9949 - val_loss: 0.1176 - val_accuracy: 0.9916 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 631/1000\n",
            "\n",
            "Epoch 631: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0282 - accuracy: 0.9942 - val_loss: 0.1188 - val_accuracy: 0.9890 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 632/1000\n",
            "\n",
            "Epoch 632: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9940 - val_loss: 0.1138 - val_accuracy: 0.9914 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 633/1000\n",
            "\n",
            "Epoch 633: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0273 - accuracy: 0.9939 - val_loss: 0.1192 - val_accuracy: 0.9919 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 634/1000\n",
            "\n",
            "Epoch 634: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9924 - val_loss: 0.1173 - val_accuracy: 0.9888 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 635/1000\n",
            "\n",
            "Epoch 635: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0259 - accuracy: 0.9944 - val_loss: 0.1167 - val_accuracy: 0.9916 - lr: 0.0010 - 407ms/epoch - 102ms/step\n",
            "Epoch 636/1000\n",
            "\n",
            "Epoch 636: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9928 - val_loss: 0.1185 - val_accuracy: 0.9910 - lr: 0.0010 - 407ms/epoch - 102ms/step\n",
            "Epoch 637/1000\n",
            "\n",
            "Epoch 637: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9948 - val_loss: 0.1186 - val_accuracy: 0.9907 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 638/1000\n",
            "\n",
            "Epoch 638: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9939 - val_loss: 0.1181 - val_accuracy: 0.9909 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 639/1000\n",
            "\n",
            "Epoch 639: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0320 - accuracy: 0.9932 - val_loss: 0.1199 - val_accuracy: 0.9919 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 640/1000\n",
            "\n",
            "Epoch 640: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9941 - val_loss: 0.1190 - val_accuracy: 0.9886 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 641/1000\n",
            "\n",
            "Epoch 641: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0252 - accuracy: 0.9942 - val_loss: 0.1150 - val_accuracy: 0.9914 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 642/1000\n",
            "\n",
            "Epoch 642: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0193 - accuracy: 0.9955 - val_loss: 0.1159 - val_accuracy: 0.9916 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 643/1000\n",
            "\n",
            "Epoch 643: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0198 - accuracy: 0.9943 - val_loss: 0.1166 - val_accuracy: 0.9914 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 644/1000\n",
            "\n",
            "Epoch 644: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9951 - val_loss: 0.1175 - val_accuracy: 0.9914 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 645/1000\n",
            "\n",
            "Epoch 645: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9941 - val_loss: 0.1159 - val_accuracy: 0.9894 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 646/1000\n",
            "\n",
            "Epoch 646: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0205 - accuracy: 0.9947 - val_loss: 0.1177 - val_accuracy: 0.9913 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 647/1000\n",
            "\n",
            "Epoch 647: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1159 - val_accuracy: 0.9900 - lr: 0.0010 - 330ms/epoch - 83ms/step\n",
            "Epoch 648/1000\n",
            "\n",
            "Epoch 648: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0222 - accuracy: 0.9947 - val_loss: 0.1173 - val_accuracy: 0.9913 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 649/1000\n",
            "\n",
            "Epoch 649: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9944 - val_loss: 0.1188 - val_accuracy: 0.9907 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 650/1000\n",
            "\n",
            "Epoch 650: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9957 - val_loss: 0.1157 - val_accuracy: 0.9918 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 651/1000\n",
            "\n",
            "Epoch 651: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0192 - accuracy: 0.9953 - val_loss: 0.1176 - val_accuracy: 0.9904 - lr: 0.0010 - 418ms/epoch - 104ms/step\n",
            "Epoch 652/1000\n",
            "\n",
            "Epoch 652: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9956 - val_loss: 0.1155 - val_accuracy: 0.9915 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 653/1000\n",
            "\n",
            "Epoch 653: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.1177 - val_accuracy: 0.9911 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 654/1000\n",
            "\n",
            "Epoch 654: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9943 - val_loss: 0.1171 - val_accuracy: 0.9904 - lr: 0.0010 - 382ms/epoch - 96ms/step\n",
            "Epoch 655/1000\n",
            "\n",
            "Epoch 655: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0230 - accuracy: 0.9947 - val_loss: 0.1146 - val_accuracy: 0.9915 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 656/1000\n",
            "\n",
            "Epoch 656: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9946 - val_loss: 0.1175 - val_accuracy: 0.9916 - lr: 0.0010 - 412ms/epoch - 103ms/step\n",
            "Epoch 657/1000\n",
            "\n",
            "Epoch 657: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0246 - accuracy: 0.9952 - val_loss: 0.1211 - val_accuracy: 0.9876 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 658/1000\n",
            "\n",
            "Epoch 658: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0298 - accuracy: 0.9936 - val_loss: 0.1168 - val_accuracy: 0.9918 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 659/1000\n",
            "\n",
            "Epoch 659: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9945 - val_loss: 0.1159 - val_accuracy: 0.9915 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 660/1000\n",
            "\n",
            "Epoch 660: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9950 - val_loss: 0.1171 - val_accuracy: 0.9899 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 661/1000\n",
            "\n",
            "Epoch 661: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9943 - val_loss: 0.1160 - val_accuracy: 0.9916 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 662/1000\n",
            "\n",
            "Epoch 662: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0214 - accuracy: 0.9958 - val_loss: 0.1173 - val_accuracy: 0.9914 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 663/1000\n",
            "\n",
            "Epoch 663: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9941 - val_loss: 0.1170 - val_accuracy: 0.9897 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 664/1000\n",
            "\n",
            "Epoch 664: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0236 - accuracy: 0.9943 - val_loss: 0.1203 - val_accuracy: 0.9921 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 665/1000\n",
            "\n",
            "Epoch 665: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0269 - accuracy: 0.9956 - val_loss: 0.1175 - val_accuracy: 0.9898 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 666/1000\n",
            "\n",
            "Epoch 666: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0333 - accuracy: 0.9937 - val_loss: 0.1158 - val_accuracy: 0.9916 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 667/1000\n",
            "\n",
            "Epoch 667: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0284 - accuracy: 0.9932 - val_loss: 0.1213 - val_accuracy: 0.9921 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 668/1000\n",
            "\n",
            "Epoch 668: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0297 - accuracy: 0.9936 - val_loss: 0.1219 - val_accuracy: 0.9893 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 669/1000\n",
            "\n",
            "Epoch 669: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0295 - accuracy: 0.9940 - val_loss: 0.1176 - val_accuracy: 0.9921 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 670/1000\n",
            "\n",
            "Epoch 670: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9938 - val_loss: 0.1167 - val_accuracy: 0.9920 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 671/1000\n",
            "\n",
            "Epoch 671: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0282 - accuracy: 0.9946 - val_loss: 0.1187 - val_accuracy: 0.9899 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 672/1000\n",
            "\n",
            "Epoch 672: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9950 - val_loss: 0.1186 - val_accuracy: 0.9916 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 673/1000\n",
            "\n",
            "Epoch 673: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9919 - val_loss: 0.1173 - val_accuracy: 0.9919 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 674/1000\n",
            "\n",
            "Epoch 674: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9959 - val_loss: 0.1197 - val_accuracy: 0.9911 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 675/1000\n",
            "\n",
            "Epoch 675: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0258 - accuracy: 0.9943 - val_loss: 0.1162 - val_accuracy: 0.9911 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 676/1000\n",
            "\n",
            "Epoch 676: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.1162 - val_accuracy: 0.9918 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 677/1000\n",
            "\n",
            "Epoch 677: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9960 - val_loss: 0.1254 - val_accuracy: 0.9902 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 678/1000\n",
            "\n",
            "Epoch 678: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0349 - accuracy: 0.9923 - val_loss: 0.1186 - val_accuracy: 0.9895 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 679/1000\n",
            "\n",
            "Epoch 679: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0253 - accuracy: 0.9946 - val_loss: 0.1183 - val_accuracy: 0.9912 - lr: 0.0010 - 349ms/epoch - 87ms/step\n",
            "Epoch 680/1000\n",
            "\n",
            "Epoch 680: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9938 - val_loss: 0.1171 - val_accuracy: 0.9906 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 681/1000\n",
            "\n",
            "Epoch 681: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9955 - val_loss: 0.1146 - val_accuracy: 0.9915 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 682/1000\n",
            "\n",
            "Epoch 682: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9925 - val_loss: 0.1169 - val_accuracy: 0.9897 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 683/1000\n",
            "\n",
            "Epoch 683: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0217 - accuracy: 0.9966 - val_loss: 0.1169 - val_accuracy: 0.9916 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 684/1000\n",
            "\n",
            "Epoch 684: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0221 - accuracy: 0.9942 - val_loss: 0.1164 - val_accuracy: 0.9915 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 685/1000\n",
            "\n",
            "Epoch 685: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0255 - accuracy: 0.9942 - val_loss: 0.1174 - val_accuracy: 0.9912 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 686/1000\n",
            "\n",
            "Epoch 686: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9951 - val_loss: 0.1170 - val_accuracy: 0.9906 - lr: 0.0010 - 413ms/epoch - 103ms/step\n",
            "Epoch 687/1000\n",
            "\n",
            "Epoch 687: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9955 - val_loss: 0.1163 - val_accuracy: 0.9914 - lr: 0.0010 - 406ms/epoch - 101ms/step\n",
            "Epoch 688/1000\n",
            "\n",
            "Epoch 688: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0256 - accuracy: 0.9942 - val_loss: 0.1178 - val_accuracy: 0.9895 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 689/1000\n",
            "\n",
            "Epoch 689: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0241 - accuracy: 0.9933 - val_loss: 0.1197 - val_accuracy: 0.9897 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 690/1000\n",
            "\n",
            "Epoch 690: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9942 - val_loss: 0.1176 - val_accuracy: 0.9907 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 691/1000\n",
            "\n",
            "Epoch 691: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0209 - accuracy: 0.9942 - val_loss: 0.1153 - val_accuracy: 0.9905 - lr: 0.0010 - 406ms/epoch - 102ms/step\n",
            "Epoch 692/1000\n",
            "\n",
            "Epoch 692: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0245 - accuracy: 0.9954 - val_loss: 0.1188 - val_accuracy: 0.9904 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 693/1000\n",
            "\n",
            "Epoch 693: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0245 - accuracy: 0.9934 - val_loss: 0.1182 - val_accuracy: 0.9900 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 694/1000\n",
            "\n",
            "Epoch 694: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9929 - val_loss: 0.1168 - val_accuracy: 0.9916 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 695/1000\n",
            "\n",
            "Epoch 695: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0221 - accuracy: 0.9952 - val_loss: 0.1181 - val_accuracy: 0.9892 - lr: 0.0010 - 346ms/epoch - 87ms/step\n",
            "Epoch 696/1000\n",
            "\n",
            "Epoch 696: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0225 - accuracy: 0.9947 - val_loss: 0.1177 - val_accuracy: 0.9897 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 697/1000\n",
            "\n",
            "Epoch 697: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0213 - accuracy: 0.9945 - val_loss: 0.1177 - val_accuracy: 0.9903 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 698/1000\n",
            "\n",
            "Epoch 698: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9929 - val_loss: 0.1160 - val_accuracy: 0.9901 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 699/1000\n",
            "\n",
            "Epoch 699: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9947 - val_loss: 0.1188 - val_accuracy: 0.9912 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 700/1000\n",
            "\n",
            "Epoch 700: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0217 - accuracy: 0.9953 - val_loss: 0.1153 - val_accuracy: 0.9899 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 701/1000\n",
            "\n",
            "Epoch 701: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9940 - val_loss: 0.1155 - val_accuracy: 0.9913 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 702/1000\n",
            "\n",
            "Epoch 702: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9950 - val_loss: 0.1172 - val_accuracy: 0.9888 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 703/1000\n",
            "\n",
            "Epoch 703: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0237 - accuracy: 0.9939 - val_loss: 0.1146 - val_accuracy: 0.9899 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 704/1000\n",
            "\n",
            "Epoch 704: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9947 - val_loss: 0.1153 - val_accuracy: 0.9911 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 705/1000\n",
            "\n",
            "Epoch 705: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0263 - accuracy: 0.9940 - val_loss: 0.1144 - val_accuracy: 0.9911 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 706/1000\n",
            "\n",
            "Epoch 706: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0333 - accuracy: 0.9922 - val_loss: 0.1182 - val_accuracy: 0.9909 - lr: 0.0010 - 408ms/epoch - 102ms/step\n",
            "Epoch 707/1000\n",
            "\n",
            "Epoch 707: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0294 - accuracy: 0.9935 - val_loss: 0.1166 - val_accuracy: 0.9876 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 708/1000\n",
            "\n",
            "Epoch 708: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0261 - accuracy: 0.9940 - val_loss: 0.1168 - val_accuracy: 0.9914 - lr: 0.0010 - 382ms/epoch - 95ms/step\n",
            "Epoch 709/1000\n",
            "\n",
            "Epoch 709: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.1155 - val_accuracy: 0.9911 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 710/1000\n",
            "\n",
            "Epoch 710: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0198 - accuracy: 0.9949 - val_loss: 0.1173 - val_accuracy: 0.9915 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 711/1000\n",
            "\n",
            "Epoch 711: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9953 - val_loss: 0.1165 - val_accuracy: 0.9901 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 712/1000\n",
            "\n",
            "Epoch 712: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0219 - accuracy: 0.9940 - val_loss: 0.1156 - val_accuracy: 0.9911 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 713/1000\n",
            "\n",
            "Epoch 713: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9960 - val_loss: 0.1160 - val_accuracy: 0.9904 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 714/1000\n",
            "\n",
            "Epoch 714: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9930 - val_loss: 0.1174 - val_accuracy: 0.9904 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 715/1000\n",
            "\n",
            "Epoch 715: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0196 - accuracy: 0.9961 - val_loss: 0.1166 - val_accuracy: 0.9911 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 716/1000\n",
            "\n",
            "Epoch 716: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0226 - accuracy: 0.9945 - val_loss: 0.1161 - val_accuracy: 0.9913 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 717/1000\n",
            "\n",
            "Epoch 717: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.1171 - val_accuracy: 0.9915 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 718/1000\n",
            "\n",
            "Epoch 718: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0214 - accuracy: 0.9964 - val_loss: 0.1171 - val_accuracy: 0.9909 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 719/1000\n",
            "\n",
            "Epoch 719: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.1139 - val_accuracy: 0.9910 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 720/1000\n",
            "\n",
            "Epoch 720: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0222 - accuracy: 0.9947 - val_loss: 0.1198 - val_accuracy: 0.9897 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 721/1000\n",
            "\n",
            "Epoch 721: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0209 - accuracy: 0.9945 - val_loss: 0.1154 - val_accuracy: 0.9903 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 722/1000\n",
            "\n",
            "Epoch 722: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9947 - val_loss: 0.1168 - val_accuracy: 0.9914 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 723/1000\n",
            "\n",
            "Epoch 723: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9959 - val_loss: 0.1195 - val_accuracy: 0.9901 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 724/1000\n",
            "\n",
            "Epoch 724: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0241 - accuracy: 0.9932 - val_loss: 0.1180 - val_accuracy: 0.9886 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 725/1000\n",
            "\n",
            "Epoch 725: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9936 - val_loss: 0.1161 - val_accuracy: 0.9900 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 726/1000\n",
            "\n",
            "Epoch 726: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9947 - val_loss: 0.1146 - val_accuracy: 0.9913 - lr: 0.0010 - 426ms/epoch - 106ms/step\n",
            "Epoch 727/1000\n",
            "\n",
            "Epoch 727: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0220 - accuracy: 0.9952 - val_loss: 0.1192 - val_accuracy: 0.9913 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 728/1000\n",
            "\n",
            "Epoch 728: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.1164 - val_accuracy: 0.9911 - lr: 0.0010 - 384ms/epoch - 96ms/step\n",
            "Epoch 729/1000\n",
            "\n",
            "Epoch 729: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.1195 - val_accuracy: 0.9908 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 730/1000\n",
            "\n",
            "Epoch 730: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0297 - accuracy: 0.9937 - val_loss: 0.1155 - val_accuracy: 0.9913 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 731/1000\n",
            "\n",
            "Epoch 731: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0239 - accuracy: 0.9950 - val_loss: 0.1173 - val_accuracy: 0.9916 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 732/1000\n",
            "\n",
            "Epoch 732: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0219 - accuracy: 0.9947 - val_loss: 0.1151 - val_accuracy: 0.9910 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 733/1000\n",
            "\n",
            "Epoch 733: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0225 - accuracy: 0.9932 - val_loss: 0.1212 - val_accuracy: 0.9921 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 734/1000\n",
            "\n",
            "Epoch 734: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0246 - accuracy: 0.9929 - val_loss: 0.1157 - val_accuracy: 0.9895 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 735/1000\n",
            "\n",
            "Epoch 735: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0233 - accuracy: 0.9946 - val_loss: 0.1200 - val_accuracy: 0.9900 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 736/1000\n",
            "\n",
            "Epoch 736: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9949 - val_loss: 0.1185 - val_accuracy: 0.9897 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 737/1000\n",
            "\n",
            "Epoch 737: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9932 - val_loss: 0.1170 - val_accuracy: 0.9915 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 738/1000\n",
            "\n",
            "Epoch 738: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0232 - accuracy: 0.9941 - val_loss: 0.1159 - val_accuracy: 0.9916 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 739/1000\n",
            "\n",
            "Epoch 739: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0255 - accuracy: 0.9963 - val_loss: 0.1205 - val_accuracy: 0.9905 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 740/1000\n",
            "\n",
            "Epoch 740: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9948 - val_loss: 0.1176 - val_accuracy: 0.9909 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 741/1000\n",
            "\n",
            "Epoch 741: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9945 - val_loss: 0.1179 - val_accuracy: 0.9913 - lr: 0.0010 - 394ms/epoch - 99ms/step\n",
            "Epoch 742/1000\n",
            "\n",
            "Epoch 742: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.1169 - val_accuracy: 0.9909 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 743/1000\n",
            "\n",
            "Epoch 743: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0183 - accuracy: 0.9956 - val_loss: 0.1166 - val_accuracy: 0.9910 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 744/1000\n",
            "\n",
            "Epoch 744: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9944 - val_loss: 0.1175 - val_accuracy: 0.9901 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 745/1000\n",
            "\n",
            "Epoch 745: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9958 - val_loss: 0.1186 - val_accuracy: 0.9903 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 746/1000\n",
            "\n",
            "Epoch 746: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0191 - accuracy: 0.9952 - val_loss: 0.1170 - val_accuracy: 0.9903 - lr: 0.0010 - 402ms/epoch - 101ms/step\n",
            "Epoch 747/1000\n",
            "\n",
            "Epoch 747: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.1176 - val_accuracy: 0.9904 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 748/1000\n",
            "\n",
            "Epoch 748: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.1170 - val_accuracy: 0.9898 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 749/1000\n",
            "\n",
            "Epoch 749: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.1161 - val_accuracy: 0.9910 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 750/1000\n",
            "\n",
            "Epoch 750: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0191 - accuracy: 0.9947 - val_loss: 0.1165 - val_accuracy: 0.9908 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 751/1000\n",
            "\n",
            "Epoch 751: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9952 - val_loss: 0.1182 - val_accuracy: 0.9909 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 752/1000\n",
            "\n",
            "Epoch 752: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9949 - val_loss: 0.1174 - val_accuracy: 0.9901 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 753/1000\n",
            "\n",
            "Epoch 753: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0250 - accuracy: 0.9943 - val_loss: 0.1160 - val_accuracy: 0.9912 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 754/1000\n",
            "\n",
            "Epoch 754: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.1199 - val_accuracy: 0.9909 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 755/1000\n",
            "\n",
            "Epoch 755: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0211 - accuracy: 0.9962 - val_loss: 0.1170 - val_accuracy: 0.9910 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 756/1000\n",
            "\n",
            "Epoch 756: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.1182 - val_accuracy: 0.9908 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 757/1000\n",
            "\n",
            "Epoch 757: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9946 - val_loss: 0.1191 - val_accuracy: 0.9908 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 758/1000\n",
            "\n",
            "Epoch 758: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0156 - accuracy: 0.9957 - val_loss: 0.1173 - val_accuracy: 0.9909 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 759/1000\n",
            "\n",
            "Epoch 759: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9962 - val_loss: 0.1165 - val_accuracy: 0.9912 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 760/1000\n",
            "\n",
            "Epoch 760: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.1174 - val_accuracy: 0.9920 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 761/1000\n",
            "\n",
            "Epoch 761: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.1187 - val_accuracy: 0.9913 - lr: 0.0010 - 402ms/epoch - 100ms/step\n",
            "Epoch 762/1000\n",
            "\n",
            "Epoch 762: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.1178 - val_accuracy: 0.9913 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 763/1000\n",
            "\n",
            "Epoch 763: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9956 - val_loss: 0.1180 - val_accuracy: 0.9902 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 764/1000\n",
            "\n",
            "Epoch 764: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9950 - val_loss: 0.1160 - val_accuracy: 0.9908 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 765/1000\n",
            "\n",
            "Epoch 765: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.1179 - val_accuracy: 0.9906 - lr: 0.0010 - 382ms/epoch - 96ms/step\n",
            "Epoch 766/1000\n",
            "\n",
            "Epoch 766: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0164 - accuracy: 0.9958 - val_loss: 0.1162 - val_accuracy: 0.9906 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 767/1000\n",
            "\n",
            "Epoch 767: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.1189 - val_accuracy: 0.9906 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 768/1000\n",
            "\n",
            "Epoch 768: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9953 - val_loss: 0.1166 - val_accuracy: 0.9918 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 769/1000\n",
            "\n",
            "Epoch 769: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9947 - val_loss: 0.1196 - val_accuracy: 0.9908 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 770/1000\n",
            "\n",
            "Epoch 770: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0184 - accuracy: 0.9949 - val_loss: 0.1178 - val_accuracy: 0.9915 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 771/1000\n",
            "\n",
            "Epoch 771: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.1180 - val_accuracy: 0.9914 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 772/1000\n",
            "\n",
            "Epoch 772: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0142 - accuracy: 0.9955 - val_loss: 0.1160 - val_accuracy: 0.9907 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 773/1000\n",
            "\n",
            "Epoch 773: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9957 - val_loss: 0.1170 - val_accuracy: 0.9912 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 774/1000\n",
            "\n",
            "Epoch 774: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.1177 - val_accuracy: 0.9909 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 775/1000\n",
            "\n",
            "Epoch 775: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1165 - val_accuracy: 0.9909 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 776/1000\n",
            "\n",
            "Epoch 776: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.1172 - val_accuracy: 0.9907 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 777/1000\n",
            "\n",
            "Epoch 777: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9960 - val_loss: 0.1158 - val_accuracy: 0.9918 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 778/1000\n",
            "\n",
            "Epoch 778: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.1155 - val_accuracy: 0.9907 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 779/1000\n",
            "\n",
            "Epoch 779: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9945 - val_loss: 0.1160 - val_accuracy: 0.9917 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 780/1000\n",
            "\n",
            "Epoch 780: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9962 - val_loss: 0.1161 - val_accuracy: 0.9904 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 781/1000\n",
            "\n",
            "Epoch 781: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9946 - val_loss: 0.1187 - val_accuracy: 0.9920 - lr: 0.0010 - 410ms/epoch - 102ms/step\n",
            "Epoch 782/1000\n",
            "\n",
            "Epoch 782: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0277 - accuracy: 0.9945 - val_loss: 0.1181 - val_accuracy: 0.9907 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 783/1000\n",
            "\n",
            "Epoch 783: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0213 - accuracy: 0.9949 - val_loss: 0.1187 - val_accuracy: 0.9915 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 784/1000\n",
            "\n",
            "Epoch 784: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.1178 - val_accuracy: 0.9907 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 785/1000\n",
            "\n",
            "Epoch 785: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9955 - val_loss: 0.1169 - val_accuracy: 0.9905 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 786/1000\n",
            "\n",
            "Epoch 786: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.1176 - val_accuracy: 0.9909 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 787/1000\n",
            "\n",
            "Epoch 787: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0156 - accuracy: 0.9953 - val_loss: 0.1188 - val_accuracy: 0.9899 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 788/1000\n",
            "\n",
            "Epoch 788: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1184 - val_accuracy: 0.9910 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 789/1000\n",
            "\n",
            "Epoch 789: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.1174 - val_accuracy: 0.9896 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 790/1000\n",
            "\n",
            "Epoch 790: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.1157 - val_accuracy: 0.9912 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 791/1000\n",
            "\n",
            "Epoch 791: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9955 - val_loss: 0.1173 - val_accuracy: 0.9912 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 792/1000\n",
            "\n",
            "Epoch 792: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0246 - accuracy: 0.9951 - val_loss: 0.1195 - val_accuracy: 0.9876 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 793/1000\n",
            "\n",
            "Epoch 793: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9950 - val_loss: 0.1213 - val_accuracy: 0.9913 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 794/1000\n",
            "\n",
            "Epoch 794: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0254 - accuracy: 0.9941 - val_loss: 0.1187 - val_accuracy: 0.9908 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 795/1000\n",
            "\n",
            "Epoch 795: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0247 - accuracy: 0.9951 - val_loss: 0.1168 - val_accuracy: 0.9912 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 796/1000\n",
            "\n",
            "Epoch 796: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0270 - accuracy: 0.9948 - val_loss: 0.1174 - val_accuracy: 0.9919 - lr: 0.0010 - 419ms/epoch - 105ms/step\n",
            "Epoch 797/1000\n",
            "\n",
            "Epoch 797: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9946 - val_loss: 0.1180 - val_accuracy: 0.9901 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 798/1000\n",
            "\n",
            "Epoch 798: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.1188 - val_accuracy: 0.9912 - lr: 0.0010 - 402ms/epoch - 100ms/step\n",
            "Epoch 799/1000\n",
            "\n",
            "Epoch 799: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0203 - accuracy: 0.9953 - val_loss: 0.1178 - val_accuracy: 0.9911 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 800/1000\n",
            "\n",
            "Epoch 800: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.1176 - val_accuracy: 0.9907 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 801/1000\n",
            "\n",
            "Epoch 801: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9956 - val_loss: 0.1157 - val_accuracy: 0.9899 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 802/1000\n",
            "\n",
            "Epoch 802: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9953 - val_loss: 0.1180 - val_accuracy: 0.9907 - lr: 0.0010 - 354ms/epoch - 88ms/step\n",
            "Epoch 803/1000\n",
            "\n",
            "Epoch 803: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9945 - val_loss: 0.1208 - val_accuracy: 0.9901 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 804/1000\n",
            "\n",
            "Epoch 804: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0219 - accuracy: 0.9924 - val_loss: 0.1191 - val_accuracy: 0.9909 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 805/1000\n",
            "\n",
            "Epoch 805: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9975 - val_loss: 0.1184 - val_accuracy: 0.9905 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 806/1000\n",
            "\n",
            "Epoch 806: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9950 - val_loss: 0.1186 - val_accuracy: 0.9911 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 807/1000\n",
            "\n",
            "Epoch 807: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9949 - val_loss: 0.1167 - val_accuracy: 0.9905 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 808/1000\n",
            "\n",
            "Epoch 808: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9947 - val_loss: 0.1165 - val_accuracy: 0.9908 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 809/1000\n",
            "\n",
            "Epoch 809: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.1181 - val_accuracy: 0.9920 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 810/1000\n",
            "\n",
            "Epoch 810: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9946 - val_loss: 0.1164 - val_accuracy: 0.9915 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 811/1000\n",
            "\n",
            "Epoch 811: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9947 - val_loss: 0.1195 - val_accuracy: 0.9918 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 812/1000\n",
            "\n",
            "Epoch 812: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.1166 - val_accuracy: 0.9918 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 813/1000\n",
            "\n",
            "Epoch 813: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.1175 - val_accuracy: 0.9915 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 814/1000\n",
            "\n",
            "Epoch 814: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0157 - accuracy: 0.9963 - val_loss: 0.1161 - val_accuracy: 0.9903 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 815/1000\n",
            "\n",
            "Epoch 815: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.1190 - val_accuracy: 0.9913 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 816/1000\n",
            "\n",
            "Epoch 816: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0241 - accuracy: 0.9943 - val_loss: 0.1179 - val_accuracy: 0.9901 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 817/1000\n",
            "\n",
            "Epoch 817: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0278 - accuracy: 0.9930 - val_loss: 0.1168 - val_accuracy: 0.9894 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 818/1000\n",
            "\n",
            "Epoch 818: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0258 - accuracy: 0.9961 - val_loss: 0.1176 - val_accuracy: 0.9907 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 819/1000\n",
            "\n",
            "Epoch 819: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0296 - accuracy: 0.9940 - val_loss: 0.1147 - val_accuracy: 0.9906 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 820/1000\n",
            "\n",
            "Epoch 820: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0284 - accuracy: 0.9937 - val_loss: 0.1204 - val_accuracy: 0.9915 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 821/1000\n",
            "\n",
            "Epoch 821: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0256 - accuracy: 0.9942 - val_loss: 0.1175 - val_accuracy: 0.9901 - lr: 0.0010 - 422ms/epoch - 105ms/step\n",
            "Epoch 822/1000\n",
            "\n",
            "Epoch 822: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9952 - val_loss: 0.1167 - val_accuracy: 0.9908 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 823/1000\n",
            "\n",
            "Epoch 823: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9931 - val_loss: 0.1178 - val_accuracy: 0.9880 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 824/1000\n",
            "\n",
            "Epoch 824: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9944 - val_loss: 0.1172 - val_accuracy: 0.9906 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 825/1000\n",
            "\n",
            "Epoch 825: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.1207 - val_accuracy: 0.9914 - lr: 0.0010 - 344ms/epoch - 86ms/step\n",
            "Epoch 826/1000\n",
            "\n",
            "Epoch 826: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0212 - accuracy: 0.9956 - val_loss: 0.1176 - val_accuracy: 0.9911 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 827/1000\n",
            "\n",
            "Epoch 827: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9925 - val_loss: 0.1175 - val_accuracy: 0.9908 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 828/1000\n",
            "\n",
            "Epoch 828: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9963 - val_loss: 0.1191 - val_accuracy: 0.9914 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 829/1000\n",
            "\n",
            "Epoch 829: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.1173 - val_accuracy: 0.9910 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 830/1000\n",
            "\n",
            "Epoch 830: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9936 - val_loss: 0.1148 - val_accuracy: 0.9915 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 831/1000\n",
            "\n",
            "Epoch 831: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.1184 - val_accuracy: 0.9901 - lr: 0.0010 - 410ms/epoch - 103ms/step\n",
            "Epoch 832/1000\n",
            "\n",
            "Epoch 832: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9955 - val_loss: 0.1177 - val_accuracy: 0.9915 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 833/1000\n",
            "\n",
            "Epoch 833: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1199 - val_accuracy: 0.9904 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 834/1000\n",
            "\n",
            "Epoch 834: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9943 - val_loss: 0.1162 - val_accuracy: 0.9912 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 835/1000\n",
            "\n",
            "Epoch 835: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.1176 - val_accuracy: 0.9915 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 836/1000\n",
            "\n",
            "Epoch 836: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9947 - val_loss: 0.1163 - val_accuracy: 0.9914 - lr: 0.0010 - 424ms/epoch - 106ms/step\n",
            "Epoch 837/1000\n",
            "\n",
            "Epoch 837: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9940 - val_loss: 0.1169 - val_accuracy: 0.9918 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 838/1000\n",
            "\n",
            "Epoch 838: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9961 - val_loss: 0.1182 - val_accuracy: 0.9912 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 839/1000\n",
            "\n",
            "Epoch 839: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.1157 - val_accuracy: 0.9921 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 840/1000\n",
            "\n",
            "Epoch 840: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9943 - val_loss: 0.1171 - val_accuracy: 0.9904 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 841/1000\n",
            "\n",
            "Epoch 841: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.1163 - val_accuracy: 0.9920 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 842/1000\n",
            "\n",
            "Epoch 842: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9945 - val_loss: 0.1171 - val_accuracy: 0.9896 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 843/1000\n",
            "\n",
            "Epoch 843: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9949 - val_loss: 0.1167 - val_accuracy: 0.9902 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 844/1000\n",
            "\n",
            "Epoch 844: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9941 - val_loss: 0.1168 - val_accuracy: 0.9898 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 845/1000\n",
            "\n",
            "Epoch 845: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0147 - accuracy: 0.9957 - val_loss: 0.1189 - val_accuracy: 0.9907 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 846/1000\n",
            "\n",
            "Epoch 846: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.1162 - val_accuracy: 0.9911 - lr: 0.0010 - 404ms/epoch - 101ms/step\n",
            "Epoch 847/1000\n",
            "\n",
            "Epoch 847: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9941 - val_loss: 0.1173 - val_accuracy: 0.9918 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 848/1000\n",
            "\n",
            "Epoch 848: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9937 - val_loss: 0.1198 - val_accuracy: 0.9918 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 849/1000\n",
            "\n",
            "Epoch 849: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0193 - accuracy: 0.9953 - val_loss: 0.1178 - val_accuracy: 0.9903 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 850/1000\n",
            "\n",
            "Epoch 850: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.1211 - val_accuracy: 0.9905 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 851/1000\n",
            "\n",
            "Epoch 851: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9949 - val_loss: 0.1200 - val_accuracy: 0.9907 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 852/1000\n",
            "\n",
            "Epoch 852: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.1172 - val_accuracy: 0.9908 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 853/1000\n",
            "\n",
            "Epoch 853: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9947 - val_loss: 0.1171 - val_accuracy: 0.9907 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 854/1000\n",
            "\n",
            "Epoch 854: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.1164 - val_accuracy: 0.9910 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 855/1000\n",
            "\n",
            "Epoch 855: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.1178 - val_accuracy: 0.9916 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 856/1000\n",
            "\n",
            "Epoch 856: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9952 - val_loss: 0.1200 - val_accuracy: 0.9909 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 857/1000\n",
            "\n",
            "Epoch 857: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.1184 - val_accuracy: 0.9911 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 858/1000\n",
            "\n",
            "Epoch 858: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.1177 - val_accuracy: 0.9911 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 859/1000\n",
            "\n",
            "Epoch 859: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0228 - accuracy: 0.9954 - val_loss: 0.1189 - val_accuracy: 0.9900 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 860/1000\n",
            "\n",
            "Epoch 860: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0227 - accuracy: 0.9941 - val_loss: 0.1231 - val_accuracy: 0.9909 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 861/1000\n",
            "\n",
            "Epoch 861: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9942 - val_loss: 0.1167 - val_accuracy: 0.9901 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 862/1000\n",
            "\n",
            "Epoch 862: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9953 - val_loss: 0.1167 - val_accuracy: 0.9909 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 863/1000\n",
            "\n",
            "Epoch 863: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9951 - val_loss: 0.1164 - val_accuracy: 0.9917 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 864/1000\n",
            "\n",
            "Epoch 864: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0299 - accuracy: 0.9947 - val_loss: 0.1194 - val_accuracy: 0.9902 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 865/1000\n",
            "\n",
            "Epoch 865: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0271 - accuracy: 0.9946 - val_loss: 0.1229 - val_accuracy: 0.9892 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 866/1000\n",
            "\n",
            "Epoch 866: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0287 - accuracy: 0.9939 - val_loss: 0.1202 - val_accuracy: 0.9916 - lr: 0.0010 - 435ms/epoch - 109ms/step\n",
            "Epoch 867/1000\n",
            "\n",
            "Epoch 867: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0292 - accuracy: 0.9946 - val_loss: 0.1176 - val_accuracy: 0.9901 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 868/1000\n",
            "\n",
            "Epoch 868: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9954 - val_loss: 0.1182 - val_accuracy: 0.9919 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 869/1000\n",
            "\n",
            "Epoch 869: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0263 - accuracy: 0.9945 - val_loss: 0.1180 - val_accuracy: 0.9909 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 870/1000\n",
            "\n",
            "Epoch 870: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9944 - val_loss: 0.1176 - val_accuracy: 0.9911 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 871/1000\n",
            "\n",
            "Epoch 871: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0201 - accuracy: 0.9950 - val_loss: 0.1165 - val_accuracy: 0.9903 - lr: 0.0010 - 424ms/epoch - 106ms/step\n",
            "Epoch 872/1000\n",
            "\n",
            "Epoch 872: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9961 - val_loss: 0.1162 - val_accuracy: 0.9912 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 873/1000\n",
            "\n",
            "Epoch 873: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0224 - accuracy: 0.9942 - val_loss: 0.1177 - val_accuracy: 0.9909 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 874/1000\n",
            "\n",
            "Epoch 874: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9956 - val_loss: 0.1176 - val_accuracy: 0.9913 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 875/1000\n",
            "\n",
            "Epoch 875: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.1163 - val_accuracy: 0.9906 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 876/1000\n",
            "\n",
            "Epoch 876: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0214 - accuracy: 0.9956 - val_loss: 0.1197 - val_accuracy: 0.9912 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 877/1000\n",
            "\n",
            "Epoch 877: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0242 - accuracy: 0.9934 - val_loss: 0.1174 - val_accuracy: 0.9900 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 878/1000\n",
            "\n",
            "Epoch 878: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0220 - accuracy: 0.9934 - val_loss: 0.1181 - val_accuracy: 0.9908 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 879/1000\n",
            "\n",
            "Epoch 879: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9950 - val_loss: 0.1161 - val_accuracy: 0.9901 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 880/1000\n",
            "\n",
            "Epoch 880: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.1175 - val_accuracy: 0.9916 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 881/1000\n",
            "\n",
            "Epoch 881: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0183 - accuracy: 0.9953 - val_loss: 0.1179 - val_accuracy: 0.9905 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 882/1000\n",
            "\n",
            "Epoch 882: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9946 - val_loss: 0.1174 - val_accuracy: 0.9914 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 883/1000\n",
            "\n",
            "Epoch 883: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9956 - val_loss: 0.1153 - val_accuracy: 0.9914 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 884/1000\n",
            "\n",
            "Epoch 884: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9955 - val_loss: 0.1172 - val_accuracy: 0.9911 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 885/1000\n",
            "\n",
            "Epoch 885: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.1168 - val_accuracy: 0.9914 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 886/1000\n",
            "\n",
            "Epoch 886: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.1166 - val_accuracy: 0.9899 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 887/1000\n",
            "\n",
            "Epoch 887: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.1173 - val_accuracy: 0.9909 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 888/1000\n",
            "\n",
            "Epoch 888: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0157 - accuracy: 0.9964 - val_loss: 0.1169 - val_accuracy: 0.9890 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 889/1000\n",
            "\n",
            "Epoch 889: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9951 - val_loss: 0.1179 - val_accuracy: 0.9914 - lr: 0.0010 - 349ms/epoch - 87ms/step\n",
            "Epoch 890/1000\n",
            "\n",
            "Epoch 890: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.1152 - val_accuracy: 0.9917 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 891/1000\n",
            "\n",
            "Epoch 891: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.1179 - val_accuracy: 0.9919 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 892/1000\n",
            "\n",
            "Epoch 892: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.1157 - val_accuracy: 0.9917 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 893/1000\n",
            "\n",
            "Epoch 893: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.1162 - val_accuracy: 0.9899 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 894/1000\n",
            "\n",
            "Epoch 894: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1200 - val_accuracy: 0.9905 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 895/1000\n",
            "\n",
            "Epoch 895: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.1170 - val_accuracy: 0.9917 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 896/1000\n",
            "\n",
            "Epoch 896: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0192 - accuracy: 0.9946 - val_loss: 0.1173 - val_accuracy: 0.9913 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 897/1000\n",
            "\n",
            "Epoch 897: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9952 - val_loss: 0.1166 - val_accuracy: 0.9909 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 898/1000\n",
            "\n",
            "Epoch 898: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0169 - accuracy: 0.9948 - val_loss: 0.1167 - val_accuracy: 0.9902 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 899/1000\n",
            "\n",
            "Epoch 899: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0177 - accuracy: 0.9956 - val_loss: 0.1160 - val_accuracy: 0.9914 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 900/1000\n",
            "\n",
            "Epoch 900: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.1171 - val_accuracy: 0.9905 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 901/1000\n",
            "\n",
            "Epoch 901: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.1164 - val_accuracy: 0.9903 - lr: 0.0010 - 427ms/epoch - 107ms/step\n",
            "Epoch 902/1000\n",
            "\n",
            "Epoch 902: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9959 - val_loss: 0.1170 - val_accuracy: 0.9898 - lr: 0.0010 - 383ms/epoch - 96ms/step\n",
            "Epoch 903/1000\n",
            "\n",
            "Epoch 903: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1175 - val_accuracy: 0.9914 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 904/1000\n",
            "\n",
            "Epoch 904: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9957 - val_loss: 0.1170 - val_accuracy: 0.9904 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 905/1000\n",
            "\n",
            "Epoch 905: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9955 - val_loss: 0.1167 - val_accuracy: 0.9915 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 906/1000\n",
            "\n",
            "Epoch 906: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0221 - accuracy: 0.9953 - val_loss: 0.1208 - val_accuracy: 0.9905 - lr: 0.0010 - 427ms/epoch - 107ms/step\n",
            "Epoch 907/1000\n",
            "\n",
            "Epoch 907: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9966 - val_loss: 0.1179 - val_accuracy: 0.9916 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 908/1000\n",
            "\n",
            "Epoch 908: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0191 - accuracy: 0.9959 - val_loss: 0.1199 - val_accuracy: 0.9916 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 909/1000\n",
            "\n",
            "Epoch 909: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0312 - accuracy: 0.9936 - val_loss: 0.1172 - val_accuracy: 0.9917 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 910/1000\n",
            "\n",
            "Epoch 910: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0334 - accuracy: 0.9944 - val_loss: 0.1167 - val_accuracy: 0.9905 - lr: 0.0010 - 380ms/epoch - 95ms/step\n",
            "Epoch 911/1000\n",
            "\n",
            "Epoch 911: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0337 - accuracy: 0.9944 - val_loss: 0.1175 - val_accuracy: 0.9902 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 912/1000\n",
            "\n",
            "Epoch 912: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0280 - accuracy: 0.9935 - val_loss: 0.1195 - val_accuracy: 0.9903 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 913/1000\n",
            "\n",
            "Epoch 913: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0262 - accuracy: 0.9945 - val_loss: 0.1206 - val_accuracy: 0.9911 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 914/1000\n",
            "\n",
            "Epoch 914: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0237 - accuracy: 0.9952 - val_loss: 0.1171 - val_accuracy: 0.9890 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 915/1000\n",
            "\n",
            "Epoch 915: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9947 - val_loss: 0.1167 - val_accuracy: 0.9916 - lr: 0.0010 - 344ms/epoch - 86ms/step\n",
            "Epoch 916/1000\n",
            "\n",
            "Epoch 916: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0232 - accuracy: 0.9946 - val_loss: 0.1182 - val_accuracy: 0.9897 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 917/1000\n",
            "\n",
            "Epoch 917: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0193 - accuracy: 0.9944 - val_loss: 0.1175 - val_accuracy: 0.9913 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 918/1000\n",
            "\n",
            "Epoch 918: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0331 - accuracy: 0.9943 - val_loss: 0.1229 - val_accuracy: 0.9897 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 919/1000\n",
            "\n",
            "Epoch 919: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0297 - accuracy: 0.9935 - val_loss: 0.1230 - val_accuracy: 0.9908 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 920/1000\n",
            "\n",
            "Epoch 920: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0404 - accuracy: 0.9946 - val_loss: 0.1309 - val_accuracy: 0.9867 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 921/1000\n",
            "\n",
            "Epoch 921: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0539 - accuracy: 0.9886 - val_loss: 0.1223 - val_accuracy: 0.9901 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 922/1000\n",
            "\n",
            "Epoch 922: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1964 - accuracy: 0.8525 - val_loss: 0.8539 - val_accuracy: 0.4277 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 923/1000\n",
            "\n",
            "Epoch 923: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 74.8919 - accuracy: 0.6172 - val_loss: 307.0567 - val_accuracy: 0.2493 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 924/1000\n",
            "\n",
            "Epoch 924: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 519.3663 - accuracy: 0.2927 - val_loss: 316.3219 - val_accuracy: 0.4610 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 925/1000\n",
            "\n",
            "Epoch 925: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 486.9380 - accuracy: 0.3705 - val_loss: 179.0938 - val_accuracy: 0.2446 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 926/1000\n",
            "\n",
            "Epoch 926: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 437.3020 - accuracy: 0.3634 - val_loss: 381.5961 - val_accuracy: 0.5278 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 927/1000\n",
            "\n",
            "Epoch 927: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 300.7290 - accuracy: 0.3924 - val_loss: 107.6802 - val_accuracy: 0.4050 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 928/1000\n",
            "\n",
            "Epoch 928: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 132.8123 - accuracy: 0.3787 - val_loss: 203.2881 - val_accuracy: 0.3675 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 929/1000\n",
            "\n",
            "Epoch 929: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 160.4931 - accuracy: 0.4250 - val_loss: 65.7262 - val_accuracy: 0.5219 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 930/1000\n",
            "\n",
            "Epoch 930: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 76.1537 - accuracy: 0.2994 - val_loss: 85.5648 - val_accuracy: 0.4703 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 931/1000\n",
            "\n",
            "Epoch 931: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 55.3900 - accuracy: 0.3436 - val_loss: 32.1880 - val_accuracy: 0.1209 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 932/1000\n",
            "\n",
            "Epoch 932: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 27.7210 - accuracy: 0.3032 - val_loss: 21.3337 - val_accuracy: 0.3837 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 933/1000\n",
            "\n",
            "Epoch 933: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 18.5027 - accuracy: 0.3182 - val_loss: 11.0976 - val_accuracy: 0.3004 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 934/1000\n",
            "\n",
            "Epoch 934: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 13.6493 - accuracy: 0.3181 - val_loss: 9.7928 - val_accuracy: 0.3645 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 935/1000\n",
            "\n",
            "Epoch 935: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 10.3120 - accuracy: 0.4184 - val_loss: 6.3514 - val_accuracy: 0.3378 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 936/1000\n",
            "\n",
            "Epoch 936: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 7.0657 - accuracy: 0.3365 - val_loss: 4.7919 - val_accuracy: 0.2537 - lr: 0.0010 - 413ms/epoch - 103ms/step\n",
            "Epoch 937/1000\n",
            "\n",
            "Epoch 937: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 4.0883 - accuracy: 0.3162 - val_loss: 2.6162 - val_accuracy: 0.2794 - lr: 0.0010 - 394ms/epoch - 98ms/step\n",
            "Epoch 938/1000\n",
            "\n",
            "Epoch 938: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 2.9479 - accuracy: 0.2882 - val_loss: 2.1502 - val_accuracy: 0.2397 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 939/1000\n",
            "\n",
            "Epoch 939: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 2.2071 - accuracy: 0.2706 - val_loss: 1.4085 - val_accuracy: 0.3162 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 940/1000\n",
            "\n",
            "Epoch 940: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 1.5729 - accuracy: 0.3492 - val_loss: 1.0735 - val_accuracy: 0.3980 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 941/1000\n",
            "\n",
            "Epoch 941: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 1.2274 - accuracy: 0.3700 - val_loss: 0.9822 - val_accuracy: 0.4048 - lr: 0.0010 - 418ms/epoch - 104ms/step\n",
            "Epoch 942/1000\n",
            "\n",
            "Epoch 942: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.9989 - accuracy: 0.3666 - val_loss: 0.7455 - val_accuracy: 0.3611 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 943/1000\n",
            "\n",
            "Epoch 943: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.8169 - accuracy: 0.3197 - val_loss: 0.6448 - val_accuracy: 0.2892 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 944/1000\n",
            "\n",
            "Epoch 944: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.7249 - accuracy: 0.3090 - val_loss: 0.5741 - val_accuracy: 0.2861 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 945/1000\n",
            "\n",
            "Epoch 945: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.6456 - accuracy: 0.3114 - val_loss: 0.5128 - val_accuracy: 0.3061 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 946/1000\n",
            "\n",
            "Epoch 946: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.5667 - accuracy: 0.3290 - val_loss: 0.4728 - val_accuracy: 0.3029 - lr: 0.0010 - 406ms/epoch - 101ms/step\n",
            "Epoch 947/1000\n",
            "\n",
            "Epoch 947: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.5100 - accuracy: 0.3388 - val_loss: 0.4661 - val_accuracy: 0.3319 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 948/1000\n",
            "\n",
            "Epoch 948: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.5075 - accuracy: 0.3535 - val_loss: 0.4261 - val_accuracy: 0.3300 - lr: 0.0010 - 342ms/epoch - 85ms/step\n",
            "Epoch 949/1000\n",
            "\n",
            "Epoch 949: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.4508 - accuracy: 0.3560 - val_loss: 0.4069 - val_accuracy: 0.3439 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 950/1000\n",
            "\n",
            "Epoch 950: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.4359 - accuracy: 0.3643 - val_loss: 0.3876 - val_accuracy: 0.3739 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 951/1000\n",
            "\n",
            "Epoch 951: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.4036 - accuracy: 0.3890 - val_loss: 0.3743 - val_accuracy: 0.3927 - lr: 0.0010 - 404ms/epoch - 101ms/step\n",
            "Epoch 952/1000\n",
            "\n",
            "Epoch 952: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.4008 - accuracy: 0.4027 - val_loss: 0.3591 - val_accuracy: 0.4099 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 953/1000\n",
            "\n",
            "Epoch 953: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3708 - accuracy: 0.4246 - val_loss: 0.3459 - val_accuracy: 0.4272 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 954/1000\n",
            "\n",
            "Epoch 954: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3598 - accuracy: 0.4508 - val_loss: 0.3341 - val_accuracy: 0.4564 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 955/1000\n",
            "\n",
            "Epoch 955: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3517 - accuracy: 0.4590 - val_loss: 0.3217 - val_accuracy: 0.4659 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 956/1000\n",
            "\n",
            "Epoch 956: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3400 - accuracy: 0.4696 - val_loss: 0.3181 - val_accuracy: 0.4863 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 957/1000\n",
            "\n",
            "Epoch 957: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3212 - accuracy: 0.4865 - val_loss: 0.3134 - val_accuracy: 0.4958 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 958/1000\n",
            "\n",
            "Epoch 958: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3135 - accuracy: 0.4929 - val_loss: 0.2959 - val_accuracy: 0.5023 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 959/1000\n",
            "\n",
            "Epoch 959: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2967 - accuracy: 0.5048 - val_loss: 0.2900 - val_accuracy: 0.5378 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 960/1000\n",
            "\n",
            "Epoch 960: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3023 - accuracy: 0.5237 - val_loss: 0.2773 - val_accuracy: 0.5213 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 961/1000\n",
            "\n",
            "Epoch 961: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2847 - accuracy: 0.5072 - val_loss: 0.2715 - val_accuracy: 0.5468 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 962/1000\n",
            "\n",
            "Epoch 962: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2687 - accuracy: 0.5399 - val_loss: 0.2667 - val_accuracy: 0.5459 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 963/1000\n",
            "\n",
            "Epoch 963: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2737 - accuracy: 0.5210 - val_loss: 0.2594 - val_accuracy: 0.5540 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 964/1000\n",
            "\n",
            "Epoch 964: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2659 - accuracy: 0.5489 - val_loss: 0.2512 - val_accuracy: 0.5669 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 965/1000\n",
            "\n",
            "Epoch 965: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2538 - accuracy: 0.5504 - val_loss: 0.2507 - val_accuracy: 0.5756 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 966/1000\n",
            "\n",
            "Epoch 966: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2502 - accuracy: 0.5632 - val_loss: 0.2455 - val_accuracy: 0.5802 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 967/1000\n",
            "\n",
            "Epoch 967: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2461 - accuracy: 0.5522 - val_loss: 0.2396 - val_accuracy: 0.5898 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 968/1000\n",
            "\n",
            "Epoch 968: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2426 - accuracy: 0.5697 - val_loss: 0.2387 - val_accuracy: 0.5982 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 969/1000\n",
            "\n",
            "Epoch 969: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2420 - accuracy: 0.5747 - val_loss: 0.2424 - val_accuracy: 0.6039 - lr: 0.0010 - 345ms/epoch - 86ms/step\n",
            "Epoch 970/1000\n",
            "\n",
            "Epoch 970: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2422 - accuracy: 0.5663 - val_loss: 0.2494 - val_accuracy: 0.5760 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 971/1000\n",
            "\n",
            "Epoch 971: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2387 - accuracy: 0.5656 - val_loss: 0.2374 - val_accuracy: 0.6007 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 972/1000\n",
            "\n",
            "Epoch 972: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2300 - accuracy: 0.5839 - val_loss: 0.2251 - val_accuracy: 0.6264 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 973/1000\n",
            "\n",
            "Epoch 973: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2212 - accuracy: 0.6006 - val_loss: 0.2218 - val_accuracy: 0.6260 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 974/1000\n",
            "\n",
            "Epoch 974: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2209 - accuracy: 0.6012 - val_loss: 0.2179 - val_accuracy: 0.6376 - lr: 0.0010 - 408ms/epoch - 102ms/step\n",
            "Epoch 975/1000\n",
            "\n",
            "Epoch 975: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2246 - accuracy: 0.5973 - val_loss: 0.2357 - val_accuracy: 0.5927 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 976/1000\n",
            "\n",
            "Epoch 976: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2209 - accuracy: 0.6022 - val_loss: 0.2242 - val_accuracy: 0.6221 - lr: 0.0010 - 419ms/epoch - 105ms/step\n",
            "Epoch 977/1000\n",
            "\n",
            "Epoch 977: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2200 - accuracy: 0.5974 - val_loss: 0.2189 - val_accuracy: 0.6223 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 978/1000\n",
            "\n",
            "Epoch 978: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2114 - accuracy: 0.6137 - val_loss: 0.2128 - val_accuracy: 0.6472 - lr: 0.0010 - 352ms/epoch - 88ms/step\n",
            "Epoch 979/1000\n",
            "\n",
            "Epoch 979: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1993 - accuracy: 0.6242 - val_loss: 0.2106 - val_accuracy: 0.6516 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 980/1000\n",
            "\n",
            "Epoch 980: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2033 - accuracy: 0.6210 - val_loss: 0.2091 - val_accuracy: 0.6660 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 981/1000\n",
            "\n",
            "Epoch 981: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1960 - accuracy: 0.6439 - val_loss: 0.2049 - val_accuracy: 0.6725 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 982/1000\n",
            "\n",
            "Epoch 982: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1906 - accuracy: 0.6431 - val_loss: 0.2015 - val_accuracy: 0.6954 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 983/1000\n",
            "\n",
            "Epoch 983: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1860 - accuracy: 0.6587 - val_loss: 0.2018 - val_accuracy: 0.6713 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 984/1000\n",
            "\n",
            "Epoch 984: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1969 - accuracy: 0.6420 - val_loss: 0.1994 - val_accuracy: 0.6898 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 985/1000\n",
            "\n",
            "Epoch 985: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1828 - accuracy: 0.6636 - val_loss: 0.2010 - val_accuracy: 0.6651 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 986/1000\n",
            "\n",
            "Epoch 986: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1845 - accuracy: 0.6492 - val_loss: 0.2036 - val_accuracy: 0.6727 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 987/1000\n",
            "\n",
            "Epoch 987: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1835 - accuracy: 0.6599 - val_loss: 0.1967 - val_accuracy: 0.6769 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 988/1000\n",
            "\n",
            "Epoch 988: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1865 - accuracy: 0.6524 - val_loss: 0.1969 - val_accuracy: 0.7008 - lr: 0.0010 - 349ms/epoch - 87ms/step\n",
            "Epoch 989/1000\n",
            "\n",
            "Epoch 989: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1827 - accuracy: 0.6588 - val_loss: 0.2049 - val_accuracy: 0.6847 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 990/1000\n",
            "\n",
            "Epoch 990: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2010 - accuracy: 0.6441 - val_loss: 0.2130 - val_accuracy: 0.6704 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 991/1000\n",
            "\n",
            "Epoch 991: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1950 - accuracy: 0.6523 - val_loss: 0.1955 - val_accuracy: 0.6965 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 992/1000\n",
            "\n",
            "Epoch 992: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1847 - accuracy: 0.6682 - val_loss: 0.1916 - val_accuracy: 0.6972 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 993/1000\n",
            "\n",
            "Epoch 993: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1721 - accuracy: 0.6775 - val_loss: 0.1913 - val_accuracy: 0.7075 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 994/1000\n",
            "\n",
            "Epoch 994: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1712 - accuracy: 0.6783 - val_loss: 0.1984 - val_accuracy: 0.7079 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 995/1000\n",
            "\n",
            "Epoch 995: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1713 - accuracy: 0.6824 - val_loss: 0.1889 - val_accuracy: 0.7210 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 996/1000\n",
            "\n",
            "Epoch 996: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1650 - accuracy: 0.6865 - val_loss: 0.1873 - val_accuracy: 0.7249 - lr: 0.0010 - 378ms/epoch - 95ms/step\n",
            "Epoch 997/1000\n",
            "\n",
            "Epoch 997: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1670 - accuracy: 0.6997 - val_loss: 0.1847 - val_accuracy: 0.7290 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 998/1000\n",
            "\n",
            "Epoch 998: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1638 - accuracy: 0.6947 - val_loss: 0.1831 - val_accuracy: 0.7313 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 999/1000\n",
            "\n",
            "Epoch 999: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1653 - accuracy: 0.7016 - val_loss: 0.1835 - val_accuracy: 0.7299 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 1000/1000\n",
            "\n",
            "Epoch 1000: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1653 - accuracy: 0.6993 - val_loss: 0.1848 - val_accuracy: 0.7322 - lr: 0.0010 - 346ms/epoch - 87ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "iqtUEMgHXifY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdHn9Fk-fXuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "_QbNgTsBCD4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1235a9-d685-43dd-a933-f9fb214658ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a0b4023fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HIzIiMvRc18t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = model.fit(xtrain,ytrain, batch_size=32,\n",
        "               steps_per_epoch=xtrain.shape[0]//32,\n",
        "               epochs=500,\n",
        "               verbose=2,\n",
        "               callbacks=[anne, checkpoint],\n",
        "               validation_split = 0.1,\n",
        "               validation_steps = xtest.shape[0]//32)"
      ],
      "metadata": {
        "id": "7R4HkjOQEwU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5f7b2e-ef80-4b10-eceb-70039b9a4666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\n",
            "Epoch 1: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9890 - val_loss: 0.1185 - val_accuracy: 0.9891 - lr: 0.0010 - 492ms/epoch - 123ms/step\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 2: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0208 - accuracy: 0.9910 - val_loss: 0.1195 - val_accuracy: 0.9881 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 3: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9907 - val_loss: 0.1171 - val_accuracy: 0.9898 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.1192 - val_accuracy: 0.9878 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9906 - val_loss: 0.1179 - val_accuracy: 0.9897 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9923 - val_loss: 0.1179 - val_accuracy: 0.9880 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9913 - val_loss: 0.1180 - val_accuracy: 0.9884 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9909 - val_loss: 0.1184 - val_accuracy: 0.9890 - lr: 0.0010 - 328ms/epoch - 82ms/step\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9920 - val_loss: 0.1186 - val_accuracy: 0.9889 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0217 - accuracy: 0.9896 - val_loss: 0.1186 - val_accuracy: 0.9870 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 11: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0236 - accuracy: 0.9875 - val_loss: 0.1190 - val_accuracy: 0.9866 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0203 - accuracy: 0.9920 - val_loss: 0.1175 - val_accuracy: 0.9876 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 13: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9912 - val_loss: 0.1189 - val_accuracy: 0.9888 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9911 - val_loss: 0.1199 - val_accuracy: 0.9885 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9930 - val_loss: 0.1188 - val_accuracy: 0.9886 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9924 - val_loss: 0.1187 - val_accuracy: 0.9878 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9909 - val_loss: 0.1185 - val_accuracy: 0.9866 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 18: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9923 - val_loss: 0.1186 - val_accuracy: 0.9880 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9931 - val_loss: 0.1176 - val_accuracy: 0.9900 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9922 - val_loss: 0.1191 - val_accuracy: 0.9899 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 21: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9927 - val_loss: 0.1188 - val_accuracy: 0.9887 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 22: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9925 - val_loss: 0.1174 - val_accuracy: 0.9896 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 23: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9932 - val_loss: 0.1186 - val_accuracy: 0.9891 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 24: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9920 - val_loss: 0.1179 - val_accuracy: 0.9901 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9929 - val_loss: 0.1179 - val_accuracy: 0.9899 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9923 - val_loss: 0.1176 - val_accuracy: 0.9895 - lr: 0.0010 - 419ms/epoch - 105ms/step\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9932 - val_loss: 0.1190 - val_accuracy: 0.9880 - lr: 0.0010 - 382ms/epoch - 96ms/step\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.1181 - val_accuracy: 0.9885 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 29: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9913 - val_loss: 0.1187 - val_accuracy: 0.9895 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 30: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.1167 - val_accuracy: 0.9913 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 31: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0218 - accuracy: 0.9926 - val_loss: 0.1198 - val_accuracy: 0.9896 - lr: 0.0010 - 417ms/epoch - 104ms/step\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 32: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0209 - accuracy: 0.9926 - val_loss: 0.1170 - val_accuracy: 0.9902 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 33: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0213 - accuracy: 0.9926 - val_loss: 0.1201 - val_accuracy: 0.9879 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 34: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9927 - val_loss: 0.1180 - val_accuracy: 0.9889 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 35: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9926 - val_loss: 0.1190 - val_accuracy: 0.9886 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 36: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.1194 - val_accuracy: 0.9890 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 37: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9909 - val_loss: 0.1176 - val_accuracy: 0.9901 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 38: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9922 - val_loss: 0.1179 - val_accuracy: 0.9894 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 39: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.1170 - val_accuracy: 0.9878 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 40: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0186 - accuracy: 0.9925 - val_loss: 0.1164 - val_accuracy: 0.9904 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 41: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9926 - val_loss: 0.1173 - val_accuracy: 0.9893 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 42: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0201 - accuracy: 0.9916 - val_loss: 0.1205 - val_accuracy: 0.9871 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 43: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.1173 - val_accuracy: 0.9902 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 44: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9910 - val_loss: 0.1177 - val_accuracy: 0.9897 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 45: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.1182 - val_accuracy: 0.9888 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 46: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9922 - val_loss: 0.1182 - val_accuracy: 0.9894 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 47: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9932 - val_loss: 0.1191 - val_accuracy: 0.9883 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 48: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9940 - val_loss: 0.1179 - val_accuracy: 0.9886 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 49: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0156 - accuracy: 0.9920 - val_loss: 0.1183 - val_accuracy: 0.9895 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 50: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0157 - accuracy: 0.9936 - val_loss: 0.1176 - val_accuracy: 0.9888 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 51: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9938 - val_loss: 0.1177 - val_accuracy: 0.9901 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 52: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9929 - val_loss: 0.1171 - val_accuracy: 0.9907 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 53: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1173 - val_accuracy: 0.9895 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 54: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1216 - val_accuracy: 0.9874 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 55: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9925 - val_loss: 0.1248 - val_accuracy: 0.9831 - lr: 0.0010 - 344ms/epoch - 86ms/step\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 56: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0279 - accuracy: 0.9889 - val_loss: 0.1195 - val_accuracy: 0.9865 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 57: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0282 - accuracy: 0.9890 - val_loss: 0.1194 - val_accuracy: 0.9913 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 58: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0343 - accuracy: 0.9906 - val_loss: 0.1226 - val_accuracy: 0.9789 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 59: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0307 - accuracy: 0.9824 - val_loss: 0.1181 - val_accuracy: 0.9905 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 60: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0269 - accuracy: 0.9890 - val_loss: 0.1208 - val_accuracy: 0.9758 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 61: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0264 - accuracy: 0.9844 - val_loss: 0.1174 - val_accuracy: 0.9889 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 62: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0236 - accuracy: 0.9894 - val_loss: 0.1164 - val_accuracy: 0.9894 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 63: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9910 - val_loss: 0.1183 - val_accuracy: 0.9891 - lr: 0.0010 - 383ms/epoch - 96ms/step\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 64: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9921 - val_loss: 0.1175 - val_accuracy: 0.9888 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 65: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9925 - val_loss: 0.1183 - val_accuracy: 0.9895 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 66: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9918 - val_loss: 0.1176 - val_accuracy: 0.9898 - lr: 0.0010 - 422ms/epoch - 105ms/step\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 67: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0155 - accuracy: 0.9933 - val_loss: 0.1180 - val_accuracy: 0.9901 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 68: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9926 - val_loss: 0.1182 - val_accuracy: 0.9887 - lr: 0.0010 - 354ms/epoch - 89ms/step\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 69: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9939 - val_loss: 0.1188 - val_accuracy: 0.9896 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 70: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0154 - accuracy: 0.9930 - val_loss: 0.1169 - val_accuracy: 0.9899 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 71: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9936 - val_loss: 0.1190 - val_accuracy: 0.9895 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 72: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0144 - accuracy: 0.9933 - val_loss: 0.1177 - val_accuracy: 0.9893 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 73: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0145 - accuracy: 0.9940 - val_loss: 0.1175 - val_accuracy: 0.9898 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 74: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0154 - accuracy: 0.9928 - val_loss: 0.1183 - val_accuracy: 0.9898 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 75: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0147 - accuracy: 0.9935 - val_loss: 0.1169 - val_accuracy: 0.9903 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 76: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9930 - val_loss: 0.1185 - val_accuracy: 0.9890 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 77: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9935 - val_loss: 0.1176 - val_accuracy: 0.9904 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 78: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9944 - val_loss: 0.1186 - val_accuracy: 0.9897 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 79: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9927 - val_loss: 0.1181 - val_accuracy: 0.9892 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 80: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9938 - val_loss: 0.1198 - val_accuracy: 0.9884 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 81: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0168 - accuracy: 0.9937 - val_loss: 0.1175 - val_accuracy: 0.9908 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 82: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9933 - val_loss: 0.1177 - val_accuracy: 0.9909 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 83: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1186 - val_accuracy: 0.9894 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 84: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0234 - accuracy: 0.9911 - val_loss: 0.1178 - val_accuracy: 0.9893 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 85: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0265 - accuracy: 0.9849 - val_loss: 0.1207 - val_accuracy: 0.9805 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 86: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0228 - accuracy: 0.9871 - val_loss: 0.1200 - val_accuracy: 0.9689 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 87: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0248 - accuracy: 0.9895 - val_loss: 0.1228 - val_accuracy: 0.9843 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 88: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9891 - val_loss: 0.1194 - val_accuracy: 0.9811 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 89: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0223 - accuracy: 0.9911 - val_loss: 0.1181 - val_accuracy: 0.9883 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 90: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0208 - accuracy: 0.9913 - val_loss: 0.1184 - val_accuracy: 0.9893 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 91: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0200 - accuracy: 0.9929 - val_loss: 0.1190 - val_accuracy: 0.9883 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 92: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1186 - val_accuracy: 0.9870 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 93: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9927 - val_loss: 0.1183 - val_accuracy: 0.9894 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 94: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9929 - val_loss: 0.1172 - val_accuracy: 0.9888 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 95: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9921 - val_loss: 0.1181 - val_accuracy: 0.9887 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 96: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9925 - val_loss: 0.1190 - val_accuracy: 0.9878 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 97: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.1192 - val_accuracy: 0.9888 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 98: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9912 - val_loss: 0.1187 - val_accuracy: 0.9899 - lr: 0.0010 - 408ms/epoch - 102ms/step\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 99: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9919 - val_loss: 0.1184 - val_accuracy: 0.9899 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 100: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9936 - val_loss: 0.1180 - val_accuracy: 0.9904 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 101: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9931 - val_loss: 0.1197 - val_accuracy: 0.9888 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 102: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9918 - val_loss: 0.1187 - val_accuracy: 0.9884 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 103: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9921 - val_loss: 0.1182 - val_accuracy: 0.9901 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 104: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9937 - val_loss: 0.1176 - val_accuracy: 0.9893 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 105: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.1196 - val_accuracy: 0.9892 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 106: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9923 - val_loss: 0.1184 - val_accuracy: 0.9891 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 107: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9943 - val_loss: 0.1206 - val_accuracy: 0.9877 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 108: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9891 - val_loss: 0.1190 - val_accuracy: 0.9882 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 109: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9957 - val_loss: 0.1198 - val_accuracy: 0.9897 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 110: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9924 - val_loss: 0.1194 - val_accuracy: 0.9887 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 111: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9928 - val_loss: 0.1204 - val_accuracy: 0.9882 - lr: 0.0010 - 400ms/epoch - 100ms/step\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 112: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1183 - val_accuracy: 0.9901 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 113: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0213 - accuracy: 0.9919 - val_loss: 0.1201 - val_accuracy: 0.9911 - lr: 0.0010 - 372ms/epoch - 93ms/step\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 114: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.1205 - val_accuracy: 0.9827 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 115: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0286 - accuracy: 0.9889 - val_loss: 0.1218 - val_accuracy: 0.9756 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 116: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0327 - accuracy: 0.9847 - val_loss: 0.1256 - val_accuracy: 0.9770 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 117: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0391 - accuracy: 0.9548 - val_loss: 0.1551 - val_accuracy: 0.9913 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 118: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1534 - accuracy: 0.8262 - val_loss: 0.3930 - val_accuracy: 0.6172 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 119: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.5608 - accuracy: 0.4499 - val_loss: 0.2339 - val_accuracy: 0.6410 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 120: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.3072 - accuracy: 0.4974 - val_loss: 0.2491 - val_accuracy: 0.5756 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 121: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.2411 - accuracy: 0.6786 - val_loss: 0.1972 - val_accuracy: 0.6790 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 122: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1931 - accuracy: 0.7449 - val_loss: 0.1687 - val_accuracy: 0.6930 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 123: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.1055 - accuracy: 0.7556 - val_loss: 0.1492 - val_accuracy: 0.8880 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 124: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0831 - accuracy: 0.8727 - val_loss: 0.1451 - val_accuracy: 0.7823 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 125: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0706 - accuracy: 0.8766 - val_loss: 0.1331 - val_accuracy: 0.9127 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 126: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0552 - accuracy: 0.9442 - val_loss: 0.1293 - val_accuracy: 0.9792 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 127: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0426 - accuracy: 0.9793 - val_loss: 0.1247 - val_accuracy: 0.9740 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 128: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0396 - accuracy: 0.9614 - val_loss: 0.1213 - val_accuracy: 0.9840 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 129: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0314 - accuracy: 0.9845 - val_loss: 0.1233 - val_accuracy: 0.9811 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 130: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0268 - accuracy: 0.9793 - val_loss: 0.1186 - val_accuracy: 0.9876 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 131: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0243 - accuracy: 0.9904 - val_loss: 0.1213 - val_accuracy: 0.9903 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 132: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9888 - val_loss: 0.1189 - val_accuracy: 0.9901 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 133: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9920 - val_loss: 0.1180 - val_accuracy: 0.9898 - lr: 0.0010 - 382ms/epoch - 95ms/step\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 134: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9910 - val_loss: 0.1179 - val_accuracy: 0.9891 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 135: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9935 - val_loss: 0.1194 - val_accuracy: 0.9895 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 136: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9927 - val_loss: 0.1189 - val_accuracy: 0.9889 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 137: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0169 - accuracy: 0.9935 - val_loss: 0.1191 - val_accuracy: 0.9891 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 138: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0173 - accuracy: 0.9935 - val_loss: 0.1191 - val_accuracy: 0.9897 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 139: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9911 - val_loss: 0.1192 - val_accuracy: 0.9898 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 140: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.1196 - val_accuracy: 0.9892 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 141: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0154 - accuracy: 0.9936 - val_loss: 0.1174 - val_accuracy: 0.9905 - lr: 0.0010 - 409ms/epoch - 102ms/step\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 142: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9941 - val_loss: 0.1204 - val_accuracy: 0.9900 - lr: 0.0010 - 327ms/epoch - 82ms/step\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 143: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1174 - val_accuracy: 0.9897 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 144: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0156 - accuracy: 0.9918 - val_loss: 0.1176 - val_accuracy: 0.9887 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 145: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0168 - accuracy: 0.9937 - val_loss: 0.1207 - val_accuracy: 0.9891 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 146: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1177 - val_accuracy: 0.9896 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 147: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9928 - val_loss: 0.1195 - val_accuracy: 0.9902 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 148: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.1181 - val_accuracy: 0.9901 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 149: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9955 - val_loss: 0.1202 - val_accuracy: 0.9881 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 150: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9921 - val_loss: 0.1176 - val_accuracy: 0.9903 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 151: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9943 - val_loss: 0.1178 - val_accuracy: 0.9900 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 152: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9930 - val_loss: 0.1182 - val_accuracy: 0.9900 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 153: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9942 - val_loss: 0.1170 - val_accuracy: 0.9908 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 154: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9934 - val_loss: 0.1160 - val_accuracy: 0.9901 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 155: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0211 - accuracy: 0.9940 - val_loss: 0.1175 - val_accuracy: 0.9898 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 156: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0272 - accuracy: 0.9930 - val_loss: 0.1173 - val_accuracy: 0.9910 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 157: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0288 - accuracy: 0.9923 - val_loss: 0.1230 - val_accuracy: 0.9905 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 158: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0315 - accuracy: 0.9914 - val_loss: 0.1186 - val_accuracy: 0.9898 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 159: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0238 - accuracy: 0.9949 - val_loss: 0.1163 - val_accuracy: 0.9902 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 160: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9922 - val_loss: 0.1160 - val_accuracy: 0.9903 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 161: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9937 - val_loss: 0.1172 - val_accuracy: 0.9914 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 162: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0191 - accuracy: 0.9933 - val_loss: 0.1172 - val_accuracy: 0.9904 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 163: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9937 - val_loss: 0.1180 - val_accuracy: 0.9894 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 164: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.1170 - val_accuracy: 0.9900 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 165: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0184 - accuracy: 0.9943 - val_loss: 0.1175 - val_accuracy: 0.9905 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 166: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0253 - accuracy: 0.9924 - val_loss: 0.1163 - val_accuracy: 0.9907 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 167: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0267 - accuracy: 0.9936 - val_loss: 0.1185 - val_accuracy: 0.9908 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 168: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0276 - accuracy: 0.9927 - val_loss: 0.1179 - val_accuracy: 0.9911 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 169: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0225 - accuracy: 0.9926 - val_loss: 0.1171 - val_accuracy: 0.9908 - lr: 0.0010 - 379ms/epoch - 95ms/step\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 170: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0261 - accuracy: 0.9930 - val_loss: 0.1181 - val_accuracy: 0.9904 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 171: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0264 - accuracy: 0.9933 - val_loss: 0.1167 - val_accuracy: 0.9893 - lr: 0.0010 - 409ms/epoch - 102ms/step\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 172: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0183 - accuracy: 0.9925 - val_loss: 0.1168 - val_accuracy: 0.9895 - lr: 0.0010 - 384ms/epoch - 96ms/step\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 173: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0183 - accuracy: 0.9931 - val_loss: 0.1179 - val_accuracy: 0.9912 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 174: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9943 - val_loss: 0.1188 - val_accuracy: 0.9910 - lr: 0.0010 - 409ms/epoch - 102ms/step\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 175: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1185 - val_accuracy: 0.9898 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 176: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9941 - val_loss: 0.1173 - val_accuracy: 0.9890 - lr: 0.0010 - 414ms/epoch - 103ms/step\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 177: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.1189 - val_accuracy: 0.9893 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 178: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9925 - val_loss: 0.1178 - val_accuracy: 0.9898 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 179: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0155 - accuracy: 0.9945 - val_loss: 0.1190 - val_accuracy: 0.9899 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 180: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9931 - val_loss: 0.1174 - val_accuracy: 0.9907 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 181: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0157 - accuracy: 0.9937 - val_loss: 0.1192 - val_accuracy: 0.9894 - lr: 0.0010 - 355ms/epoch - 89ms/step\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 182: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9942 - val_loss: 0.1175 - val_accuracy: 0.9897 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 183: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.1199 - val_accuracy: 0.9901 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 184: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9944 - val_loss: 0.1180 - val_accuracy: 0.9901 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 185: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0137 - accuracy: 0.9936 - val_loss: 0.1194 - val_accuracy: 0.9899 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 186: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9939 - val_loss: 0.1176 - val_accuracy: 0.9900 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 187: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9946 - val_loss: 0.1193 - val_accuracy: 0.9897 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 188: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0155 - accuracy: 0.9936 - val_loss: 0.1176 - val_accuracy: 0.9892 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 189: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.1219 - val_accuracy: 0.9875 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 190: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9925 - val_loss: 0.1200 - val_accuracy: 0.9884 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 191: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9935 - val_loss: 0.1192 - val_accuracy: 0.9896 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 192: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9934 - val_loss: 0.1169 - val_accuracy: 0.9907 - lr: 0.0010 - 342ms/epoch - 86ms/step\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 193: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.1192 - val_accuracy: 0.9894 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 194: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9945 - val_loss: 0.1179 - val_accuracy: 0.9895 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 195: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9938 - val_loss: 0.1191 - val_accuracy: 0.9892 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 196: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0137 - accuracy: 0.9941 - val_loss: 0.1185 - val_accuracy: 0.9891 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 197: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9942 - val_loss: 0.1181 - val_accuracy: 0.9899 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 198: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0124 - accuracy: 0.9941 - val_loss: 0.1184 - val_accuracy: 0.9895 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 199: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9949 - val_loss: 0.1182 - val_accuracy: 0.9889 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 200: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9933 - val_loss: 0.1168 - val_accuracy: 0.9904 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 201: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0168 - accuracy: 0.9940 - val_loss: 0.1170 - val_accuracy: 0.9900 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 202: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.1193 - val_accuracy: 0.9896 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 203: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0155 - accuracy: 0.9944 - val_loss: 0.1172 - val_accuracy: 0.9898 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 204: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9927 - val_loss: 0.1178 - val_accuracy: 0.9907 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 205: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.1180 - val_accuracy: 0.9896 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 206: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9941 - val_loss: 0.1198 - val_accuracy: 0.9890 - lr: 0.0010 - 419ms/epoch - 105ms/step\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 207: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9940 - val_loss: 0.1182 - val_accuracy: 0.9886 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 208: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0167 - accuracy: 0.9927 - val_loss: 0.1188 - val_accuracy: 0.9899 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 209: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9953 - val_loss: 0.1170 - val_accuracy: 0.9908 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 210: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9932 - val_loss: 0.1166 - val_accuracy: 0.9903 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 211: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.1190 - val_accuracy: 0.9895 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 212: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9935 - val_loss: 0.1167 - val_accuracy: 0.9892 - lr: 0.0010 - 382ms/epoch - 96ms/step\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 213: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0139 - accuracy: 0.9939 - val_loss: 0.1173 - val_accuracy: 0.9899 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 214: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0132 - accuracy: 0.9945 - val_loss: 0.1177 - val_accuracy: 0.9897 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 215: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9942 - val_loss: 0.1182 - val_accuracy: 0.9896 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 216: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0128 - accuracy: 0.9935 - val_loss: 0.1179 - val_accuracy: 0.9902 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 217: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0132 - accuracy: 0.9967 - val_loss: 0.1187 - val_accuracy: 0.9900 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 218: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0131 - accuracy: 0.9923 - val_loss: 0.1194 - val_accuracy: 0.9890 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 219: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0147 - accuracy: 0.9945 - val_loss: 0.1175 - val_accuracy: 0.9903 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 220: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9936 - val_loss: 0.1191 - val_accuracy: 0.9905 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 221: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9928 - val_loss: 0.1180 - val_accuracy: 0.9900 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 222: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9944 - val_loss: 0.1195 - val_accuracy: 0.9888 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 223: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9926 - val_loss: 0.1182 - val_accuracy: 0.9899 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 224: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0139 - accuracy: 0.9942 - val_loss: 0.1183 - val_accuracy: 0.9905 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 225: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0137 - accuracy: 0.9952 - val_loss: 0.1180 - val_accuracy: 0.9896 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 226: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0144 - accuracy: 0.9939 - val_loss: 0.1182 - val_accuracy: 0.9887 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 227: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0123 - accuracy: 0.9936 - val_loss: 0.1175 - val_accuracy: 0.9899 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 228: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0127 - accuracy: 0.9943 - val_loss: 0.1188 - val_accuracy: 0.9895 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 229: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.1180 - val_accuracy: 0.9895 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 230: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9942 - val_loss: 0.1179 - val_accuracy: 0.9896 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 231: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9939 - val_loss: 0.1171 - val_accuracy: 0.9903 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 232: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9944 - val_loss: 0.1189 - val_accuracy: 0.9889 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 233: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9943 - val_loss: 0.1179 - val_accuracy: 0.9894 - lr: 0.0010 - 375ms/epoch - 94ms/step\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 234: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0136 - accuracy: 0.9949 - val_loss: 0.1156 - val_accuracy: 0.9904 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 235: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9934 - val_loss: 0.1195 - val_accuracy: 0.9889 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 236: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9935 - val_loss: 0.1200 - val_accuracy: 0.9884 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 237: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.1224 - val_accuracy: 0.9874 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 238: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0240 - accuracy: 0.9921 - val_loss: 0.1210 - val_accuracy: 0.9896 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 239: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0226 - accuracy: 0.9954 - val_loss: 0.1165 - val_accuracy: 0.9907 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 240: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9928 - val_loss: 0.1195 - val_accuracy: 0.9889 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 241: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9925 - val_loss: 0.1163 - val_accuracy: 0.9904 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 242: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0204 - accuracy: 0.9924 - val_loss: 0.1177 - val_accuracy: 0.9904 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 243: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0160 - accuracy: 0.9937 - val_loss: 0.1173 - val_accuracy: 0.9904 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 244: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9941 - val_loss: 0.1179 - val_accuracy: 0.9896 - lr: 0.0010 - 382ms/epoch - 95ms/step\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 245: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9938 - val_loss: 0.1205 - val_accuracy: 0.9892 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 246: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9945 - val_loss: 0.1167 - val_accuracy: 0.9904 - lr: 0.0010 - 411ms/epoch - 103ms/step\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 247: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0136 - accuracy: 0.9944 - val_loss: 0.1188 - val_accuracy: 0.9895 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 248: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9924 - val_loss: 0.1170 - val_accuracy: 0.9900 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 249: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0131 - accuracy: 0.9950 - val_loss: 0.1175 - val_accuracy: 0.9904 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 250: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0139 - accuracy: 0.9942 - val_loss: 0.1183 - val_accuracy: 0.9896 - lr: 0.0010 - 355ms/epoch - 89ms/step\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 251: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9947 - val_loss: 0.1183 - val_accuracy: 0.9901 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 252: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0118 - accuracy: 0.9940 - val_loss: 0.1178 - val_accuracy: 0.9903 - lr: 0.0010 - 327ms/epoch - 82ms/step\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 253: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0126 - accuracy: 0.9949 - val_loss: 0.1186 - val_accuracy: 0.9898 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 254: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0123 - accuracy: 0.9938 - val_loss: 0.1166 - val_accuracy: 0.9905 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 255: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0128 - accuracy: 0.9944 - val_loss: 0.1186 - val_accuracy: 0.9891 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 256: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0127 - accuracy: 0.9939 - val_loss: 0.1170 - val_accuracy: 0.9904 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 257: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0145 - accuracy: 0.9940 - val_loss: 0.1176 - val_accuracy: 0.9900 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 258: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0132 - accuracy: 0.9945 - val_loss: 0.1174 - val_accuracy: 0.9900 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 259: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9932 - val_loss: 0.1182 - val_accuracy: 0.9899 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 260: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.1181 - val_accuracy: 0.9902 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 261: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0141 - accuracy: 0.9941 - val_loss: 0.1173 - val_accuracy: 0.9897 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 262: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9951 - val_loss: 0.1212 - val_accuracy: 0.9885 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 263: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0206 - accuracy: 0.9952 - val_loss: 0.1199 - val_accuracy: 0.9888 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 264: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0202 - accuracy: 0.9920 - val_loss: 0.1179 - val_accuracy: 0.9899 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 265: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0211 - accuracy: 0.9937 - val_loss: 0.1189 - val_accuracy: 0.9895 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 266: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0237 - accuracy: 0.9931 - val_loss: 0.1192 - val_accuracy: 0.9896 - lr: 0.0010 - 386ms/epoch - 97ms/step\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 267: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.1197 - val_accuracy: 0.9889 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 268: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0247 - accuracy: 0.9929 - val_loss: 0.1246 - val_accuracy: 0.9862 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 269: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0250 - accuracy: 0.9934 - val_loss: 0.1206 - val_accuracy: 0.9888 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 270: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.1190 - val_accuracy: 0.9885 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 271: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9929 - val_loss: 0.1197 - val_accuracy: 0.9889 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 272: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0174 - accuracy: 0.9934 - val_loss: 0.1169 - val_accuracy: 0.9886 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 273: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.1165 - val_accuracy: 0.9899 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 274: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0177 - accuracy: 0.9935 - val_loss: 0.1181 - val_accuracy: 0.9902 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 275: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9941 - val_loss: 0.1167 - val_accuracy: 0.9909 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 276: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9936 - val_loss: 0.1188 - val_accuracy: 0.9901 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 277: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1173 - val_accuracy: 0.9889 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 278: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0170 - accuracy: 0.9920 - val_loss: 0.1193 - val_accuracy: 0.9888 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 279: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9955 - val_loss: 0.1181 - val_accuracy: 0.9905 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 280: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0138 - accuracy: 0.9937 - val_loss: 0.1184 - val_accuracy: 0.9904 - lr: 0.0010 - 377ms/epoch - 94ms/step\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 281: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0137 - accuracy: 0.9939 - val_loss: 0.1172 - val_accuracy: 0.9904 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 282: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0136 - accuracy: 0.9939 - val_loss: 0.1171 - val_accuracy: 0.9897 - lr: 0.0010 - 405ms/epoch - 101ms/step\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 283: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0132 - accuracy: 0.9941 - val_loss: 0.1171 - val_accuracy: 0.9910 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 284: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.1166 - val_accuracy: 0.9900 - lr: 0.0010 - 399ms/epoch - 100ms/step\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 285: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9950 - val_loss: 0.1189 - val_accuracy: 0.9890 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 286: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.1181 - val_accuracy: 0.9890 - lr: 0.0010 - 415ms/epoch - 104ms/step\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 287: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9927 - val_loss: 0.1169 - val_accuracy: 0.9901 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 288: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9957 - val_loss: 0.1177 - val_accuracy: 0.9891 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 289: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9940 - val_loss: 0.1204 - val_accuracy: 0.9876 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 290: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0182 - accuracy: 0.9918 - val_loss: 0.1188 - val_accuracy: 0.9889 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 291: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9922 - val_loss: 0.1184 - val_accuracy: 0.9907 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 292: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0209 - accuracy: 0.9911 - val_loss: 0.1180 - val_accuracy: 0.9907 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 293: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.1184 - val_accuracy: 0.9898 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 294: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9946 - val_loss: 0.1186 - val_accuracy: 0.9897 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 295: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0177 - accuracy: 0.9934 - val_loss: 0.1171 - val_accuracy: 0.9905 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 296: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0203 - accuracy: 0.9927 - val_loss: 0.1182 - val_accuracy: 0.9910 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 297: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.1176 - val_accuracy: 0.9907 - lr: 0.0010 - 338ms/epoch - 85ms/step\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 298: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0194 - accuracy: 0.9944 - val_loss: 0.1176 - val_accuracy: 0.9880 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 299: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9933 - val_loss: 0.1175 - val_accuracy: 0.9896 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 300: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0193 - accuracy: 0.9913 - val_loss: 0.1173 - val_accuracy: 0.9913 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 301: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0251 - accuracy: 0.9939 - val_loss: 0.1198 - val_accuracy: 0.9913 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 302: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.1172 - val_accuracy: 0.9900 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 303: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9930 - val_loss: 0.1171 - val_accuracy: 0.9899 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 304: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0208 - accuracy: 0.9936 - val_loss: 0.1197 - val_accuracy: 0.9896 - lr: 0.0010 - 339ms/epoch - 85ms/step\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 305: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1181 - val_accuracy: 0.9887 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 306: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.1214 - val_accuracy: 0.9885 - lr: 0.0010 - 398ms/epoch - 100ms/step\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 307: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9942 - val_loss: 0.1226 - val_accuracy: 0.9874 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 308: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.1190 - val_accuracy: 0.9892 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 309: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9914 - val_loss: 0.1243 - val_accuracy: 0.9867 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 310: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0271 - accuracy: 0.9943 - val_loss: 0.1306 - val_accuracy: 0.9800 - lr: 0.0010 - 343ms/epoch - 86ms/step\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 311: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9890 - val_loss: 0.1296 - val_accuracy: 0.9759 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 312: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0238 - accuracy: 0.9913 - val_loss: 0.1202 - val_accuracy: 0.9899 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 313: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1208 - val_accuracy: 0.9887 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 314: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.1183 - val_accuracy: 0.9901 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 315: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0201 - accuracy: 0.9928 - val_loss: 0.1193 - val_accuracy: 0.9902 - lr: 0.0010 - 370ms/epoch - 92ms/step\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 316: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0183 - accuracy: 0.9938 - val_loss: 0.1198 - val_accuracy: 0.9882 - lr: 0.0010 - 417ms/epoch - 104ms/step\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 317: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0164 - accuracy: 0.9928 - val_loss: 0.1194 - val_accuracy: 0.9891 - lr: 0.0010 - 354ms/epoch - 88ms/step\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 318: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0159 - accuracy: 0.9940 - val_loss: 0.1186 - val_accuracy: 0.9892 - lr: 0.0010 - 386ms/epoch - 97ms/step\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 319: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9927 - val_loss: 0.1171 - val_accuracy: 0.9902 - lr: 0.0010 - 381ms/epoch - 95ms/step\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 320: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0145 - accuracy: 0.9938 - val_loss: 0.1191 - val_accuracy: 0.9902 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 321: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0144 - accuracy: 0.9944 - val_loss: 0.1189 - val_accuracy: 0.9899 - lr: 0.0010 - 416ms/epoch - 104ms/step\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 322: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0138 - accuracy: 0.9941 - val_loss: 0.1188 - val_accuracy: 0.9897 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 323: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0132 - accuracy: 0.9943 - val_loss: 0.1176 - val_accuracy: 0.9898 - lr: 0.0010 - 376ms/epoch - 94ms/step\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 324: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0123 - accuracy: 0.9951 - val_loss: 0.1183 - val_accuracy: 0.9899 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 325: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9936 - val_loss: 0.1179 - val_accuracy: 0.9893 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 326: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0146 - accuracy: 0.9938 - val_loss: 0.1196 - val_accuracy: 0.9887 - lr: 0.0010 - 396ms/epoch - 99ms/step\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 327: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.1197 - val_accuracy: 0.9887 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 328: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0178 - accuracy: 0.9932 - val_loss: 0.1185 - val_accuracy: 0.9889 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 329: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9943 - val_loss: 0.1181 - val_accuracy: 0.9907 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 330: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9931 - val_loss: 0.1170 - val_accuracy: 0.9915 - lr: 0.0010 - 344ms/epoch - 86ms/step\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 331: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9929 - val_loss: 0.1178 - val_accuracy: 0.9903 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 332: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0187 - accuracy: 0.9957 - val_loss: 0.1192 - val_accuracy: 0.9902 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 333: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.1169 - val_accuracy: 0.9906 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 334: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0176 - accuracy: 0.9934 - val_loss: 0.1173 - val_accuracy: 0.9904 - lr: 0.0010 - 347ms/epoch - 87ms/step\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 335: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1178 - val_accuracy: 0.9893 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 336: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0165 - accuracy: 0.9934 - val_loss: 0.1181 - val_accuracy: 0.9905 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 337: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0142 - accuracy: 0.9950 - val_loss: 0.1189 - val_accuracy: 0.9899 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 338: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0134 - accuracy: 0.9947 - val_loss: 0.1178 - val_accuracy: 0.9896 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 339: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0128 - accuracy: 0.9938 - val_loss: 0.1175 - val_accuracy: 0.9904 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 340: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0133 - accuracy: 0.9946 - val_loss: 0.1187 - val_accuracy: 0.9897 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 341: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9945 - val_loss: 0.1178 - val_accuracy: 0.9897 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 342: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0119 - accuracy: 0.9955 - val_loss: 0.1178 - val_accuracy: 0.9896 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 343: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9928 - val_loss: 0.1175 - val_accuracy: 0.9902 - lr: 0.0010 - 345ms/epoch - 86ms/step\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 344: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0117 - accuracy: 0.9952 - val_loss: 0.1182 - val_accuracy: 0.9895 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 345: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0114 - accuracy: 0.9952 - val_loss: 0.1188 - val_accuracy: 0.9892 - lr: 0.0010 - 346ms/epoch - 87ms/step\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 346: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0123 - accuracy: 0.9944 - val_loss: 0.1171 - val_accuracy: 0.9905 - lr: 0.0010 - 406ms/epoch - 102ms/step\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 347: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9947 - val_loss: 0.1183 - val_accuracy: 0.9901 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 348: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0112 - accuracy: 0.9953 - val_loss: 0.1174 - val_accuracy: 0.9902 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 349: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0119 - accuracy: 0.9937 - val_loss: 0.1174 - val_accuracy: 0.9900 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 350: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0111 - accuracy: 0.9952 - val_loss: 0.1186 - val_accuracy: 0.9892 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 351: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9952 - val_loss: 0.1181 - val_accuracy: 0.9903 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 352: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0135 - accuracy: 0.9943 - val_loss: 0.1170 - val_accuracy: 0.9901 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 353: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0154 - accuracy: 0.9940 - val_loss: 0.1175 - val_accuracy: 0.9898 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 354: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.1184 - val_accuracy: 0.9904 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 355: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9935 - val_loss: 0.1216 - val_accuracy: 0.9892 - lr: 0.0010 - 384ms/epoch - 96ms/step\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 356: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9927 - val_loss: 0.1201 - val_accuracy: 0.9894 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 357: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1193 - val_accuracy: 0.9886 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 358: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9925 - val_loss: 0.1182 - val_accuracy: 0.9868 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 359: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0183 - accuracy: 0.9916 - val_loss: 0.1163 - val_accuracy: 0.9893 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 360: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1185 - val_accuracy: 0.9902 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 361: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0185 - accuracy: 0.9920 - val_loss: 0.1197 - val_accuracy: 0.9908 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 362: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0189 - accuracy: 0.9946 - val_loss: 0.1177 - val_accuracy: 0.9905 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 363: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0158 - accuracy: 0.9938 - val_loss: 0.1182 - val_accuracy: 0.9904 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 364: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0154 - accuracy: 0.9943 - val_loss: 0.1186 - val_accuracy: 0.9900 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 365: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0146 - accuracy: 0.9930 - val_loss: 0.1178 - val_accuracy: 0.9899 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 366: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0131 - accuracy: 0.9949 - val_loss: 0.1172 - val_accuracy: 0.9903 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 367: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9935 - val_loss: 0.1187 - val_accuracy: 0.9892 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 368: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.1173 - val_accuracy: 0.9906 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 369: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9940 - val_loss: 0.1177 - val_accuracy: 0.9905 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 370: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9948 - val_loss: 0.1184 - val_accuracy: 0.9903 - lr: 0.0010 - 374ms/epoch - 93ms/step\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 371: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0119 - accuracy: 0.9946 - val_loss: 0.1179 - val_accuracy: 0.9908 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 372: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9949 - val_loss: 0.1179 - val_accuracy: 0.9904 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 373: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0127 - accuracy: 0.9936 - val_loss: 0.1188 - val_accuracy: 0.9898 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 374: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9950 - val_loss: 0.1177 - val_accuracy: 0.9908 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 375: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.1186 - val_accuracy: 0.9899 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 376: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0135 - accuracy: 0.9949 - val_loss: 0.1166 - val_accuracy: 0.9897 - lr: 0.0010 - 390ms/epoch - 98ms/step\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 377: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0125 - accuracy: 0.9949 - val_loss: 0.1172 - val_accuracy: 0.9903 - lr: 0.0010 - 358ms/epoch - 89ms/step\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 378: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0139 - accuracy: 0.9935 - val_loss: 0.1183 - val_accuracy: 0.9896 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 379: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.1177 - val_accuracy: 0.9898 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 380: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0115 - accuracy: 0.9938 - val_loss: 0.1176 - val_accuracy: 0.9901 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 381: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0118 - accuracy: 0.9949 - val_loss: 0.1175 - val_accuracy: 0.9903 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 382: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0110 - accuracy: 0.9948 - val_loss: 0.1177 - val_accuracy: 0.9900 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 383: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0118 - accuracy: 0.9940 - val_loss: 0.1179 - val_accuracy: 0.9903 - lr: 0.0010 - 329ms/epoch - 82ms/step\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 384: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.1172 - val_accuracy: 0.9904 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 385: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0114 - accuracy: 0.9939 - val_loss: 0.1181 - val_accuracy: 0.9898 - lr: 0.0010 - 334ms/epoch - 84ms/step\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 386: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0110 - accuracy: 0.9951 - val_loss: 0.1173 - val_accuracy: 0.9897 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 387: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0116 - accuracy: 0.9946 - val_loss: 0.1184 - val_accuracy: 0.9899 - lr: 0.0010 - 351ms/epoch - 88ms/step\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 388: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0108 - accuracy: 0.9944 - val_loss: 0.1177 - val_accuracy: 0.9904 - lr: 0.0010 - 386ms/epoch - 96ms/step\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 389: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0124 - accuracy: 0.9952 - val_loss: 0.1191 - val_accuracy: 0.9895 - lr: 0.0010 - 378ms/epoch - 94ms/step\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 390: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0136 - accuracy: 0.9950 - val_loss: 0.1171 - val_accuracy: 0.9908 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 391: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0141 - accuracy: 0.9948 - val_loss: 0.1192 - val_accuracy: 0.9896 - lr: 0.0010 - 414ms/epoch - 103ms/step\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 392: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0111 - accuracy: 0.9951 - val_loss: 0.1182 - val_accuracy: 0.9900 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 393: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9942 - val_loss: 0.1173 - val_accuracy: 0.9908 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 394: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9947 - val_loss: 0.1184 - val_accuracy: 0.9895 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 395: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.1179 - val_accuracy: 0.9895 - lr: 0.0010 - 395ms/epoch - 99ms/step\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 396: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0146 - accuracy: 0.9943 - val_loss: 0.1166 - val_accuracy: 0.9906 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 397: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1176 - val_accuracy: 0.9902 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 398: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.1178 - val_accuracy: 0.9895 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 399: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.1201 - val_accuracy: 0.9884 - lr: 0.0010 - 328ms/epoch - 82ms/step\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 400: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.1188 - val_accuracy: 0.9886 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 401: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0163 - accuracy: 0.9933 - val_loss: 0.1188 - val_accuracy: 0.9894 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 402: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0157 - accuracy: 0.9944 - val_loss: 0.1168 - val_accuracy: 0.9895 - lr: 0.0010 - 366ms/epoch - 92ms/step\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 403: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9935 - val_loss: 0.1184 - val_accuracy: 0.9897 - lr: 0.0010 - 373ms/epoch - 93ms/step\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 404: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0175 - accuracy: 0.9936 - val_loss: 0.1176 - val_accuracy: 0.9891 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 405: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9933 - val_loss: 0.1165 - val_accuracy: 0.9896 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 406: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0162 - accuracy: 0.9931 - val_loss: 0.1182 - val_accuracy: 0.9887 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 407: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9956 - val_loss: 0.1205 - val_accuracy: 0.9874 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 408: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.1190 - val_accuracy: 0.9893 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 409: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0177 - accuracy: 0.9921 - val_loss: 0.1193 - val_accuracy: 0.9903 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 410: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9934 - val_loss: 0.1177 - val_accuracy: 0.9897 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 411: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9934 - val_loss: 0.1183 - val_accuracy: 0.9896 - lr: 0.0010 - 401ms/epoch - 100ms/step\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 412: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0142 - accuracy: 0.9949 - val_loss: 0.1174 - val_accuracy: 0.9904 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 413: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0152 - accuracy: 0.9909 - val_loss: 0.1180 - val_accuracy: 0.9906 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 414: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0148 - accuracy: 0.9937 - val_loss: 0.1185 - val_accuracy: 0.9906 - lr: 0.0010 - 366ms/epoch - 91ms/step\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 415: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0149 - accuracy: 0.9942 - val_loss: 0.1177 - val_accuracy: 0.9899 - lr: 0.0010 - 422ms/epoch - 106ms/step\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 416: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9938 - val_loss: 0.1191 - val_accuracy: 0.9891 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 417: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.1175 - val_accuracy: 0.9897 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 418: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9946 - val_loss: 0.1174 - val_accuracy: 0.9900 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 419: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9939 - val_loss: 0.1173 - val_accuracy: 0.9893 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 420: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0131 - accuracy: 0.9936 - val_loss: 0.1172 - val_accuracy: 0.9902 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 421: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0132 - accuracy: 0.9948 - val_loss: 0.1175 - val_accuracy: 0.9898 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 422: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9939 - val_loss: 0.1184 - val_accuracy: 0.9889 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 423: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0130 - accuracy: 0.9926 - val_loss: 0.1177 - val_accuracy: 0.9891 - lr: 0.0010 - 389ms/epoch - 97ms/step\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 424: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.1190 - val_accuracy: 0.9889 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 425: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0123 - accuracy: 0.9947 - val_loss: 0.1175 - val_accuracy: 0.9893 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 426: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9942 - val_loss: 0.1181 - val_accuracy: 0.9896 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 427: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0118 - accuracy: 0.9948 - val_loss: 0.1169 - val_accuracy: 0.9897 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 428: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0117 - accuracy: 0.9950 - val_loss: 0.1184 - val_accuracy: 0.9893 - lr: 0.0010 - 390ms/epoch - 97ms/step\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 429: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0127 - accuracy: 0.9951 - val_loss: 0.1176 - val_accuracy: 0.9899 - lr: 0.0010 - 353ms/epoch - 88ms/step\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 430: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0133 - accuracy: 0.9948 - val_loss: 0.1187 - val_accuracy: 0.9893 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 431: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0133 - accuracy: 0.9945 - val_loss: 0.1164 - val_accuracy: 0.9900 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 432: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.1188 - val_accuracy: 0.9896 - lr: 0.0010 - 340ms/epoch - 85ms/step\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 433: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0145 - accuracy: 0.9938 - val_loss: 0.1188 - val_accuracy: 0.9887 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 434: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.1180 - val_accuracy: 0.9901 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 435: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0157 - accuracy: 0.9944 - val_loss: 0.1183 - val_accuracy: 0.9903 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 436: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.1187 - val_accuracy: 0.9898 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 437: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0136 - accuracy: 0.9946 - val_loss: 0.1169 - val_accuracy: 0.9904 - lr: 0.0010 - 361ms/epoch - 90ms/step\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 438: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0169 - accuracy: 0.9956 - val_loss: 0.1186 - val_accuracy: 0.9890 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 439: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0147 - accuracy: 0.9937 - val_loss: 0.1185 - val_accuracy: 0.9891 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 440: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0146 - accuracy: 0.9944 - val_loss: 0.1195 - val_accuracy: 0.9895 - lr: 0.0010 - 332ms/epoch - 83ms/step\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 441: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0126 - accuracy: 0.9945 - val_loss: 0.1174 - val_accuracy: 0.9904 - lr: 0.0010 - 410ms/epoch - 103ms/step\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 442: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0125 - accuracy: 0.9952 - val_loss: 0.1190 - val_accuracy: 0.9896 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 443: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0142 - accuracy: 0.9948 - val_loss: 0.1173 - val_accuracy: 0.9900 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 444: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0119 - accuracy: 0.9949 - val_loss: 0.1171 - val_accuracy: 0.9904 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 445: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0143 - accuracy: 0.9944 - val_loss: 0.1198 - val_accuracy: 0.9899 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 446: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.1183 - val_accuracy: 0.9898 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 447: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0125 - accuracy: 0.9937 - val_loss: 0.1189 - val_accuracy: 0.9894 - lr: 0.0010 - 338ms/epoch - 84ms/step\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 448: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.1172 - val_accuracy: 0.9903 - lr: 0.0010 - 337ms/epoch - 84ms/step\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 449: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0119 - accuracy: 0.9960 - val_loss: 0.1183 - val_accuracy: 0.9904 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 450: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0120 - accuracy: 0.9942 - val_loss: 0.1175 - val_accuracy: 0.9900 - lr: 0.0010 - 330ms/epoch - 83ms/step\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 451: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0113 - accuracy: 0.9956 - val_loss: 0.1171 - val_accuracy: 0.9903 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 452: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0119 - accuracy: 0.9938 - val_loss: 0.1184 - val_accuracy: 0.9899 - lr: 0.0010 - 334ms/epoch - 83ms/step\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 453: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.1178 - val_accuracy: 0.9902 - lr: 0.0010 - 328ms/epoch - 82ms/step\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 454: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0109 - accuracy: 0.9948 - val_loss: 0.1184 - val_accuracy: 0.9903 - lr: 0.0010 - 331ms/epoch - 83ms/step\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 455: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0108 - accuracy: 0.9949 - val_loss: 0.1183 - val_accuracy: 0.9894 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 456: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9947 - val_loss: 0.1177 - val_accuracy: 0.9900 - lr: 0.0010 - 393ms/epoch - 98ms/step\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 457: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.1191 - val_accuracy: 0.9895 - lr: 0.0010 - 362ms/epoch - 90ms/step\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 458: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0172 - accuracy: 0.9937 - val_loss: 0.1188 - val_accuracy: 0.9894 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 459: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.1207 - val_accuracy: 0.9861 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 460: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0209 - accuracy: 0.9928 - val_loss: 0.1186 - val_accuracy: 0.9887 - lr: 0.0010 - 392ms/epoch - 98ms/step\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 461: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0199 - accuracy: 0.9941 - val_loss: 0.1183 - val_accuracy: 0.9904 - lr: 0.0010 - 429ms/epoch - 107ms/step\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 462: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0229 - accuracy: 0.9898 - val_loss: 0.1173 - val_accuracy: 0.9911 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 463: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.1178 - val_accuracy: 0.9911 - lr: 0.0010 - 356ms/epoch - 89ms/step\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 464: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.1180 - val_accuracy: 0.9874 - lr: 0.0010 - 385ms/epoch - 96ms/step\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 465: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0197 - accuracy: 0.9908 - val_loss: 0.1202 - val_accuracy: 0.9869 - lr: 0.0010 - 403ms/epoch - 101ms/step\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 466: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0231 - accuracy: 0.9930 - val_loss: 0.1267 - val_accuracy: 0.9837 - lr: 0.0010 - 397ms/epoch - 99ms/step\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 467: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0260 - accuracy: 0.9954 - val_loss: 0.1331 - val_accuracy: 0.9595 - lr: 0.0010 - 370ms/epoch - 93ms/step\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 468: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0361 - accuracy: 0.9735 - val_loss: 0.1234 - val_accuracy: 0.9869 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 469: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0300 - accuracy: 0.9892 - val_loss: 0.1239 - val_accuracy: 0.9833 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 470: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0232 - accuracy: 0.9928 - val_loss: 0.1188 - val_accuracy: 0.9894 - lr: 0.0010 - 369ms/epoch - 92ms/step\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 471: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.1183 - val_accuracy: 0.9904 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 472: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0181 - accuracy: 0.9919 - val_loss: 0.1177 - val_accuracy: 0.9913 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 473: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0150 - accuracy: 0.9942 - val_loss: 0.1179 - val_accuracy: 0.9892 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 474: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0140 - accuracy: 0.9942 - val_loss: 0.1171 - val_accuracy: 0.9902 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 475: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0135 - accuracy: 0.9938 - val_loss: 0.1171 - val_accuracy: 0.9905 - lr: 0.0010 - 374ms/epoch - 94ms/step\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 476: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0133 - accuracy: 0.9939 - val_loss: 0.1171 - val_accuracy: 0.9903 - lr: 0.0010 - 398ms/epoch - 99ms/step\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 477: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0122 - accuracy: 0.9955 - val_loss: 0.1175 - val_accuracy: 0.9905 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 478: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0153 - accuracy: 0.9935 - val_loss: 0.1175 - val_accuracy: 0.9897 - lr: 0.0010 - 368ms/epoch - 92ms/step\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 479: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9942 - val_loss: 0.1190 - val_accuracy: 0.9900 - lr: 0.0010 - 364ms/epoch - 91ms/step\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 480: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0164 - accuracy: 0.9951 - val_loss: 0.1196 - val_accuracy: 0.9897 - lr: 0.0010 - 371ms/epoch - 93ms/step\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 481: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0195 - accuracy: 0.9938 - val_loss: 0.1219 - val_accuracy: 0.9893 - lr: 0.0010 - 388ms/epoch - 97ms/step\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 482: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.1270 - val_accuracy: 0.9833 - lr: 0.0010 - 360ms/epoch - 90ms/step\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 483: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0301 - accuracy: 0.9881 - val_loss: 0.1225 - val_accuracy: 0.9882 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 484: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0295 - accuracy: 0.9815 - val_loss: 0.1207 - val_accuracy: 0.9910 - lr: 0.0010 - 363ms/epoch - 91ms/step\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 485: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0303 - accuracy: 0.9916 - val_loss: 0.1197 - val_accuracy: 0.9902 - lr: 0.0010 - 367ms/epoch - 92ms/step\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 486: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0186 - accuracy: 0.9922 - val_loss: 0.1191 - val_accuracy: 0.9890 - lr: 0.0010 - 359ms/epoch - 90ms/step\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 487: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0179 - accuracy: 0.9915 - val_loss: 0.1182 - val_accuracy: 0.9904 - lr: 0.0010 - 341ms/epoch - 85ms/step\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 488: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0166 - accuracy: 0.9937 - val_loss: 0.1195 - val_accuracy: 0.9902 - lr: 0.0010 - 365ms/epoch - 91ms/step\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 489: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1181 - val_accuracy: 0.9909 - lr: 0.0010 - 357ms/epoch - 89ms/step\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 490: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0142 - accuracy: 0.9942 - val_loss: 0.1182 - val_accuracy: 0.9904 - lr: 0.0010 - 362ms/epoch - 91ms/step\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 491: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0133 - accuracy: 0.9948 - val_loss: 0.1178 - val_accuracy: 0.9907 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 492: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0131 - accuracy: 0.9934 - val_loss: 0.1183 - val_accuracy: 0.9911 - lr: 0.0010 - 330ms/epoch - 82ms/step\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 493: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0139 - accuracy: 0.9940 - val_loss: 0.1172 - val_accuracy: 0.9906 - lr: 0.0010 - 336ms/epoch - 84ms/step\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 494: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.1185 - val_accuracy: 0.9897 - lr: 0.0010 - 333ms/epoch - 83ms/step\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 495: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0114 - accuracy: 0.9948 - val_loss: 0.1178 - val_accuracy: 0.9904 - lr: 0.0010 - 335ms/epoch - 84ms/step\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 496: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0116 - accuracy: 0.9947 - val_loss: 0.1181 - val_accuracy: 0.9898 - lr: 0.0010 - 419ms/epoch - 105ms/step\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 497: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0124 - accuracy: 0.9953 - val_loss: 0.1184 - val_accuracy: 0.9905 - lr: 0.0010 - 358ms/epoch - 90ms/step\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 498: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0108 - accuracy: 0.9948 - val_loss: 0.1187 - val_accuracy: 0.9902 - lr: 0.0010 - 391ms/epoch - 98ms/step\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 499: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0138 - accuracy: 0.9949 - val_loss: 0.1172 - val_accuracy: 0.9901 - lr: 0.0010 - 387ms/epoch - 97ms/step\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 500: val_loss did not improve from 0.11285\n",
            "4/4 - 0s - loss: 0.0105 - accuracy: 0.9955 - val_loss: 0.1171 - val_accuracy: 0.9903 - lr: 0.0010 - 386ms/epoch - 96ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "BXLRaBZtIiZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predY = model.predict(xtest)"
      ],
      "metadata": {
        "id": "JUqA2fPKDkFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b5e19d-28a1-41ac-a626-4957e400ac78"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 13s 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm = predY\n",
        "for i in range(len(predY)):\n",
        "  x_norm[i]=((predY[i]-np.min(predY[i]))/(np.max(predY[i])-np.min(predY[i])))*255.0\n",
        "  xtest[i]=((xtest[i]-np.min(xtest[i]))/(np.max(xtest[i])-np.min(xtest[i])))*255.0"
      ],
      "metadata": {
        "id": "Lulw6Se_CD_g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm.shape"
      ],
      "metadata": {
        "id": "qJBLbsoxD2Z3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbbb9b7-f171-496b-b23e-29d19801803e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 128, 128, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totalAcc = 0\n",
        "mae = 0\n",
        "for img in range(len(x_norm)):\n",
        "  sumY = 0\n",
        "  sumPred = 0\n",
        "  for i in range(len(x_norm[img])):\n",
        "    for j in range(len(x_norm[img][i])):\n",
        "      for k in range(len(x_norm[img][i][j])):\n",
        "        sumY = sumY+xtest[img][i][j][k]\n",
        "        sumPred = sumPred+x_norm[img][i][j][k]\n",
        "  error = np.abs(sumPred-sumY)\n",
        "  diff = np.abs(1-(sumPred/sumY))\n",
        "  acc = (1-diff)*100\n",
        "  totalAcc = totalAcc+acc\n",
        "  mae = mae + error\n",
        "print(\"final acc \",totalAcc/len(x_norm))\n",
        "print(\"mae \",mae/len(x_norm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHueD-kzMo7",
        "outputId": "7fb5bac3-cc10-4246-c8b3-cc3344a43696"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final acc  86.59599288735049\n",
            "mae  983104.0703804387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming y_test contains the true labels for the test data\n",
        "y_pred = model.predict(xtest)\n",
        "\n",
        "# Assuming y_pred contains the predicted labels\n",
        "accuracy = accuracy_score(ytest, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Assuming y_test contains the true labels for the test data\n",
        "y_pred = model.predict(xtest)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(ytest, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "fHSVlvEDBK9y",
        "outputId": "80d4f34a-ebed-4698-92d0-a2e00afc73be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 80ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-e2a1355d87e6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Assuming y_pred contains the predicted labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(x_norm[0])"
      ],
      "metadata": {
        "id": "H7Vtu1E9CD7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "6e838e41-5f2c-46d9-9be8-759483ae414f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(x_norm)"
      ],
      "metadata": {
        "id": "NiXIJcNeHGCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2120acb5-c4af-4635-8542-371c70be74c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255.0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = tensorflow.keras.metrics.Accuracy()\n",
        "metric.update_state(xtest*255.0,predY*255.0)\n",
        "metric.result().numpy()"
      ],
      "metadata": {
        "id": "A2IsMTlgL41L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6815c687-4b41-4dcf-fcd6-a726c156eac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_values = np.array(xtest)\n",
        "predictions = np.array(predY)\n",
        "\n",
        "N = true_values.shape[1]\n",
        "accuracy = (true_values == predictions).sum() / N\n",
        "TP = ((predictions == 1) & (true_values == 1)).sum()\n",
        "FP = ((predictions == 1) & (true_values == 0)).sum()\n",
        "precision = TP / (TP+FP)"
      ],
      "metadata": {
        "id": "2cP2gRMWL43S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_values.shape"
      ],
      "metadata": {
        "id": "bO22GraclVgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKnQX6YBL486"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(xtrain,ytrain, batch_size=64,\n",
        "               steps_per_epoch=xtrain.shape[0]/64,\n",
        "               epochs=5000,\n",
        "               verbose=2,\n",
        "               callbacks=[anne, checkpoint],\n",
        "               validation_split = 0.1)"
      ],
      "metadata": {
        "id": "2JRMw-bdT6v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "pWoVAvc_jj7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "odoon1Kajo_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}